{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "iICFRAFzXeGj",
   "metadata": {
    "id": "iICFRAFzXeGj"
   },
   "source": [
    "# ðŸ“Š Fund Review Automation\n",
    "\n",
    "This notebook performs the full pipeline:\n",
    "- Connect to Neo4j: Interface with a graph database to store and query legal content as a knowledge graph.\n",
    "\n",
    "- Construct a Legal Knowledge Graph: Transform unstructured legal documents (e.g., constitutions, prospectuses, shareholder agreements) into a structured graph using custom document splitters, entity/relation extraction, and semantic linking.\n",
    "\n",
    "- Run Louvain Community Detection: Identify hierarchical legal taxonomies by discovering clusters of semantically or referentially related legal content.\n",
    "\n",
    "- Label and Analyze Communities: Use generative AI to assign interpretable labels to communities of legal texts, forming a multidimensional taxonomy for fund review reports.\n",
    "\n",
    "- GraphRAG : Use a first version of GraphRAG to query the knowledge graph and retrieve legal texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2262444d",
   "metadata": {},
   "source": [
    "## Constitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5000124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.experimental.components.text_splitters.base import TextSplitter\n",
    "from neo4j_graphrag.experimental.components.types import TextChunk, TextChunks\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "import asyncio\n",
    "\n",
    "@dataclass\n",
    "class HierarchyLevel:\n",
    "    \"\"\"Defines a hierarchy level with its regular expression extraction and metadata key.\"\"\"\n",
    "    name: str\n",
    "    pattern: str\n",
    "    metadata_key: str\n",
    "    title_key: Optional[str] = None\n",
    "    strip_header_pattern: Optional[str] = None\n",
    "\n",
    "class ConstitutionSplitter(TextSplitter):\n",
    "    \"\"\"Splitter for constitution documents.\"\"\"\n",
    "\n",
    "    def __init__(self, llm, graph_id: str, overlap_percentage: float = 0.2) -> None:\n",
    "        self.llm = llm\n",
    "        self.graph_id = graph_id\n",
    "        #self.overlap_percentage = max(0.0, min(1.0, overlap_percentage))\n",
    "        \n",
    "        # Define the hierarchy levels in order : section -> article\n",
    "        self.hierarchy_levels = [\n",
    "            HierarchyLevel(\n",
    "                name=\"section\",\n",
    "                pattern=r'^Section\\s+(\\d+)\\s+[â€“\\-]\\s+([^\\n]+)',\n",
    "                metadata_key=\"section_num\",\n",
    "                title_key=\"section_title\",\n",
    "                strip_header_pattern=r'^Section\\s+{num}\\s+[â€“\\-]\\s+{title}\\n?'\n",
    "            ),\n",
    "            HierarchyLevel(\n",
    "                name=\"article\", \n",
    "                pattern=r'^(?:[ \\t]*)(\\d+)\\s+[â€“\\-]\\s+([^\\n]+)',\n",
    "                metadata_key=\"article_num\",\n",
    "                title_key=\"article_title\",\n",
    "                strip_header_pattern=r'^{num}\\s+[â€“\\-]\\s+{title}\\n?'\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        model_name = self.llm.model_name\n",
    "        try:\n",
    "            self.tokenizer = tiktoken.encoding_for_model(model_name)\n",
    "        except KeyError:\n",
    "            self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    def split_text(self, text: str) -> TextChunks:\n",
    "        \"\"\"Synchronous wrapper for the main async processing method.\"\"\"\n",
    "        try:\n",
    "            loop = asyncio.get_running_loop()\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "        return asyncio.run(self.run(text.strip()))\n",
    "\n",
    "    async def run(self, text: str) -> TextChunks:\n",
    "        \"\"\"Main processing method using recursive hierarchy processing.\"\"\"\n",
    "        \n",
    "        cleaned_text = re.sub(r'\\\\n?', '', text).strip()\n",
    "        \n",
    "        #header_info = self._extract_header(cleaned_text)\n",
    "        header_info = {\n",
    "            \"document_id\": self.graph_id,\n",
    "            \"source\": \"legal_document\",\n",
    "            \"document_type\": \"legal_constitution\",\n",
    "            \"document_title\": f\"Legal Document {self.graph_id}\"\n",
    "        }\n",
    "        base_metadata = {**header_info, \"graph_id\": self.graph_id}\n",
    "        \n",
    "        chunks = []\n",
    "        chunk_index = [0]  # Use list to allow modification in nested calls\n",
    "        \n",
    "        \n",
    "        # Process hierarchical content recursively\n",
    "        self._process_hierarchy_level(\n",
    "            content=cleaned_text,\n",
    "            level_index=0,\n",
    "            metadata=base_metadata,\n",
    "            chunks=chunks,\n",
    "            chunk_index=chunk_index\n",
    "        )\n",
    "        \n",
    "        return TextChunks(chunks=chunks)\n",
    "\n",
    "\n",
    "    def _process_hierarchy_level(self, content: str, level_index: int, metadata: dict, \n",
    "                                chunks: List, chunk_index: List[int]):\n",
    "        \"\"\"\n",
    "        Process recursively a hierarchy level. \n",
    "        Args:\n",
    "            content: Text content to process at this level\n",
    "            level_index: Current hierarchy level (0=section, 1=article)\n",
    "            metadata: Accumulated metadata from parent levels\n",
    "            chunks: List of chunks\n",
    "            chunk_index: Current chunk index\n",
    "        \"\"\"\n",
    "        \n",
    "        # Base case: no more hierarchy levels to process\n",
    "        if level_index >= len(self.hierarchy_levels):\n",
    "            if content.strip():\n",
    "                chunks.append(self._create_chunk(chunk_index[0], content, metadata))\n",
    "                chunk_index[0] += 1\n",
    "            return\n",
    "        \n",
    "        current_level = self.hierarchy_levels[level_index]\n",
    "        items = self._extract_items_at_level(content, current_level)\n",
    "        \n",
    "        # If no items found at this level, try the next level or create chunk\n",
    "        if not items:\n",
    "            self._process_hierarchy_level(content, level_index + 1, metadata, chunks, chunk_index)\n",
    "            return\n",
    "        \n",
    "        # Process each item at this level\n",
    "        for item_id, item_title, item_content in items:\n",
    "            # Build metadata for this level, {**metadata} to do a shallow copy\n",
    "            current_metadata = {**metadata}\n",
    "            current_metadata[current_level.metadata_key] = item_id\n",
    "            if current_level.title_key and item_title:\n",
    "                current_metadata[current_level.title_key] = item_title\n",
    "            current_metadata[\"chunk_type\"] = current_level.name\n",
    "            \n",
    "            # Strip header if needed\n",
    "            processed_content = self._strip_level_header(\n",
    "                item_content, current_level, item_id, item_title\n",
    "            )\n",
    "            \n",
    "            # Recursively process the next level\n",
    "            self._process_hierarchy_level(\n",
    "                content=processed_content,\n",
    "                level_index=level_index + 1,\n",
    "                metadata=current_metadata,\n",
    "                chunks=chunks,\n",
    "                chunk_index=chunk_index\n",
    "            )\n",
    "\n",
    "    def _extract_items_at_level(self, content: str, level: HierarchyLevel) -> List[Tuple[str, Optional[str], str]]:\n",
    "        \"\"\"\n",
    "        Extract items at a specific hierarchy level. An item can be a section or an article.\n",
    "        Args:\n",
    "            content: Text content to search for items\n",
    "            level: HierarchyLevel object defining the pattern and metadata keys\n",
    "        \n",
    "        Returns:\n",
    "            List of (item_id, item_title, item_content) tuples\n",
    "        \"\"\"\n",
    "        \n",
    "        matches = list(re.finditer(level.pattern, content, re.MULTILINE))\n",
    "        if not matches:\n",
    "            return []\n",
    "        \n",
    "        items = []\n",
    "        for i, match in enumerate(matches):\n",
    "            # Extract ID and title based on pattern groups\n",
    "            if level.title_key:\n",
    "                item_id, item_title = match.group(1), match.group(2).strip() # group(1) is ID, group(2) is title\n",
    "            else:  # Only has ID (paragraphs)\n",
    "                if match.lastindex >= 1:\n",
    "                    item_id = match.group(1)\n",
    "                else:\n",
    "                    match.group('id')\n",
    "                item_title = None\n",
    "            \n",
    "            # Extract content block\n",
    "            start_pos = match.start()\n",
    "            if i < len(matches) - 1:\n",
    "                # Get content until the next match\n",
    "                end_pos = matches[i + 1].start()\n",
    "            else:\n",
    "                end_pos = len(content)\n",
    "                \n",
    "            item_content = content[start_pos:end_pos].strip()\n",
    "            \n",
    "            if item_content:\n",
    "                items.append((item_id, item_title, item_content))\n",
    "        \n",
    "        return items\n",
    "\n",
    "    def _strip_level_header(self, content: str, level: HierarchyLevel, \n",
    "                           item_id: str, item_title: Optional[str]) -> str:\n",
    "        \"\"\"Strip the header from content at a specific level.\"\"\"\n",
    "        if not level.strip_header_pattern:\n",
    "            # For enumerated items, strip the leading pattern\n",
    "            pattern = level.pattern\n",
    "            return re.sub(pattern, '', content, count=1, flags=re.MULTILINE).strip()\n",
    "        \n",
    "        # For sections/articles with titles\n",
    "        if item_title:\n",
    "            pattern = level.strip_header_pattern.format(\n",
    "                num=re.escape(item_id),\n",
    "                title=re.escape(item_title)\n",
    "            )\n",
    "        else:\n",
    "            pattern = level.strip_header_pattern.format(num=re.escape(item_id))\n",
    "        \n",
    "        return re.sub(pattern, '', content, count=1, flags=re.MULTILINE).strip()\n",
    "\n",
    "    def _extract_header(self, text: str) -> Dict[str, str]:\n",
    "        \"\"\"Extract document header information with required fields for lexical graph.\"\"\"\n",
    "        header = {\n",
    "            \"document_id\": self.graph_id,\n",
    "            \"source\": \"legal_document\",\n",
    "            \"document_type\": \"legal_constitution\"\n",
    "        }\n",
    "        \n",
    "        return header\n",
    "\n",
    "    def _create_chunk(self, index: int, text: str, metadata: dict) -> TextChunk:\n",
    "        \"\"\"Creates a TextChunk with cleaned text and hierarchical metadata.\"\"\"\n",
    "        \n",
    "        # Clean the text\n",
    "        clean_text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Determine level\n",
    "        chunk_type = metadata.get(\"chunk_type\", \"content\")\n",
    "        level_mapping = {\n",
    "            \"document_root\": \"document\",\n",
    "            \"section\": \"section\",\n",
    "            \"article\": \"article\", \n",
    "        }\n",
    "        level = level_mapping.get(chunk_type, chunk_type)\n",
    "        \n",
    "        # Build hierarchical path\n",
    "        path_parts = self._build_hierarchical_path(metadata, level)\n",
    "        \n",
    "        final_metadata = {\n",
    "            **metadata,\n",
    "            \"level\": level,\n",
    "            \"hierarchical_path\": \" â†’ \".join(path_parts) if path_parts else \"Document Content\",\n",
    "        }\n",
    "        \n",
    "        return TextChunk(index=index, text=clean_text, metadata=final_metadata)\n",
    "\n",
    "    def _build_hierarchical_path(self, metadata: dict, level: str) -> List[str]:\n",
    "        \"\"\"Build the hierarchical path for a chunk.\"\"\"\n",
    "        path_parts = []\n",
    "        \n",
    "        if level == \"document\" and \"document_title\" in metadata:\n",
    "            path_parts.append(metadata[\"document_title\"])\n",
    "        else:\n",
    "            if \"section_title\" in metadata:\n",
    "                path_parts.append(f\"Section {metadata.get('section_num','?')} â€“ {metadata.get('section_title','Untitled Section')}\")\n",
    "            if \"article_title\" in metadata:\n",
    "                path_parts.append(f\"Article {metadata.get('article_num','?')} â€“ {metadata.get('article_title','Untitled Article')}\")\n",
    "        \n",
    "        return path_parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554f75a2",
   "metadata": {},
   "source": [
    "# Prospectus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "XzObjqtykvUe",
   "metadata": {
    "executionInfo": {
     "elapsed": 82,
     "status": "ok",
     "timestamp": 1747302409602,
     "user": {
      "displayName": "Arnaud Crucifix",
      "userId": "14208215153378642511"
     },
     "user_tz": -120
    },
    "id": "XzObjqtykvUe"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import asyncio\n",
    "from typing import List, Dict\n",
    "import tiktoken\n",
    "from neo4j_graphrag.experimental.components.text_splitters.base import TextSplitter\n",
    "from neo4j_graphrag.experimental.components.types import TextChunk, TextChunks\n",
    "\n",
    "class ProspectusSplitter(TextSplitter):\n",
    "    \"\"\"Splitter for prospectus documents.\"\"\"\n",
    "\n",
    "    def __init__(self, llm, graph_id: str, overlap_percentage: float = 0.2) -> None:\n",
    "        self.llm = llm\n",
    "        self.graph_id = graph_id\n",
    "        self.overlap_percentage = overlap_percentage\n",
    "        if not 0.0 <= overlap_percentage <= 1.0:\n",
    "            raise ValueError(\"overlap_percentage must be between 0.0 and 1.0\")\n",
    "\n",
    "        # Get the encoding for the model being used\n",
    "        model_name = self.llm.model_name\n",
    "        try:\n",
    "            self.tokenizer = tiktoken.encoding_for_model(model_name)\n",
    "        except KeyError:\n",
    "            # Fallback to cl100k_base encoding (used by many OpenAI models)\n",
    "            self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "        # Document structure regular expressions\n",
    "        self.patterns = {\n",
    "            # Major sections with all-caps titles (e.g., \"UNDERSTANDING INVESTMENT POWERS\")\n",
    "            'major_section': r'(?:\\n|\\A)([A-Z][A-Z\\s]+[A-Z])(?:\\n|$)(.*?)(?=(?:\\n|\\A)[A-Z][A-Z\\s]+[A-Z](?:\\n|$)|\\Z)',\n",
    "\n",
    "            # Sub-sections with Title Case (e.g., \"What to Know About Investment Policy\")\n",
    "            'sub_section': r'(?:\\n|\\A)([A-Z][a-zA-Z\\s]+[a-zA-Z])[:\\.\\n](.*?)(?=(?:\\n|\\A)[A-Z][a-zA-Z\\s]+[a-zA-Z][:\\.\\n]|\\Z)',\n",
    "\n",
    "            # Tables that often begin with headers and have structured data\n",
    "            'table': r'(?:\\n|\\A)([A-Za-z\\s]+)\\n((?:[^\\n]+\\n)+?(?=\\n|\\Z))',\n",
    "\n",
    "            # Definitions (e.g., \"Class: Limited partner interests...\")\n",
    "            'definition': r'(?:\\n|\\A)([A-Za-z\\s]+)\\.\\s+(.*?)(?=(?:\\n|\\A)[A-Za-z\\s]+\\.\\s+|\\Z)',\n",
    "\n",
    "            # Bullet points\n",
    "            'bullet_points': r'(?:\\n|\\A)(?:â€¢|\\*|\\-)\\s+(.*?)(?=(?:\\n|\\A)(?:â€¢|\\*|\\-)|\\Z)',\n",
    "\n",
    "            # For numbered items\n",
    "            'numbered_item': r'(?:\\n|\\A)(\\d+\\.)\\s+(.*?)(?=(?:\\n|\\A)\\d+\\.|\\Z)',\n",
    "        }\n",
    "\n",
    "    async def run(self, text: str) -> TextChunks:\n",
    "        # Identify the document type and extract metadata\n",
    "        document_metadata = self._extract_document_metadata(text)\n",
    "\n",
    "        # Extract major sections from the document\n",
    "        major_sections = self._extract_major_sections(text)\n",
    "\n",
    "        # If no clear major sections found, fall back to sub-sections\n",
    "        if not major_sections:\n",
    "            major_sections = self._extract_sub_sections(text)\n",
    "\n",
    "        # If still no clear structure, fallback to paragraphs\n",
    "        if not major_sections:\n",
    "            major_sections = self._extract_paragraphs(text)\n",
    "\n",
    "        \n",
    "        all_chunks = []\n",
    "        # Process each section into chunks\n",
    "        for section_idx, (section_title, section_content) in enumerate(major_sections):\n",
    "            # Add metadata about the section\n",
    "            metadata = {\n",
    "                **document_metadata,\n",
    "                \"graph_id\": self.graph_id,\n",
    "                \"section_title\": section_title,\n",
    "                \"section_index\": section_idx,\n",
    "            }\n",
    "\n",
    "            # Tokenize the section\n",
    "            tokens = self.tokenizer.encode(section_content)\n",
    "            max_tokens = min(\n",
    "                4096,  # Default context window\n",
    "                self.llm.model_params.get(\"max_tokens\", 3000) * 3,\n",
    "            )\n",
    "\n",
    "            # If the section fits in one chunk\n",
    "            if len(tokens) <= max_tokens:\n",
    "                all_chunks.append(\n",
    "                    TextChunk(\n",
    "                        index=len(all_chunks),\n",
    "                        text=section_content,\n",
    "                        metadata=metadata\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                # Need to split the section, try by sub-sections first\n",
    "                sub_sections = self._extract_sub_sections(section_content)\n",
    "\n",
    "                if sub_sections:\n",
    "                    # Process each sub-section\n",
    "                    current_chunk_text = \"\"\n",
    "                    current_chunk_tokens = 0\n",
    "                    subsection_metadata = metadata.copy()\n",
    "                    subsection_metadata[\"subsections\"] = []\n",
    "\n",
    "                    for subsec_title, subsec_content in sub_sections:\n",
    "                        subsec_tokens = self.tokenizer.encode(subsec_content)\n",
    "\n",
    "                        # If adding this subsection would exceed max tokens, create a new chunk\n",
    "                        if current_chunk_tokens + len(subsec_tokens) > max_tokens and current_chunk_text:\n",
    "                            all_chunks.append(\n",
    "                                TextChunk(\n",
    "                                    index=len(all_chunks),\n",
    "                                    text=current_chunk_text,\n",
    "                                    metadata=subsection_metadata\n",
    "                                )\n",
    "                            )\n",
    "                            current_chunk_text = f\"[Continued from {section_title}]\\n\\n\" # Indicate continuation from prev chunk\n",
    "                            current_chunk_tokens = len(self.tokenizer.encode(current_chunk_text))\n",
    "                            subsection_metadata = metadata.copy()\n",
    "                            subsection_metadata[\"continued\"] = True\n",
    "                            subsection_metadata[\"subsections\"] = []\n",
    "\n",
    "                        # Add the subsection to the current chunk\n",
    "                        current_chunk_text += subsec_content + \"\\n\\n\"\n",
    "                        current_chunk_tokens += len(subsec_tokens) + len(self.tokenizer.encode(\"\\n\\n\"))\n",
    "\n",
    "                        # Track subsection information\n",
    "                        subsection_metadata[\"subsections\"].append(subsec_title)\n",
    "\n",
    "                    # Add the final chunk if there is a content\n",
    "                    if current_chunk_text:\n",
    "                        all_chunks.append(\n",
    "                            TextChunk(\n",
    "                                index=len(all_chunks),\n",
    "                                text=current_chunk_text,\n",
    "                                metadata=subsection_metadata\n",
    "                            )\n",
    "                        )\n",
    "                else:\n",
    "                    # No subsection found : try bullet points or fall back to overlapping chunks\n",
    "                    bullet_points = self._extract_bullet_points(section_content)\n",
    "\n",
    "                    if bullet_points and len(bullet_points) > 1:\n",
    "                        # Process bullet points as chunks\n",
    "                        current_chunk_text = \"\"\n",
    "                        current_chunk_tokens = 0\n",
    "                        bullet_metadata = metadata.copy()\n",
    "                        bullet_metadata[\"bullet_points\"] = True\n",
    "\n",
    "                        for bullet_title, bullet_content in bullet_points:\n",
    "                            bullet_tokens = self.tokenizer.encode(bullet_content)\n",
    "\n",
    "                            # If adding this bullet exceed max tokens, create a new chunk\n",
    "                            if current_chunk_tokens + len(bullet_tokens) > max_tokens and current_chunk_text:\n",
    "                                all_chunks.append(\n",
    "                                    TextChunk(\n",
    "                                        index=len(all_chunks),\n",
    "                                        text=current_chunk_text,\n",
    "                                        metadata=bullet_metadata\n",
    "                                    )\n",
    "                                )\n",
    "                                current_chunk_text = f\"[Continued from {section_title} - Bullet Points]\\n\\n\"\n",
    "                                current_chunk_tokens = len(self.tokenizer.encode(current_chunk_text))\n",
    "                                bullet_metadata = metadata.copy()\n",
    "                                bullet_metadata[\"continued\"] = True\n",
    "                                bullet_metadata[\"bullet_points\"] = True\n",
    "\n",
    "                            # Add the bullet to the current chunk\n",
    "                            current_chunk_text += bullet_content + \"\\n\\n\"\n",
    "                            current_chunk_tokens += len(bullet_tokens) + len(self.tokenizer.encode(\"\\n\\n\"))\n",
    "\n",
    "                        # Add the final chunk if there's content\n",
    "                        if current_chunk_text:\n",
    "                            all_chunks.append(\n",
    "                                TextChunk(\n",
    "                                    index=len(all_chunks),\n",
    "                                    text=current_chunk_text,\n",
    "                                    metadata=bullet_metadata\n",
    "                                )\n",
    "                            )\n",
    "                    else:\n",
    "                        # Fall back to overlapping chunks\n",
    "                        chunk_size = int(max_tokens * (1 - self.overlap_percentage))\n",
    "                        overlap_size = int(max_tokens * self.overlap_percentage)\n",
    "\n",
    "                        for i in range(0, len(tokens), chunk_size):\n",
    "                            start_index = max(0, i)\n",
    "                            end_index = min(i + chunk_size + overlap_size, len(tokens))\n",
    "                            current_chunk_tokens = tokens[start_index:end_index]\n",
    "                            current_chunk_text = self.tokenizer.decode(current_chunk_tokens)\n",
    "\n",
    "                            chunk_metadata = metadata.copy()\n",
    "                            if i > 0:\n",
    "                                chunk_metadata[\"continued\"] = True\n",
    "                                current_chunk_text = f\"[Continued from {section_title}]\\n\\n{current_chunk_text}\"\n",
    "\n",
    "                            all_chunks.append(\n",
    "                                TextChunk(\n",
    "                                    index=len(all_chunks),\n",
    "                                    text=current_chunk_text,\n",
    "                                    metadata=chunk_metadata\n",
    "                                )\n",
    "                            )\n",
    "\n",
    "        # Identify and mark special chunks like tables, definitions.\n",
    "        all_chunks = self._mark_special_chunks(all_chunks)\n",
    "\n",
    "        return TextChunks(chunks=all_chunks)\n",
    "\n",
    "    def _extract_document_metadata(self, text: str) -> Dict:\n",
    "        \"\"\"Extract key metadata from the document.\"\"\"\n",
    "        \n",
    "        metadata = {\n",
    "            \"document_type\": \"Offering Document\",\n",
    "            \"document_title\": \"\"\n",
    "        }\n",
    "\n",
    "        # Try to extract the document title\n",
    "        title_pattern = r'(?:\\A|\\n)([^\\n]{5,150}(?:Fund|Offering|Prospectus|Document)[^\\n]{0,50})'\n",
    "        title_match = re.search(title_pattern, text[:1000], re.IGNORECASE)\n",
    "        if title_match:\n",
    "            metadata[\"document_title\"] = title_match.group(1).strip()\n",
    "\n",
    "        # Try to extract the fund name\n",
    "        fund_pattern = r'(?:\\A|\\n)([A-Za-z0-9\\s]+(?:Fund|Partnership|Trust|Company))'\n",
    "        fund_match = re.search(fund_pattern, text[:1000], re.IGNORECASE)\n",
    "        if fund_match:\n",
    "            metadata[\"fund_name\"] = fund_match.group(1).strip()\n",
    "\n",
    "        return metadata\n",
    "\n",
    "    def _extract_major_sections(self, text: str) -> List[tuple]:\n",
    "        \"\"\"Extract major sections from the document.\"\"\"\n",
    "        \n",
    "        matches = re.finditer(self.patterns['major_section'], text, re.DOTALL)\n",
    "        sections = []\n",
    "\n",
    "        prev_end = 0\n",
    "        for match in matches:\n",
    "            # If there is a content before the first section, add it\n",
    "            if prev_end == 0 and match.start() > 0:\n",
    "                intro_content = text[:match.start()].strip()\n",
    "                if intro_content:\n",
    "                    sections.append((\"Introduction\", intro_content))\n",
    "\n",
    "            section_title = match.group(1).strip()\n",
    "            section_content = match.group(0).strip()\n",
    "\n",
    "            sections.append((section_title, section_content))\n",
    "            prev_end = match.end()\n",
    "\n",
    "        # If there's a content after the last section, add it\n",
    "        if prev_end < len(text) and text[prev_end:].strip():\n",
    "            sections.append((\"Additional information\", text[prev_end:].strip()))\n",
    "\n",
    "        return sections\n",
    "\n",
    "    def _extract_sub_sections(self, text: str) -> List[tuple]:\n",
    "        \"\"\"Extract sub-sections from a section of text.\"\"\"\n",
    "        \n",
    "        matches = re.finditer(self.patterns['sub_section'], text, re.DOTALL)\n",
    "        sub_sections = []\n",
    "\n",
    "        for match in matches:\n",
    "            subsec_title = match.group(1).strip()\n",
    "            subsec_content = match.group(0).strip()\n",
    "\n",
    "            sub_sections.append((subsec_title, subsec_content))\n",
    "\n",
    "        return sub_sections\n",
    "    \n",
    "    \n",
    "    def _extract_paragraphs(self, text: str) -> List[tuple]:\n",
    "        \"\"\"Extract paragraphs when no clear section structure is found.\"\"\"\n",
    "        \n",
    "        paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "        result = []\n",
    "\n",
    "        for i, para in enumerate(paragraphs):\n",
    "            if para.strip():\n",
    "                # Try to a title from the first line\n",
    "                lines = para.strip().split('\\n')\n",
    "                if lines and len(lines[0]) < 100:\n",
    "                    title = lines[0].strip()\n",
    "                else:\n",
    "                    words = para.strip().split()[:5]\n",
    "                    \n",
    "                    if words:\n",
    "                        title = \" \".join(words) + \"...\"\n",
    "                    else:\n",
    "                        f\"Paragraph {i+1}\"\n",
    "\n",
    "                result.append((title, para.strip()))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _extract_bullet_points(self, text: str) -> List[tuple]:\n",
    "        \"\"\"Extract bullet points from a text section.\"\"\"\n",
    "        \n",
    "        matches = re.finditer(self.patterns['bullet_points'], text, re.DOTALL)\n",
    "        bullets = []\n",
    "\n",
    "        for i, match in enumerate(matches):\n",
    "            bullet_content = match.group(0).strip()\n",
    "            bullet_text = match.group(1).strip()\n",
    "\n",
    "            words = bullet_text.split()\n",
    "            title = \" \".join(words[:min(5, len(words))]) + \"...\"\n",
    "\n",
    "            bullets.append((f\"Bullet {i+1}: {title}\", bullet_content))\n",
    "\n",
    "        return bullets\n",
    "\n",
    "\n",
    "    def _mark_special_chunks(self, chunks: List[TextChunk]) -> List[TextChunk]:\n",
    "        \"\"\"Identify chunks that contain tables, definitions, or numbered items.\"\"\"\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            # Check for tables\n",
    "            table_matches = re.findall(r'(?:\\n|\\A)([A-Za-z\\s]+)\\n((?:[^\\n]+\\n)+?(?=\\n|\\Z))', chunk.text, re.DOTALL)\n",
    "            if table_matches:\n",
    "                if chunk.metadata is None:\n",
    "                    chunk.metadata = {}\n",
    "                chunk.metadata[\"contains_tables\"] = True\n",
    "                chunk.metadata[\"table_titles\"] = [title.strip() for title, _ in table_matches]\n",
    "\n",
    "            # Check for definitions\n",
    "            definitions = []\n",
    "            for match in re.finditer(self.patterns['definition'], chunk.text, re.DOTALL):\n",
    "                term = match.group(1)\n",
    "                definition = match.group(2)\n",
    "                if term and definition:\n",
    "                    definitions.append((term.strip(), definition.strip()))\n",
    "\n",
    "            if definitions:\n",
    "                if chunk.metadata is None:\n",
    "                    chunk.metadata = {}\n",
    "                chunk.metadata[\"contains_definitions\"] = True\n",
    "                chunk.metadata[\"definitions\"] = [term for term, _ in definitions]\n",
    "\n",
    "            # Check for numbered items\n",
    "            numbered_items = re.findall(self.patterns['numbered_item'], chunk.text, re.DOTALL)\n",
    "            if numbered_items:\n",
    "                if chunk.metadata is None:\n",
    "                    chunk.metadata = {}\n",
    "                chunk.metadata[\"contains_numbered_items\"] = True\n",
    "                chunk.metadata[\"item_count\"] = len(numbered_items)\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def split_text(self, text: str) -> TextChunks:\n",
    "        \"\"\"Synchronously calls the async run method.\"\"\"\n",
    "        return asyncio.run(self.run(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749ca0b9",
   "metadata": {},
   "source": [
    "# Shareholders Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8afba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Tuple, Dict, Any\n",
    "import re\n",
    "import asyncio\n",
    "import tiktoken\n",
    "\n",
    "@dataclass\n",
    "class HierarchyLevel:\n",
    "    \"\"\"Defines a hierarchy level with its regular expression extraction and metadata key.\"\"\"\n",
    "    name: str\n",
    "    pattern: str\n",
    "    metadata_key: str\n",
    "    title_key: Optional[str] = None\n",
    "    strip_header_pattern: Optional[str] = None\n",
    "    level_number: int = 0  # Added this missing field\n",
    "\n",
    "\n",
    "class ShareholdersAgreementSplitter(TextSplitter):\n",
    "    \"\"\"Improved splitter for shareholders documents using recursive hierarchy processing.\"\"\"\n",
    "\n",
    "    def __init__(self, llm, graph_id: str, overlap_percentage: float = 0.2) -> None:\n",
    "        self.llm = llm\n",
    "        self.graph_id = graph_id\n",
    "        self.overlap_percentage = overlap_percentage\n",
    "        if not 0.0 <= overlap_percentage <= 1.0:\n",
    "            raise ValueError(\"overlap_percentage must be between 0.0 and 1.0\")\n",
    "\n",
    "        # Define the hierarchy levels in order of nesting\n",
    "        self.hierarchy_levels = [\n",
    "            # Level 1: Main numbered clauses (e.g., \"1. Definitions and Interpretation\")\n",
    "            HierarchyLevel(\n",
    "                name=\"main_clause\",\n",
    "                pattern=r'^(\\d+)\\.\\s+([A-Z][A-Za-z0-9\\s\\-\\(\\),;&\\']+?)(?:\\n|$)',\n",
    "                metadata_key=\"main_clause_num\",\n",
    "                title_key=\"main_clause_title\",\n",
    "                strip_header_pattern=r'^{num}\\.\\s+{title}\\n?',\n",
    "                level_number=1\n",
    "            ),\n",
    "            # Level 2: First-level sub-clauses (e.g., \"1.1 Definitions\")\n",
    "            HierarchyLevel(\n",
    "                name=\"sub_clause\",\n",
    "                pattern=r'^(\\d+\\.\\d+)\\s+([A-Z][A-Za-z0-9\\s\\-\\(\\),;&\\']+?)(?:\\n|$)',\n",
    "                metadata_key=\"sub_clause_num\",\n",
    "                title_key=\"sub_clause_title\",\n",
    "                strip_header_pattern=r'^{num}\\s+{title}\\n?',\n",
    "                level_number=2\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Regular expressions for recitals, schedules and annexures\n",
    "        self.special_patterns = {\n",
    "            'recital': r'^([A-Z])\\.\\s+(.*)',\n",
    "            'schedule': r'^Schedule\\s+(\\d+)\\s*([A-Za-z0-9\\s\\-\\(\\)]*)',\n",
    "            'annexure': r'^(?:Annexure|ANNEXURE)\\s+([A-Z])[:\\s]*([^\\n]*)',\n",
    "            'schedule_part': r'^PART\\s+([A-Z]+)'\n",
    "        }\n",
    "\n",
    "        # TOC patterns for filtering\n",
    "        self.toc_patterns = [\n",
    "            r'^(\\d+(?:\\.\\d+)*)\\.\\s+([^\\n]+?)\\.{2,}\\s*\\d+\\s*$',\n",
    "            r'^(\\d+(?:\\.\\d+)*)\\.\\s+([A-Za-z][A-Za-z0-9\\s\\-\\(\\),;&\\']+?)\\s+\\d+\\s*$',\n",
    "            r'^(\\d+(?:\\.\\d+)*)\\.\\s+([^\\t\\n]+?)\\t+\\d+\\s*$',\n",
    "            r'^(\\d+(?:\\.\\d+)*)\\.\\s+([A-Za-z][^\\n]+?)\\s{2,}\\d+\\s*$',\n",
    "            r'^([ivxlcdm]+)\\.\\s+([A-Za-z][^\\n]+?)\\s+\\d+\\s*$',\n",
    "            r'^([A-Z])\\.\\s+([A-Za-z][^\\n]+?)\\s+\\d+\\s*$'\n",
    "        ]\n",
    "\n",
    "        model_name = self.llm.model_name\n",
    "        try:\n",
    "            self.tokenizer = tiktoken.encoding_for_model(model_name)\n",
    "        except KeyError:\n",
    "            self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    def split_text(self, text: str) -> TextChunks:\n",
    "        \"\"\"Synchronous wrapper for the main async processing method.\"\"\"\n",
    "        return asyncio.run(self.run(text))\n",
    "\n",
    "    async def run(self, text: str) -> TextChunks:\n",
    "        \"\"\"Main processing method using recursive hierarchy processing.\"\"\"\n",
    "        \n",
    "        # Normalize line endings and remove TOC\n",
    "        cleaned_text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "        filtered_text = self._extract_non_toc_text(cleaned_text)\n",
    "        \n",
    "        # Extract header information\n",
    "        header_info = self._extract_document_header(filtered_text)\n",
    "        base_metadata = {**header_info, \"graph_id\": self.graph_id}\n",
    "        \n",
    "        chunks = []\n",
    "        chunk_index = [0]\n",
    "        \n",
    "        # First, process recitals separately\n",
    "        recitals_text, main_content = self._separate_recitals(filtered_text)\n",
    "        if recitals_text:\n",
    "            self._process_recitals(recitals_text, base_metadata, chunks, chunk_index)\n",
    "        \n",
    "        # Then process main hierarchical content\n",
    "        if main_content:\n",
    "            self._process_hierarchy_level(\n",
    "                content=main_content,\n",
    "                level_index=0,\n",
    "                metadata=base_metadata.copy(),\n",
    "                chunks=chunks,\n",
    "                chunk_index=chunk_index\n",
    "            )\n",
    "        \n",
    "        # Finally, process schedules and annexures\n",
    "        schedules_text = self._extract_schedules_content(filtered_text)\n",
    "        if schedules_text:\n",
    "            self._process_schedules(schedules_text, base_metadata, chunks, chunk_index)\n",
    "        \n",
    "        return TextChunks(chunks=chunks)\n",
    "\n",
    "    def _process_hierarchy_level(self, content: str, level_index: int, metadata: dict, \n",
    "                                chunks: List, chunk_index: List[int]):\n",
    "        \"\"\"\n",
    "        Recursively process a hierarchy level.\n",
    "        Args:\n",
    "            content: Text content to process at this level\n",
    "            level_index: Current hierarchy level index\n",
    "            metadata: Accumulated metadata from parent levels\n",
    "            chunks: List of chunks to append to\n",
    "            chunk_index: Current chunk index counter\n",
    "        \"\"\"\n",
    "        \n",
    "        # Base case: no more hierarchy levels to process\n",
    "        if level_index >= len(self.hierarchy_levels):\n",
    "            if content.strip():\n",
    "                chunk = self._create_chunk(chunk_index[0], content, metadata)\n",
    "                if self._should_split_chunk(chunk):\n",
    "                    sub_chunks = self._split_large_content(content, metadata, chunk_index[0])\n",
    "                    chunks.extend(sub_chunks)\n",
    "                    chunk_index[0] += len(sub_chunks)\n",
    "                else:\n",
    "                    chunks.append(chunk)\n",
    "                    chunk_index[0] += 1\n",
    "            return\n",
    "        \n",
    "        current_level = self.hierarchy_levels[level_index]\n",
    "        items = self._extract_items_at_level(content, current_level)\n",
    "        \n",
    "        # If no items found at this level, try the next level or create chunk\n",
    "        if not items:\n",
    "            self._process_hierarchy_level(content, level_index + 1, metadata, chunks, chunk_index)\n",
    "            return\n",
    "        \n",
    "        # Process each item at this level\n",
    "        for item_id, item_title, item_content in items:\n",
    "            # Build metadata for this level\n",
    "            current_metadata = {**metadata}\n",
    "            current_metadata[current_level.metadata_key] = item_id\n",
    "            if current_level.title_key and item_title:\n",
    "                current_metadata[current_level.title_key] = item_title\n",
    "            current_metadata[\"chunk_type\"] = current_level.name\n",
    "            current_metadata[\"level\"] = current_level.level_number\n",
    "            \n",
    "            # Add hierarchical path\n",
    "            current_metadata[\"hierarchical_path\"] = self._build_hierarchical_path(current_metadata)\n",
    "            \n",
    "            # Strip header if needed\n",
    "            processed_content = self._strip_level_header(\n",
    "                item_content, current_level, item_id, item_title\n",
    "            )\n",
    "            \n",
    "            # Recursively process the next level\n",
    "            self._process_hierarchy_level(\n",
    "                content=processed_content,\n",
    "                level_index=level_index + 1,\n",
    "                metadata=current_metadata,\n",
    "                chunks=chunks,\n",
    "                chunk_index=chunk_index\n",
    "            )\n",
    "\n",
    "    def _extract_items_at_level(self, content: str, level: HierarchyLevel) -> List[Tuple[str, Optional[str], str]]:\n",
    "        \"\"\"Extract items at a specific hierarchy level.\"\"\"\n",
    "        \n",
    "        matches = list(re.finditer(level.pattern, content, re.MULTILINE))\n",
    "        if not matches:\n",
    "            return []\n",
    "        \n",
    "        items = []\n",
    "        for i, match in enumerate(matches):\n",
    "            # Skip if this looks like a TOC entry\n",
    "            if self._is_toc_entry(match.group(0)):\n",
    "                continue\n",
    "                \n",
    "            # Extract ID and title based on pattern groups\n",
    "            if level.title_key:\n",
    "                item_id = match.group(1)\n",
    "                item_title = match.group(2).strip() if match.group(2) else None\n",
    "            else:\n",
    "                item_id = match.group(1)\n",
    "                item_title = None\n",
    "            \n",
    "            # Extract content block\n",
    "            start_pos = match.start()\n",
    "            if i < len(matches) - 1:\n",
    "                # Find next non-TOC match\n",
    "                next_match = None\n",
    "                for j in range(i + 1, len(matches)):\n",
    "                    if not self._is_toc_entry(matches[j].group(0)):\n",
    "                        next_match = matches[j]\n",
    "                        break\n",
    "                end_pos = next_match.start() if next_match else len(content)\n",
    "            else:\n",
    "                end_pos = len(content)\n",
    "                \n",
    "            item_content = content[start_pos:end_pos].strip()\n",
    "            \n",
    "            if item_content:\n",
    "                items.append((item_id, item_title, item_content))\n",
    "        \n",
    "        return items\n",
    "\n",
    "    def _strip_level_header(self, content: str, level: HierarchyLevel, \n",
    "                           item_id: str, item_title: Optional[str]) -> str:\n",
    "        \"\"\"Strip the header from content at a specific level.\"\"\"\n",
    "        if not level.strip_header_pattern:\n",
    "            return content\n",
    "        \n",
    "        if item_title and '{title}' in level.strip_header_pattern:\n",
    "            pattern = level.strip_header_pattern.format(\n",
    "                num=re.escape(item_id),\n",
    "                title=re.escape(item_title),\n",
    "                id=re.escape(item_id)\n",
    "            )\n",
    "        else:\n",
    "            pattern = level.strip_header_pattern.format(\n",
    "                num=re.escape(item_id),\n",
    "                id=re.escape(item_id)\n",
    "            )\n",
    "        \n",
    "        return re.sub(pattern, '', content, count=1, flags=re.MULTILINE).strip()\n",
    "\n",
    "    def _process_recitals(self, recitals_text: str, base_metadata: dict, \n",
    "                         chunks: List, chunk_index: List[int]):\n",
    "        \"\"\"Process recitals section.\"\"\"\n",
    "        recital_matches = list(re.finditer(self.special_patterns['recital'], recitals_text, re.MULTILINE))\n",
    "        \n",
    "        for match in recital_matches:\n",
    "            recital_id = match.group(1)\n",
    "            recital_content = match.group(2).strip()\n",
    "            \n",
    "            metadata = {\n",
    "                **base_metadata,\n",
    "                \"chunk_type\": \"recital\",\n",
    "                \"level\": 1,\n",
    "                \"recital_id\": recital_id,\n",
    "                \"hierarchical_path\": f\"Recital {recital_id}\"\n",
    "            }\n",
    "            \n",
    "            chunk = self._create_chunk(chunk_index[0], f\"Recital {recital_id}: {recital_content}\", metadata)\n",
    "            chunks.append(chunk)\n",
    "            chunk_index[0] += 1\n",
    "\n",
    "    def _process_schedules(self, schedules_text: str, base_metadata: dict, \n",
    "                          chunks: List, chunk_index: List[int]):\n",
    "        \"\"\"Process schedules and annexures.\"\"\"\n",
    "        schedule_sections = self._extract_schedule_sections(schedules_text)\n",
    "        \n",
    "        for section in schedule_sections:\n",
    "            metadata = {\n",
    "                **base_metadata,\n",
    "                \"chunk_type\": section[\"type\"],\n",
    "                \"level\": 1,\n",
    "                f\"{section['type']}_id\": section[\"id\"],\n",
    "                f\"{section['type']}_title\": section.get(\"title\", \"\"),\n",
    "                \"hierarchical_path\": f\"{section['type'].title()} {section['id']}\"\n",
    "            }\n",
    "            \n",
    "            if self._should_split_chunk_content(section[\"content\"]):\n",
    "                sub_chunks = self._split_large_content(section[\"content\"], metadata, chunk_index[0])\n",
    "                chunks.extend(sub_chunks)\n",
    "                chunk_index[0] += len(sub_chunks)\n",
    "            else:\n",
    "                chunk = self._create_chunk(chunk_index[0], section[\"content\"], metadata)\n",
    "                chunks.append(chunk)\n",
    "                chunk_index[0] += 1\n",
    "\n",
    "    def _separate_recitals(self, text: str) -> Tuple[str, str]:\n",
    "        \"\"\"Separate recitals from main content.\"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        recitals_lines = []\n",
    "        main_content_start = 0\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            if re.match(self.special_patterns['recital'], line.strip()):\n",
    "                recitals_lines.append(line)\n",
    "            elif re.match(r'^\\d+\\.\\s+', line.strip()) and recitals_lines:\n",
    "                main_content_start = i\n",
    "                break\n",
    "        \n",
    "        if recitals_lines:\n",
    "            recitals_text = '\\n'.join(recitals_lines)\n",
    "        else: \n",
    "            recitals_text = \"\"\n",
    "        main_content = '\\n'.join(lines[main_content_start:]) if main_content_start > 0 else text\n",
    "        \n",
    "        return recitals_text, main_content\n",
    "\n",
    "    def _extract_schedules_content(self, text: str) -> str:\n",
    "        \"\"\"Extract schedules and annexures content.\"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        schedule_start = None\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            if re.match(self.special_patterns['schedule'], line.strip()) or \\\n",
    "               re.match(self.special_patterns['annexure'], line.strip()):\n",
    "                schedule_start = i\n",
    "                break\n",
    "        \n",
    "        return '\\n'.join(lines[schedule_start:]) if schedule_start is not None else \"\"\n",
    "\n",
    "    def _extract_schedule_sections(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Extract individual schedule/annexure sections.\"\"\"\n",
    "        sections = []\n",
    "        lines = text.split('\\n')\n",
    "        current_section = None\n",
    "        \n",
    "        for line in lines:\n",
    "            line_stripped = line.strip()\n",
    "            \n",
    "            # Check for schedule\n",
    "            schedule_match = re.match(self.special_patterns['schedule'], line_stripped)\n",
    "            if schedule_match:\n",
    "                if current_section:\n",
    "                    sections.append(current_section)\n",
    "                current_section = {\n",
    "                    'type': 'schedule',\n",
    "                    'id': schedule_match.group(1),\n",
    "                    'title': schedule_match.group(2).strip() if schedule_match.group(2) else \"\",\n",
    "                    'content': line\n",
    "                }\n",
    "                continue\n",
    "            \n",
    "            # Check for annexure\n",
    "            annexure_match = re.match(self.special_patterns['annexure'], line_stripped)\n",
    "            if annexure_match:\n",
    "                if current_section:\n",
    "                    sections.append(current_section)\n",
    "                current_section = {\n",
    "                    'type': 'annexure',\n",
    "                    'id': annexure_match.group(1),\n",
    "                    'title': annexure_match.group(2).strip() if annexure_match.group(2) else \"\",\n",
    "                    'content': line\n",
    "                }\n",
    "                continue\n",
    "            \n",
    "            # Add content to current section\n",
    "            if current_section and line_stripped:\n",
    "                current_section['content'] += '\\n' + line\n",
    "        \n",
    "        if current_section:\n",
    "            sections.append(current_section)\n",
    "        \n",
    "        return sections\n",
    "\n",
    "    def _extract_non_toc_text(self, text: str) -> str:\n",
    "        \"\"\"Extract text excluding the Table of Contents section.\"\"\"\n",
    "        toc_start, toc_end = self._identify_toc_boundaries(text)\n",
    "        \n",
    "        if toc_start is None:\n",
    "            return text\n",
    "        \n",
    "        lines = text.split('\\n')\n",
    "        \n",
    "        if toc_end is not None:\n",
    "            before_toc = lines[:toc_start] if toc_start > 0 else []\n",
    "            after_toc = lines[toc_end + 1:] if toc_end + 1 < len(lines) else []\n",
    "            filtered_lines = before_toc + after_toc\n",
    "        else:\n",
    "            before_toc = lines[:toc_start] if toc_start > 0 else []\n",
    "            first_clause = self._find_first_real_clause_after_toc(lines, toc_start)\n",
    "            if first_clause is not None:\n",
    "                after_toc = lines[first_clause:]\n",
    "                filtered_lines = before_toc + after_toc\n",
    "            else:\n",
    "                filtered_lines = before_toc\n",
    "        \n",
    "        return '\\n'.join(filtered_lines)\n",
    "\n",
    "    def _identify_toc_boundaries(self, text: str) -> Tuple[Optional[int], Optional[int]]:\n",
    "        \"\"\"Identify the start and end boundaries of the Table of Contents.\"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        toc_start = None\n",
    "        toc_end = None\n",
    "        \n",
    "        # Look for TOC start\n",
    "        for i, line in enumerate(lines):\n",
    "            if re.search(r'(?i)table\\s+of\\s+contents?|^contents?$', line.strip()):\n",
    "                toc_start = i\n",
    "                break\n",
    "        \n",
    "        if toc_start is not None:\n",
    "            consecutive_toc_lines = 0\n",
    "            for i in range(toc_start + 1, min(toc_start + 100, len(lines))):\n",
    "                line_stripped = lines[i].strip()\n",
    "                if not line_stripped:\n",
    "                    continue\n",
    "                \n",
    "                if self._is_toc_entry(line_stripped):\n",
    "                    consecutive_toc_lines += 1\n",
    "                    toc_end = i\n",
    "                else:\n",
    "                    if consecutive_toc_lines > 0 and self._is_clause(line_stripped):\n",
    "                        break\n",
    "                    consecutive_toc_lines = 0\n",
    "        \n",
    "        return toc_start, toc_end\n",
    "\n",
    "    def _is_toc_entry(self, line: str) -> bool:\n",
    "        \"\"\"Check if a line appears to be a Table of Contents entry.\"\"\"\n",
    "        return any(re.match(pattern, line) for pattern in self.toc_patterns)\n",
    "\n",
    "    def _is_clause(self, line: str) -> bool:\n",
    "        \"\"\"Check if a line is a clause.\"\"\"\n",
    "        clause_patterns = [\n",
    "            r'^(\\d+)\\.\\s+([A-Z][A-Za-z0-9\\s\\-\\(\\),;&\\']+?)$',\n",
    "            r'^(\\d+\\.\\d+)\\s+([A-Z][A-Za-z0-9\\s\\-\\(\\),;&\\']+?)$',\n",
    "            r'^([A-Z])\\.\\s+([A-Za-z][^\\d]*?)$'\n",
    "        ]\n",
    "        \n",
    "        for pattern in clause_patterns:\n",
    "            match = re.match(pattern, line)\n",
    "            if match and not re.search(r'\\s+\\d+$', match.group(2) if len(match.groups()) > 1 else match.group(1)):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _find_first_real_clause_after_toc(self, lines: List[str], toc_start: int) -> Optional[int]:\n",
    "        \"\"\"Find the first real clause after TOC start.\"\"\"\n",
    "        for i in range(toc_start + 1, len(lines)):\n",
    "            if self._is_clause(lines[i].strip()):\n",
    "                return i\n",
    "        return None\n",
    "\n",
    "    def _extract_document_header(self, text: str) -> Dict[str, str]:\n",
    "        \"\"\"Extract document header information.\"\"\"\n",
    "        return {\n",
    "            \"document_type\": \"ShareholdersAgreement\",\n",
    "            \"document_title\": f\"Shareholders Agreement {self.graph_id}\",\n",
    "            \"source\": \"legal_document\"\n",
    "        }\n",
    "\n",
    "    def _build_hierarchical_path(self, metadata: dict) -> str:\n",
    "        \"\"\"Build the hierarchical path for a chunk.\"\"\"\n",
    "        path_parts = []\n",
    "        \n",
    "        if \"main_clause_num\" in metadata and \"main_clause_title\" in metadata:\n",
    "            path_parts.append(f\"Clause {metadata['main_clause_num']} â€“ {metadata['main_clause_title']}\")\n",
    "        \n",
    "        if \"sub_clause_num\" in metadata and \"sub_clause_title\" in metadata:\n",
    "            path_parts.append(f\"Sub-clause {metadata['sub_clause_num']} â€“ {metadata['sub_clause_title']}\")\n",
    "        \n",
    "        return \" â†’ \".join(path_parts) if path_parts else \"Document Content\"\n",
    "\n",
    "    def _create_chunk(self, index: int, text: str, metadata: dict) -> TextChunk:\n",
    "        \"\"\"Create a TextChunk with cleaned text and metadata.\"\"\"\n",
    "        clean_text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        final_metadata = {\n",
    "            **metadata,\n",
    "            \"component_type\": self._determine_component_type(metadata)\n",
    "        }\n",
    "        \n",
    "        return TextChunk(index=index, text=clean_text, metadata=final_metadata)\n",
    "\n",
    "    def _determine_component_type(self, metadata: dict) -> str:\n",
    "        \"\"\"Determine the component type based on metadata.\"\"\"\n",
    "        chunk_type = metadata.get(\"chunk_type\", \"content\")\n",
    "        \n",
    "        component_mapping = {\n",
    "            \"main_clause\": \"MainClause\",\n",
    "            \"sub_clause\": \"SubClause\", \n",
    "            \"recital\": \"Recital\",\n",
    "            \"schedule\": \"Schedule\",\n",
    "            \"annexure\": \"Annexure\"\n",
    "        }\n",
    "        \n",
    "        return component_mapping.get(chunk_type, \"Content\")\n",
    "\n",
    "    def _should_split_chunk(self, chunk: TextChunk) -> bool:\n",
    "        \"\"\"Check if a chunk should be split due to size.\"\"\"\n",
    "        tokens = self.tokenizer.encode(chunk.text)\n",
    "        max_tokens = min(4096, self.llm.model_params.get(\"max_tokens\", 3000) * 3)\n",
    "        return len(tokens) > max_tokens\n",
    "\n",
    "    def _should_split_chunk_content(self, content: str) -> bool:\n",
    "        \"\"\"Check if content should be split due to size.\"\"\"\n",
    "        tokens = self.tokenizer.encode(content)\n",
    "        max_tokens = min(4096, self.llm.model_params.get(\"max_tokens\", 3000) * 3)\n",
    "        return len(tokens) > max_tokens\n",
    "\n",
    "    def _split_large_content(self, content: str, metadata: dict, start_index: int) -> List[TextChunk]:\n",
    "        \"\"\"Split large content into smaller chunks with overlap.\"\"\"\n",
    "        tokens = self.tokenizer.encode(content)\n",
    "        max_tokens = min(4096, self.llm.model_params.get(\"max_tokens\", 3000) * 3)\n",
    "        \n",
    "        chunk_size = int(max_tokens * (1 - self.overlap_percentage))\n",
    "        overlap_size = int(max_tokens * self.overlap_percentage)\n",
    "        \n",
    "        chunks = []\n",
    "        for i in range(0, len(tokens), chunk_size):\n",
    "            start_idx = max(0, i)\n",
    "            end_idx = min(i + chunk_size + overlap_size, len(tokens))\n",
    "            chunk_tokens = tokens[start_idx:end_idx]\n",
    "            chunk_text = self.tokenizer.decode(chunk_tokens)\n",
    "            \n",
    "            chunk_metadata = metadata.copy()\n",
    "            if i > 0:\n",
    "                chunk_metadata[\"continued\"] = True\n",
    "                chunk_text = f\"[Continued from {metadata.get('hierarchical_path', 'previous section')}]\\n\\n{chunk_text}\"\n",
    "            \n",
    "            chunks.append(self._create_chunk(start_index + len(chunks), chunk_text, chunk_metadata))\n",
    "        \n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43909668",
   "metadata": {},
   "source": [
    "# Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4c5bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/jbarrasa/goingmeta/blob/main/session31/python/utils.py\n",
    "\n",
    "from neo4j_graphrag.experimental.components.schema import (\n",
    "    SchemaBuilder,\n",
    "    SchemaConfig,\n",
    "    SchemaEntity,\n",
    "    SchemaProperty,\n",
    "    SchemaRelation,\n",
    ")\n",
    "from rdflib.namespace import OWL, RDF, RDFS\n",
    "from rdflib import Graph\n",
    "\n",
    "def getLocalPart(uri):\n",
    "    pos = uri.rfind(\"#\")\n",
    "    if pos < 0:\n",
    "        pos = uri.rfind(\"/\")\n",
    "    if pos < 0:\n",
    "        pos = uri.rindex(\":\")\n",
    "    return uri[pos + 1 :]\n",
    "\n",
    "def getPropertiesForClass(g, cat):\n",
    "    props = []\n",
    "    for dtp in g.subjects(RDFS.domain, cat):\n",
    "        if (dtp, RDF.type, OWL.DatatypeProperty) in g:\n",
    "            propName = getLocalPart(dtp)\n",
    "            propDesc = next(g.objects(dtp, RDFS.comment), \"\")\n",
    "            props.append(SchemaProperty(name=propName, type=\"STRING\", description=propDesc))\n",
    "    return props\n",
    "\n",
    "def getSchemaFromOnto(g) -> SchemaConfig:\n",
    "    schema_builder = SchemaBuilder()\n",
    "    classes = {}\n",
    "    entities = []\n",
    "    rels = []\n",
    "    triples = []\n",
    "\n",
    "    for cat in g.subjects(RDF.type, OWL.Class):\n",
    "        classes[cat] = None\n",
    "        label = getLocalPart(cat)\n",
    "        props = getPropertiesForClass(g, cat)\n",
    "        entities.append(SchemaEntity(label=label, description=next(g.objects(cat, RDFS.comment), \"\"), properties=props))\n",
    "\n",
    "    for cat in g.objects(None, RDFS.domain):\n",
    "        if cat not in classes.keys():\n",
    "            classes[cat] = None\n",
    "            label = getLocalPart(cat)\n",
    "            props = getPropertiesForClass(g, cat)\n",
    "            entities.append(SchemaEntity(label=label, description=next(g.objects(cat, RDFS.comment), \"\"), properties=props))\n",
    "\n",
    "    for cat in g.objects(None, RDFS.range):\n",
    "        if not (cat.startswith(\"http://www.w3.org/2001/XMLSchema#\") or cat in classes.keys()):\n",
    "            classes[cat] = None\n",
    "            label = getLocalPart(cat)\n",
    "            props = getPropertiesForClass(g, cat)\n",
    "            entities.append(SchemaEntity(label=label, description=next(g.objects(cat, RDFS.comment), \"\"), properties=props))\n",
    "\n",
    "    for op in g.subjects(RDF.type, OWL.ObjectProperty):\n",
    "        relname = getLocalPart(op)\n",
    "        rels.append(SchemaRelation(label=relname, properties=[], description=next(g.objects(op, RDFS.comment), \"\")))\n",
    "\n",
    "    for op in g.subjects(RDF.type, OWL.ObjectProperty):\n",
    "        relname = getLocalPart(op)\n",
    "        doms = [getLocalPart(dom) for dom in g.objects(op, RDFS.domain) if dom in classes.keys()]\n",
    "        rans = [getLocalPart(ran) for ran in g.objects(op, RDFS.range) if ran in classes.keys()]\n",
    "        for d in doms:\n",
    "            for r in rans:\n",
    "                triples.append((d, relname, r))\n",
    "\n",
    "    return schema_builder.create_schema_model(entities=entities, relations=rels, potential_schema=triples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757e0642",
   "metadata": {},
   "source": [
    "# Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aa1c9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29261,
     "status": "ok",
     "timestamp": 1747311696979,
     "user": {
      "displayName": "Arnaud Crucifix",
      "userId": "14208215153378642511"
     },
     "user_tz": -120
    },
    "id": "77aa1c9a",
    "outputId": "b237d0ab-0fc0-443a-e4e4-cc062485a6d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing database...\n",
      "Database cleaned.\n",
      "Initializing vector index...\n",
      "Unable to create index: {code: Neo.ClientError.Procedure.ProcedureCallFailed} {message: Failed to invoke procedure `db.index.vector.createNodeIndex`: Caused by: org.neo4j.kernel.api.exceptions.schema.EquivalentSchemaRuleAlreadyExistsException: An equivalent index already exists, 'Index( id=3, name='legal_text_embeddings', type='VECTOR', schema=(:Paragraph {embedding}), indexProvider='vector-2.0' )'.}\n",
      "Building knowledge graph...\n",
      "Processing file: 1Constitution.txt\n",
      "Processing file: 2Prospectus.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM response has improper format for chunk_index=9\n",
      "LLM response has improper format for chunk_index=78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: 3Agreement.txt\n",
      "Merging graphs...\n",
      "Fetching chunks from graph Constitution\n",
      "Found 135 chunks with embeddings\n",
      "Fetching chunks from graph Prospectus\n",
      "Found 96 chunks with embeddings\n",
      "Building FAISS index with 135 vectors of dimension 1536...\n",
      "Finding top 3 similar chunks for each chunk in graph graph_id_b\n",
      "Found 96 similar chunks for each chunk in graph graph_id_b\n",
      "Scores: [[0.5367637  0.5284603  0.52012867]\n",
      " [0.6259637  0.5880318  0.5496872 ]\n",
      " [0.5779646  0.5323864  0.53067183]\n",
      " [0.5355469  0.5157231  0.51360005]\n",
      " [0.45598543 0.39387617 0.38689408]\n",
      " [0.6267467  0.5671647  0.55572885]\n",
      " [0.30426    0.27630615 0.25364658]\n",
      " [0.4731607  0.2943825  0.26604623]\n",
      " [0.26658404 0.2323254  0.22931841]\n",
      " [0.4000994  0.39882344 0.38583723]\n",
      " [0.54203016 0.5396124  0.5233353 ]\n",
      " [0.46757698 0.45210445 0.4381077 ]\n",
      " [0.44245687 0.44150355 0.4395835 ]\n",
      " [0.66028553 0.65227264 0.59230876]\n",
      " [0.45640653 0.4534918  0.44557342]\n",
      " [0.528533   0.46090922 0.44478583]\n",
      " [0.60506886 0.5140003  0.51134825]\n",
      " [0.48955604 0.43586943 0.43277636]\n",
      " [0.6533358  0.65070266 0.62917423]\n",
      " [0.5339309  0.5253764  0.5025645 ]\n",
      " [0.46607158 0.46007016 0.45696986]\n",
      " [0.525807   0.5223225  0.47432998]\n",
      " [0.54748166 0.51316124 0.5019026 ]\n",
      " [0.52097726 0.51713973 0.492351  ]\n",
      " [0.5807303  0.53919584 0.53524375]\n",
      " [0.45167005 0.4320707  0.4296618 ]\n",
      " [0.5588181  0.53176105 0.5204759 ]\n",
      " [0.51131415 0.46772945 0.45952448]\n",
      " [0.4966734  0.48548067 0.47059214]\n",
      " [0.4850431  0.47808483 0.44610107]\n",
      " [0.4449843  0.44336697 0.4355644 ]\n",
      " [0.48272243 0.46967006 0.46612263]\n",
      " [0.55092686 0.5474142  0.5138325 ]\n",
      " [0.5817951  0.5623698  0.5202577 ]\n",
      " [0.6249946  0.57127106 0.5542267 ]\n",
      " [0.49168694 0.47081807 0.4675319 ]\n",
      " [0.5309173  0.51136535 0.4738074 ]\n",
      " [0.5810616  0.5703815  0.57026565]\n",
      " [0.5672167  0.51482993 0.51442564]\n",
      " [0.5532711  0.5395275  0.5167066 ]\n",
      " [0.5753986  0.56281525 0.52626896]\n",
      " [0.5101727  0.4815763  0.4708961 ]\n",
      " [0.4961897  0.49230993 0.48840758]\n",
      " [0.4039333  0.40146574 0.39939737]\n",
      " [0.47122413 0.4450636  0.44109547]\n",
      " [0.3926771  0.36723423 0.35866007]\n",
      " [0.7717976  0.66276085 0.63354224]\n",
      " [0.69698644 0.62131214 0.61480415]\n",
      " [0.52459    0.5224929  0.49734968]\n",
      " [0.6164707  0.5794731  0.57379925]\n",
      " [0.527192   0.5026713  0.47239065]\n",
      " [0.49330243 0.48450047 0.45276874]\n",
      " [0.55840755 0.5538332  0.53634936]\n",
      " [0.5847628  0.5690175  0.5634027 ]\n",
      " [0.6549731  0.63696647 0.63499564]\n",
      " [0.49553415 0.48362553 0.47732258]\n",
      " [0.63031524 0.62628007 0.6094453 ]\n",
      " [0.57908297 0.57371616 0.5717515 ]\n",
      " [0.5243582  0.47788605 0.4708494 ]\n",
      " [0.6071764  0.59563124 0.59088886]\n",
      " [0.64724666 0.61911714 0.60804534]\n",
      " [0.704296   0.6225619  0.59903973]\n",
      " [0.61764485 0.6110468  0.5965767 ]\n",
      " [0.6356871  0.60722244 0.5992806 ]\n",
      " [0.6872269  0.6612938  0.6606119 ]\n",
      " [0.7022825  0.69677067 0.6871735 ]\n",
      " [0.68712384 0.57369816 0.5662875 ]\n",
      " [0.6893129  0.63101476 0.602576  ]\n",
      " [0.3961041  0.39426547 0.38088897]\n",
      " [0.70661306 0.59162    0.5722461 ]\n",
      " [0.54299176 0.52708775 0.51181376]\n",
      " [0.5634786  0.5292754  0.52551174]\n",
      " [0.7000757  0.6879531  0.6343675 ]\n",
      " [0.6167922  0.58656573 0.58535266]\n",
      " [0.3639789  0.34100413 0.33239794]\n",
      " [0.44992012 0.4377905  0.42772835]\n",
      " [0.4960183  0.49024704 0.4658258 ]\n",
      " [0.45974952 0.45839894 0.4549203 ]\n",
      " [0.5854286  0.56129456 0.5474912 ]\n",
      " [0.61707944 0.61663103 0.6142916 ]\n",
      " [0.63433594 0.6317908  0.61971587]\n",
      " [0.34824684 0.33890927 0.3384409 ]\n",
      " [0.6890426  0.6707493  0.6667037 ]\n",
      " [0.67780846 0.64712876 0.62103295]\n",
      " [0.7655362  0.668594   0.6410908 ]\n",
      " [0.6662409  0.63293046 0.62500197]\n",
      " [0.673704   0.5907143  0.53617376]\n",
      " [0.6523687  0.594355   0.549959  ]\n",
      " [0.60382456 0.5683711  0.54298   ]\n",
      " [0.5801998  0.5710432  0.5643039 ]\n",
      " [0.59337366 0.54773366 0.5474001 ]\n",
      " [0.5892628  0.5846272  0.55283964]\n",
      " [0.6913229  0.57853687 0.57690793]\n",
      " [0.5469725  0.5335212  0.52906835]\n",
      " [0.6377166  0.54998565 0.52813387]\n",
      " [0.5625785  0.5030162  0.5017005 ]]\n",
      "Indices: [[ 10   3   0]\n",
      " [131  15 108]\n",
      " [131   8   3]\n",
      " [  9   8   3]\n",
      " [ 64  65 103]\n",
      " [ 15  22  16]\n",
      " [113 131  10]\n",
      " [113 132 131]\n",
      " [113  44  28]\n",
      " [ 28 107 105]\n",
      " [  3   9  15]\n",
      " [ 36 105 106]\n",
      " [130  73  93]\n",
      " [  0   3  10]\n",
      " [ 92  22   3]\n",
      " [ 92 133  22]\n",
      " [ 15  17 108]\n",
      " [ 64  22  16]\n",
      " [ 22  16  15]\n",
      " [131  22  64]\n",
      " [131 106  17]\n",
      " [  3   9   6]\n",
      " [  8   3  22]\n",
      " [  3 106   9]\n",
      " [ 22 133  16]\n",
      " [  9 103   7]\n",
      " [ 15  22 109]\n",
      " [ 15 131  17]\n",
      " [131  28 107]\n",
      " [ 92   8   9]\n",
      " [106   7   3]\n",
      " [107  64 106]\n",
      " [106 131   3]\n",
      " [ 92   9 106]\n",
      " [  9  92   7]\n",
      " [108   3 106]\n",
      " [  3   9  10]\n",
      " [106 127   9]\n",
      " [  9  92   3]\n",
      " [ 65  63  92]\n",
      " [130  92   8]\n",
      " [ 92 106 108]\n",
      " [  3  10  92]\n",
      " [ 15   3  62]\n",
      " [  3   8  92]\n",
      " [108  15  28]\n",
      " [ 15  16 108]\n",
      " [ 15  23  16]\n",
      " [ 27  28  96]\n",
      " [108 107 105]\n",
      " [107 108 106]\n",
      " [ 33  37  96]\n",
      " [  3   9   8]\n",
      " [131  15 108]\n",
      " [ 27  22  24]\n",
      " [130  51  46]\n",
      " [ 32  37  41]\n",
      " [ 22 131  27]\n",
      " [ 24  57  62]\n",
      " [ 22  15  21]\n",
      " [131   3  15]\n",
      " [  0   3  93]\n",
      " [  3   0 114]\n",
      " [118  37 127]\n",
      " [124 125 123]\n",
      " [125 124 130]\n",
      " [129  10 124]\n",
      " [129 130 128]\n",
      " [ 28  36 106]\n",
      " [105 109 106]\n",
      " [105  36  28]\n",
      " [106 109 105]\n",
      " [106 105 109]\n",
      " [106 105 109]\n",
      " [ 18  86  87]\n",
      " [ 18  72 114]\n",
      " [ 22  24  15]\n",
      " [ 24 107 130]\n",
      " [131  93   3]\n",
      " [ 82  22  65]\n",
      " [  0   3 131]\n",
      " [ 88  92  15]\n",
      " [131   0   3]\n",
      " [ 93 131   3]\n",
      " [ 93   0   3]\n",
      " [ 93   3   0]\n",
      " [ 93 131   3]\n",
      " [ 93 131   0]\n",
      " [ 93 131   3]\n",
      " [ 93 104  65]\n",
      " [ 92  15   7]\n",
      " [ 92 106   8]\n",
      " [131   3   0]\n",
      " [ 10   3 131]\n",
      " [131 109 106]\n",
      " [131  15  27]]\n",
      "Prepared 96 matches\n",
      "Creating similarity links in Neo4j...\n",
      "matches:  dict_items([('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:762', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:10', np.float32(0.5367637)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:3', np.float32(0.5284603)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:0', np.float32(0.52012867))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:763', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:131', np.float32(0.6259637)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:15', np.float32(0.5880318)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:108', np.float32(0.5496872))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:764', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:131', np.float32(0.5779646)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:8', np.float32(0.5323864)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:3', np.float32(0.53067183))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:765', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:9', np.float32(0.5355469)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:8', np.float32(0.5157231)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:3', np.float32(0.51360005))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:766', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:767', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:15', np.float32(0.6267467)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:22', np.float32(0.5671647)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:16', np.float32(0.55572885))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:768', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:769', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:770', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:771', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:772', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:3', np.float32(0.54203016)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:9', np.float32(0.5396124)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:15', np.float32(0.5233353))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:773', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:774', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:775', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:0', np.float32(0.66028553)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:3', np.float32(0.65227264)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:10', np.float32(0.59230876))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:776', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:777', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:92', np.float32(0.528533))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:778', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:15', np.float32(0.60506886)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:17', np.float32(0.5140003)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:108', np.float32(0.51134825))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:779', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:780', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:22', np.float32(0.6533358)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:16', np.float32(0.65070266)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:15', np.float32(0.62917423))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:781', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:131', np.float32(0.5339309)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:22', np.float32(0.5253764)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:64', np.float32(0.5025645))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:782', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:783', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:3', np.float32(0.525807)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:9', np.float32(0.5223225))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:784', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:8', np.float32(0.54748166)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:3', np.float32(0.51316124)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:22', np.float32(0.5019026))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:785', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:3', np.float32(0.52097726)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:106', np.float32(0.51713973))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:786', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:22', np.float32(0.5807303)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1394', np.float32(0.53919584)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:16', np.float32(0.53524375))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:787', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:788', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:15', np.float32(0.5588181)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:22', np.float32(0.53176105)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:109', np.float32(0.5204759))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:789', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:15', np.float32(0.51131415))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:790', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:791', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:792', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:793', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:794', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:106', np.float32(0.55092686)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:131', np.float32(0.5474142)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:3', np.float32(0.5138325))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:795', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:92', np.float32(0.5817951)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:9', np.float32(0.5623698)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:106', np.float32(0.5202577))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:796', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:9', np.float32(0.6249946)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:92', np.float32(0.57127106)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:7', np.float32(0.5542267))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:797', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:798', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:3', np.float32(0.5309173)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:9', np.float32(0.51136535))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:799', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:106', np.float32(0.5810616)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:127', np.float32(0.5703815)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:9', np.float32(0.57026565))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:800', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:9', np.float32(0.5672167)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:92', np.float32(0.51482993)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:3', np.float32(0.51442564))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:801', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:65', np.float32(0.5532711)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:63', np.float32(0.5395275)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:92', np.float32(0.5167066))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:802', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:130', np.float32(0.5753986)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:92', np.float32(0.56281525)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:8', np.float32(0.52626896))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:803', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:92', np.float32(0.5101727))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:804', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:805', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:806', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:807', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:808', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:15', np.float32(0.7717976)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:16', np.float32(0.66276085)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:108', np.float32(0.63354224))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:809', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:15', np.float32(0.69698644)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:23', np.float32(0.62131214)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:16', np.float32(0.61480415))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:810', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:27', np.float32(0.52459)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:28', np.float32(0.5224929))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:811', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:108', np.float32(0.6164707)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:107', np.float32(0.5794731)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:105', np.float32(0.57379925))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:812', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:107', np.float32(0.527192)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:108', np.float32(0.5026713))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:813', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:814', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:3', np.float32(0.55840755)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:9', np.float32(0.5538332)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:8', np.float32(0.53634936))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:815', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:131', np.float32(0.5847628)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:15', np.float32(0.5690175)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:108', np.float32(0.5634027))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:816', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:27', np.float32(0.6549731)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:22', np.float32(0.63696647)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:24', np.float32(0.63499564))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:817', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:818', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:32', np.float32(0.63031524)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:37', np.float32(0.62628007)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:41', np.float32(0.6094453))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:819', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:22', np.float32(0.57908297)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:131', np.float32(0.57371616)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:27', np.float32(0.5717515))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:820', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:24', np.float32(0.5243582))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:821', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:22', np.float32(0.6071764)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:15', np.float32(0.59563124)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:21', np.float32(0.59088886))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:822', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:131', np.float32(0.64724666)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:3', np.float32(0.61911714)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:15', np.float32(0.60804534))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:823', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:0', np.float32(0.704296)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:3', np.float32(0.6225619)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:93', np.float32(0.59903973))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:824', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:3', np.float32(0.61764485)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:0', np.float32(0.6110468)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:114', np.float32(0.5965767))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:825', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:118', np.float32(0.6356871)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:37', np.float32(0.60722244)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:127', np.float32(0.5992806))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:826', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:124', np.float32(0.6872269)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:125', np.float32(0.6612938)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:123', np.float32(0.6606119))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:827', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:125', np.float32(0.7022825)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:124', np.float32(0.69677067)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:130', np.float32(0.6871735))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:828', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:129', np.float32(0.68712384)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:10', np.float32(0.57369816)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:124', np.float32(0.5662875))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:829', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:129', np.float32(0.6893129)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:130', np.float32(0.63101476)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:128', np.float32(0.602576))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:830', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:831', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:105', np.float32(0.70661306)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:109', np.float32(0.59162)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:106', np.float32(0.5722461))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:832', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:105', np.float32(0.54299176)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:36', np.float32(0.52708775)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:28', np.float32(0.51181376))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:833', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:106', np.float32(0.5634786)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:109', np.float32(0.5292754)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:105', np.float32(0.52551174))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:834', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:106', np.float32(0.7000757)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:105', np.float32(0.6879531)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:109', np.float32(0.6343675))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:835', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:106', np.float32(0.6167922)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:105', np.float32(0.58656573)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:109', np.float32(0.58535266))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:836', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:837', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:838', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:839', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:840', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:131', np.float32(0.5854286)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:93', np.float32(0.56129456)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:3', np.float32(0.5474912))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:841', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:82', np.float32(0.61707944)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:22', np.float32(0.61663103)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:65', np.float32(0.6142916))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:842', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:0', np.float32(0.63433594)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:3', np.float32(0.6317908)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:131', np.float32(0.61971587))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:843', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:844', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:131', np.float32(0.6890426)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:0', np.float32(0.6707493)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:3', np.float32(0.6667037))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:845', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:93', np.float32(0.67780846)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:131', np.float32(0.64712876)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:3', np.float32(0.62103295))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:846', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:93', np.float32(0.7655362)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:0', np.float32(0.668594)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:3', np.float32(0.6410908))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:847', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:93', np.float32(0.6662409)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:3', np.float32(0.63293046)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:0', np.float32(0.62500197))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:848', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:93', np.float32(0.673704)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:131', np.float32(0.5907143)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:3', np.float32(0.53617376))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:849', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:93', np.float32(0.6523687)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:131', np.float32(0.594355)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:0', np.float32(0.549959))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:850', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:93', np.float32(0.60382456)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:131', np.float32(0.5683711)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:3', np.float32(0.54298))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:851', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:93', np.float32(0.5801998)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:104', np.float32(0.5710432)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:65', np.float32(0.5643039))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:852', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:92', np.float32(0.59337366)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:15', np.float32(0.54773366)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:7', np.float32(0.5474001))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:853', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:92', np.float32(0.5892628)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:106', np.float32(0.5846272)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:8', np.float32(0.55283964))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:854', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:131', np.float32(0.6913229)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:3', np.float32(0.57853687)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:0', np.float32(0.57690793))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:855', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:10', np.float32(0.5469725)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:3', np.float32(0.5335212)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:131', np.float32(0.52906835))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:856', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:131', np.float32(0.6377166)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:109', np.float32(0.54998565)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:106', np.float32(0.52813387))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:857', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:131', np.float32(0.5625785)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:15', np.float32(0.5030162)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:27', np.float32(0.5017005))])])\n",
      "Found 135 similar chunks for each chunk in graph graph_id_a\n",
      "Scores: [[0.704296   0.67074937 0.668594  ]\n",
      " [0.3503521  0.3122293  0.31014884]\n",
      " [0.4922991  0.48297837 0.48212764]\n",
      " [0.6667038  0.65227264 0.64109063]\n",
      " [0.5037259  0.49552083 0.4703158 ]\n",
      " [0.5110565  0.5075877  0.4943662 ]\n",
      " [0.59810597 0.5062156  0.5015162 ]\n",
      " [0.55525655 0.5542267  0.5474002 ]\n",
      " [0.61443067 0.5700308  0.55283964]\n",
      " [0.6249946  0.57026565 0.5674564 ]\n",
      " [0.6132241  0.6042015  0.59306294]\n",
      " [0.46239066 0.4592669  0.43308705]\n",
      " [0.4089214  0.402056   0.3852422 ]\n",
      " [0.57539785 0.47033086 0.46325368]\n",
      " [0.45921308 0.43191522 0.42803964]\n",
      " [0.7717974  0.6969867  0.62917423]\n",
      " [0.662761   0.6507027  0.6189231 ]\n",
      " [0.6211848  0.57602465 0.56885684]\n",
      " [0.50132644 0.46307725 0.46177542]\n",
      " [0.44817916 0.44362536 0.43574867]\n",
      " [0.46149033 0.453652   0.450557  ]\n",
      " [0.59088886 0.5279398  0.51063275]\n",
      " [0.6533358  0.63696647 0.61663115]\n",
      " [0.62131214 0.573845   0.5254869 ]\n",
      " [0.63499564 0.5524206  0.5492904 ]\n",
      " [0.5901118  0.57133424 0.566378  ]\n",
      " [0.5462554  0.5097978  0.47304872]\n",
      " [0.6549731  0.5717515  0.55127156]\n",
      " [0.5638009  0.5542085  0.5335565 ]\n",
      " [0.5359754  0.52402234 0.5117048 ]\n",
      " [0.5181444  0.48666412 0.48633465]\n",
      " [0.5696727  0.4785977  0.46942347]\n",
      " [0.63031524 0.52561283 0.5067404 ]\n",
      " [0.5938947  0.5933352  0.55794805]\n",
      " [0.5543962  0.45671007 0.45119455]\n",
      " [0.5970216  0.49000943 0.46932495]\n",
      " [0.5606687  0.53778815 0.53469443]\n",
      " [0.62628007 0.60722244 0.538381  ]\n",
      " [0.5468828  0.54411787 0.52307594]\n",
      " [0.5851402  0.5729217  0.5361007 ]\n",
      " [0.59963495 0.5366199  0.53255224]\n",
      " [0.6094453  0.5090405  0.47497186]\n",
      " [0.4393431  0.3786329  0.3652651 ]\n",
      " [0.5277884  0.42960566 0.42124927]\n",
      " [0.45575416 0.39001575 0.38255483]\n",
      " [0.47354534 0.46677884 0.4584524 ]\n",
      " [0.5013425  0.49976528 0.49635372]\n",
      " [0.5484798  0.54023194 0.5289423 ]\n",
      " [0.47207928 0.45555225 0.43966126]\n",
      " [0.4611361  0.44224575 0.42487645]\n",
      " [0.5340765  0.5211537  0.5124199 ]\n",
      " [0.5685078  0.5596631  0.5246252 ]\n",
      " [0.51166266 0.4809733  0.46697974]\n",
      " [0.53566164 0.5077694  0.5046887 ]\n",
      " [0.45679572 0.44157764 0.42722535]\n",
      " [0.4550922  0.45013323 0.44369984]\n",
      " [0.46787143 0.4656312  0.4553554 ]\n",
      " [0.55914634 0.5113334  0.4894485 ]\n",
      " [0.4625162  0.4516443  0.44905168]\n",
      " [0.4966871  0.48209044 0.46803707]\n",
      " [0.3833318  0.3632992  0.3619134 ]\n",
      " [0.5182737  0.50485384 0.4762995 ]\n",
      " [0.5597862  0.54333866 0.53019184]\n",
      " [0.575371   0.5483405  0.5395275 ]\n",
      " [0.506534   0.5025645  0.48955604]\n",
      " [0.61429167 0.56771755 0.5643039 ]\n",
      " [0.48304772 0.4735574  0.47317496]\n",
      " [0.56029344 0.5534104  0.47273484]\n",
      " [0.45034486 0.43990314 0.42309055]\n",
      " [0.59770125 0.48886672 0.4511302 ]\n",
      " [0.5766264  0.46929735 0.4624555 ]\n",
      " [0.4183118  0.39620325 0.39320695]\n",
      " [0.50657433 0.49226755 0.44477677]\n",
      " [0.5252399  0.51357406 0.5061336 ]\n",
      " [0.46055388 0.41394147 0.40799025]\n",
      " [0.5033339  0.432631   0.4095443 ]\n",
      " [0.5119177  0.40705416 0.39854127]\n",
      " [0.46027654 0.45908457 0.45888788]\n",
      " [0.58946323 0.51783115 0.46129307]\n",
      " [0.56463975 0.45613948 0.44492406]\n",
      " [0.4620123  0.42894638 0.42208222]\n",
      " [0.4743895  0.43169975 0.41935405]\n",
      " [0.6170794  0.5251714  0.5001512 ]\n",
      " [0.55917066 0.43306845 0.42687106]\n",
      " [0.28350848 0.27464348 0.25975478]\n",
      " [0.47586745 0.41835943 0.40757135]\n",
      " [0.4055203  0.38234806 0.36085507]\n",
      " [0.5195428  0.49880946 0.49772418]\n",
      " [0.510419   0.4980872  0.4700164 ]\n",
      " [0.4711576  0.4526587  0.4314557 ]\n",
      " [0.4990869  0.46219167 0.4603788 ]\n",
      " [0.4279287  0.38581014 0.37136865]\n",
      " [0.5945042  0.5933738  0.58926266]\n",
      " [0.7655362  0.67780846 0.6737042 ]\n",
      " [0.50134903 0.48070577 0.45953766]\n",
      " [0.48241228 0.45794368 0.44999507]\n",
      " [0.5363676  0.49734968 0.49295375]\n",
      " [0.46147653 0.44740182 0.4321961 ]\n",
      " [0.35230905 0.3487202  0.3401674 ]\n",
      " [0.37357688 0.36575067 0.3544002 ]\n",
      " [0.5394061  0.46082518 0.43815577]\n",
      " [0.46188238 0.41698587 0.4143399 ]\n",
      " [0.4390789  0.43772218 0.41535878]\n",
      " [0.46423468 0.4320707  0.43175656]\n",
      " [0.57104325 0.5420503  0.52514386]\n",
      " [0.7066133  0.68795323 0.5865657 ]\n",
      " [0.7000757  0.6167923  0.5846274 ]\n",
      " [0.5794731  0.527192   0.51895034]\n",
      " [0.63354224 0.6164707  0.6042603 ]\n",
      " [0.6343675  0.59162015 0.58535266]\n",
      " [0.5812316  0.5803869  0.56658393]\n",
      " [0.54072905 0.5009655  0.49325556]\n",
      " [0.35990882 0.32884765 0.31412542]\n",
      " [0.4731607  0.4310508  0.4225073 ]\n",
      " [0.5965767  0.5365344  0.5056872 ]\n",
      " [0.50354964 0.4893432  0.48831993]\n",
      " [0.5788489  0.46794745 0.46101177]\n",
      " [0.54652196 0.44014707 0.4342933 ]\n",
      " [0.6356871  0.48767903 0.48686868]\n",
      " [0.49316746 0.43914714 0.40247127]\n",
      " [0.5692574  0.5358724  0.49906436]\n",
      " [0.46219555 0.45933774 0.4387288 ]\n",
      " [0.6555429  0.5551755  0.51710147]\n",
      " [0.660612   0.64325786 0.57521164]\n",
      " [0.6967708  0.6872268  0.6019662 ]\n",
      " [0.70228237 0.6612938  0.55251473]\n",
      " [0.54033244 0.5354048  0.49398774]\n",
      " [0.64059514 0.5992806  0.5703815 ]\n",
      " [0.6025759  0.53957844 0.48871374]\n",
      " [0.6893128  0.687124   0.558279  ]\n",
      " [0.6871733  0.6310147  0.59677124]\n",
      " [0.6913229  0.6890425  0.6472466 ]\n",
      " [0.507885   0.45926207 0.44582078]\n",
      " [0.6260431  0.54520017 0.54209965]\n",
      " [0.4616443  0.45941308 0.4220252 ]]\n",
      "Indices: [[61 82 84]\n",
      " [22 82 84]\n",
      " [62 87 84]\n",
      " [82 13 84]\n",
      " [82 84 37]\n",
      " [72 37 69]\n",
      " [82 83 84]\n",
      " [82 34 90]\n",
      " [82 84 91]\n",
      " [34 37 82]\n",
      " [82 80 84]\n",
      " [46 59 18]\n",
      " [26 59 46]\n",
      " [59 18 63]\n",
      " [59 46 18]\n",
      " [46 47 18]\n",
      " [46 18 82]\n",
      " [46 18 47]\n",
      " [78 58 59]\n",
      " [60 59 63]\n",
      " [59 26 57]\n",
      " [59 60 89]\n",
      " [18 54 79]\n",
      " [47 46 53]\n",
      " [54 57 79]\n",
      " [65 46 54]\n",
      " [54 60 53]\n",
      " [54 57 59]\n",
      " [54 53 73]\n",
      " [60 54 73]\n",
      " [54 65 56]\n",
      " [56 60 63]\n",
      " [56 60 53]\n",
      " [63 56 60]\n",
      " [56 53 60]\n",
      " [56 63 53]\n",
      " [73 72 56]\n",
      " [56 63 60]\n",
      " [56 47 60]\n",
      " [56 65 46]\n",
      " [56 60 63]\n",
      " [56 60 63]\n",
      " [56 60 20]\n",
      " [56 60 63]\n",
      " [56 65 60]\n",
      " [18 60 79]\n",
      " [18 54 53]\n",
      " [63 64 65]\n",
      " [56 53 55]\n",
      " [26 55 56]\n",
      " [53 73 56]\n",
      " [65 46 47]\n",
      " [73 60 34]\n",
      " [73 56 60]\n",
      " [65 73 79]\n",
      " [65 56 55]\n",
      " [55 59 53]\n",
      " [57 79 54]\n",
      " [63 60 55]\n",
      " [57 53 60]\n",
      " [55 26 56]\n",
      " [89 18 79]\n",
      " [79 18 59]\n",
      " [60 57 39]\n",
      " [82 19 17]\n",
      " [79 60 89]\n",
      " [46 59 18]\n",
      " [63 56 60]\n",
      " [59 60 79]\n",
      " [79 67 66]\n",
      " [79 59 60]\n",
      " [79 90 39]\n",
      " [78 79 89]\n",
      " [62 79 84]\n",
      " [79 91 63]\n",
      " [79 59 60]\n",
      " [79 89 67]\n",
      " [62 61 79]\n",
      " [79 67 89]\n",
      " [79 59 67]\n",
      " [79 89 67]\n",
      " [79 63 46]\n",
      " [79 67 66]\n",
      " [79 59 64]\n",
      " [79 64 65]\n",
      " [79 89 77]\n",
      " [77 79 78]\n",
      " [67 47 46]\n",
      " [83 84 89]\n",
      " [84 89 83]\n",
      " [83 84 89]\n",
      " [84 83 89]\n",
      " [84 90 91]\n",
      " [84 83 86]\n",
      " [84 91 89]\n",
      " [89 84 86]\n",
      " [49 48 91]\n",
      " [49 51 50]\n",
      " [79 84 77]\n",
      " [77 91 84]\n",
      " [90 79 67]\n",
      " [84 37 32]\n",
      " [90 60 91]\n",
      " [84 25 32]\n",
      " [89 84 79]\n",
      " [69 72 73]\n",
      " [72 73 91]\n",
      " [49 50 60]\n",
      " [46 49 65]\n",
      " [72 69 73]\n",
      " [65 73 60]\n",
      " [73 63 60]\n",
      " [20 75 22]\n",
      " [ 7 62 28]\n",
      " [62 84 13]\n",
      " [82 84 46]\n",
      " [63 79 60]\n",
      " [63 79 60]\n",
      " [63 59 60]\n",
      " [63 56 60]\n",
      " [63 62 60]\n",
      " [65 64 67]\n",
      " [64 65 79]\n",
      " [64 65 82]\n",
      " [65 64 67]\n",
      " [65 64 60]\n",
      " [65 67 64]\n",
      " [65 63 37]\n",
      " [67 66 79]\n",
      " [67 66 65]\n",
      " [65 67 46]\n",
      " [92 82 60]\n",
      " [94 92 19]\n",
      " [82 18 83]\n",
      " [93 82 94]]\n",
      "Prepared 135 reverse matches\n",
      "Creating reverse similarity links...\n",
      "matches:  dict_items([('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:0', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:823', np.float32(0.704296)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:844', np.float32(0.67074937)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:846', np.float32(0.668594))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:2', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:3', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:844', np.float32(0.6667038)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:775', np.float32(0.65227264)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:846', np.float32(0.64109063))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:4', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:844', np.float32(0.5037259))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:5', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:834', np.float32(0.5110565)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:799', np.float32(0.5075877))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:6', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:844', np.float32(0.59810597)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:845', np.float32(0.5062156)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:846', np.float32(0.5015162))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:7', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:844', np.float32(0.55525655)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:796', np.float32(0.5542267)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:852', np.float32(0.5474002))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:8', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:844', np.float32(0.61443067)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:846', np.float32(0.5700308)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:853', np.float32(0.55283964))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:9', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:796', np.float32(0.6249946)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:799', np.float32(0.57026565)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:844', np.float32(0.5674564))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:10', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:844', np.float32(0.6132241)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:842', np.float32(0.6042015)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:846', np.float32(0.59306294))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:11', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:12', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:13', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:821', np.float32(0.57539785))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:14', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:15', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:808', np.float32(0.7717974)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:809', np.float32(0.6969867)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:780', np.float32(0.62917423))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:16', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:808', np.float32(0.662761)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:780', np.float32(0.6507027)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:844', np.float32(0.6189231))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:17', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:808', np.float32(0.6211848)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:780', np.float32(0.57602465)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:809', np.float32(0.56885684))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:18', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:840', np.float32(0.50132644))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:19', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:20', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:21', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:821', np.float32(0.59088886)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:822', np.float32(0.5279398)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:851', np.float32(0.51063275))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:22', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:780', np.float32(0.6533358)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:816', np.float32(0.63696647)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:841', np.float32(0.61663115))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:23', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:809', np.float32(0.62131214)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:808', np.float32(0.573845)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:815', np.float32(0.5254869))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:24', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:816', np.float32(0.63499564)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:819', np.float32(0.5524206)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:841', np.float32(0.5492904))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:25', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:827', np.float32(0.5901118)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:808', np.float32(0.57133424)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:816', np.float32(0.566378))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:26', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:816', np.float32(0.5462554)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:822', np.float32(0.5097978))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:27', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:816', np.float32(0.6549731)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:819', np.float32(0.5717515)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:821', np.float32(0.55127156))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:28', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:816', np.float32(0.5638009)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:815', np.float32(0.5542085)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:835', np.float32(0.5335565))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:29', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:822', np.float32(0.5359754)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:816', np.float32(0.52402234)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:835', np.float32(0.5117048))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:30', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:816', np.float32(0.5181444))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:31', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:818', np.float32(0.5696727))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:32', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:818', np.float32(0.63031524)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:822', np.float32(0.52561283)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:815', np.float32(0.5067404))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:33', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:825', np.float32(0.5938947)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:818', np.float32(0.5933352)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:822', np.float32(0.55794805))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:34', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:818', np.float32(0.5543962))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:35', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:818', np.float32(0.5970216))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:36', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:835', np.float32(0.5606687)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:834', np.float32(0.53778815)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:818', np.float32(0.53469443))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:37', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:818', np.float32(0.62628007)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:825', np.float32(0.60722244)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:822', np.float32(0.538381))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:38', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:818', np.float32(0.5468828)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:809', np.float32(0.54411787)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:822', np.float32(0.52307594))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:39', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:818', np.float32(0.5851402)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:827', np.float32(0.5729217)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:808', np.float32(0.5361007))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:40', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:818', np.float32(0.59963495)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:822', np.float32(0.5366199)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:825', np.float32(0.53255224))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:41', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:818', np.float32(0.6094453)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:822', np.float32(0.5090405))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:42', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:43', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:818', np.float32(0.5277884))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:44', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:45', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:46', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:780', np.float32(0.5013425))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:47', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:825', np.float32(0.5484798)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:826', np.float32(0.54023194)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:827', np.float32(0.5289423))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:48', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:49', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:50', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:815', np.float32(0.5340765)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:835', np.float32(0.5211537)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:818', np.float32(0.5124199))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:51', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:827', np.float32(0.5685078)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:808', np.float32(0.5596631)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:809', np.float32(0.5246252))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:52', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:835', np.float32(0.51166266))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:53', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:835', np.float32(0.53566164)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:818', np.float32(0.5077694)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:822', np.float32(0.5046887))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:54', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:55', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:56', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:57', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:819', np.float32(0.55914634)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:841', np.float32(0.5113334))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:58', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:59', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:60', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:61', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:851', np.float32(0.5182737)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:780', np.float32(0.50485384))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:62', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:841', np.float32(0.5597862)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:780', np.float32(0.54333866)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:821', np.float32(0.53019184))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:63', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:822', np.float32(0.575371)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:819', np.float32(0.5483405)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:801', np.float32(0.5395275))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:64', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:844', np.float32(0.506534)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:781', np.float32(0.5025645))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:65', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:841', np.float32(0.61429167)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:822', np.float32(0.56771755)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:851', np.float32(0.5643039))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:66', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:67', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:825', np.float32(0.56029344)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:818', np.float32(0.5534104))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:68', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:69', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:841', np.float32(0.59770125))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:70', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:841', np.float32(0.5766264))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:71', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:72', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:840', np.float32(0.50657433))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:73', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:824', np.float32(0.5252399)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:841', np.float32(0.51357406)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:846', np.float32(0.5061336))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:74', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:75', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:841', np.float32(0.5033339))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:76', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:841', np.float32(0.5119177))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:77', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:78', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:841', np.float32(0.58946323)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:829', np.float32(0.51783115))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:79', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:841', np.float32(0.56463975))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:80', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:81', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:82', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:841', np.float32(0.6170794)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:829', np.float32(0.5251714)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:828', np.float32(0.5001512))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:83', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:841', np.float32(0.55917066))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:84', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:85', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:86', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:87', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:829', np.float32(0.5195428))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:88', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:845', np.float32(0.510419))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:89', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:90', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:91', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:92', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:846', np.float32(0.5945042)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:852', np.float32(0.5933738)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:853', np.float32(0.58926266))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:93', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:846', np.float32(0.7655362)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:845', np.float32(0.67780846)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:848', np.float32(0.6737042))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:94', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:846', np.float32(0.50134903))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:95', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:96', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:811', np.float32(0.5363676))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:97', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:98', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:99', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:100', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:852', np.float32(0.5394061))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:101', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:102', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:103', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:104', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:851', np.float32(0.57104325)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:846', np.float32(0.5420503)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:841', np.float32(0.52514386))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:105', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:831', np.float32(0.7066133)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:834', np.float32(0.68795323)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:835', np.float32(0.5865657))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:106', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:834', np.float32(0.7000757)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:835', np.float32(0.6167923)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:853', np.float32(0.5846274))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:107', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:811', np.float32(0.5794731)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:812', np.float32(0.527192)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:822', np.float32(0.51895034))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:108', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:808', np.float32(0.63354224)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:811', np.float32(0.6164707)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:827', np.float32(0.6042603))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:109', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:834', np.float32(0.6343675)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:831', np.float32(0.59162015)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:835', np.float32(0.58535266))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:110', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:827', np.float32(0.5812316)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:835', np.float32(0.5803869)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:822', np.float32(0.56658393))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:111', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:835', np.float32(0.54072905)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:825', np.float32(0.5009655))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:112', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:113', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:114', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:824', np.float32(0.5965767)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:846', np.float32(0.5365344)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:775', np.float32(0.5056872))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:115', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:844', np.float32(0.50354964))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:116', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:825', np.float32(0.5788489))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:117', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:825', np.float32(0.54652196))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:118', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:825', np.float32(0.6356871))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:119', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:120', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:825', np.float32(0.5692574)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:824', np.float32(0.5358724))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:121', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:122', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:826', np.float32(0.6555429)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:827', np.float32(0.5551755)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:841', np.float32(0.51710147))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:123', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:826', np.float32(0.660612)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:827', np.float32(0.64325786)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:844', np.float32(0.57521164))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:124', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:827', np.float32(0.6967708)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:826', np.float32(0.6872268)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:829', np.float32(0.6019662))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:125', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:827', np.float32(0.70228237)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:826', np.float32(0.6612938)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:822', np.float32(0.55251473))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:126', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:827', np.float32(0.54033244)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:829', np.float32(0.5354048))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:127', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:827', np.float32(0.64059514)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:825', np.float32(0.5992806)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:799', np.float32(0.5703815))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:128', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:829', np.float32(0.6025759)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:828', np.float32(0.53957844))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:129', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:829', np.float32(0.6893128)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:828', np.float32(0.687124)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:827', np.float32(0.558279))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:130', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:827', np.float32(0.6871733)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:829', np.float32(0.6310147)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:808', np.float32(0.59677124))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:131', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:854', np.float32(0.6913229)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:844', np.float32(0.6890425)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:822', np.float32(0.6472466))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:132', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:856', np.float32(0.507885))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1394', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:844', np.float32(0.6260431)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:780', np.float32(0.54520017)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:845', np.float32(0.54209965))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1488', [])])\n",
      "âœ… Created 401 cross-graph similarity links\n",
      "Fetching chunks from graph Prospectus\n",
      "Found 96 chunks with embeddings\n",
      "Fetching chunks from graph Agreement\n",
      "Found 107 chunks with embeddings\n",
      "Building FAISS index with 96 vectors of dimension 1536...\n",
      "Finding top 3 similar chunks for each chunk in graph graph_id_b\n",
      "Found 107 similar chunks for each chunk in graph graph_id_b\n",
      "Scores: [[0.39472812 0.389806   0.38678193]\n",
      " [0.44231915 0.43771315 0.43382898]\n",
      " [0.5528637  0.5468018  0.5363688 ]\n",
      " [0.462347   0.46215734 0.4551878 ]\n",
      " [0.4574757  0.4444759  0.42139217]\n",
      " [0.51821434 0.4597671  0.45816538]\n",
      " [0.51728094 0.46895024 0.46886104]\n",
      " [0.41651022 0.41113976 0.38411814]\n",
      " [0.45735157 0.4493837  0.4178017 ]\n",
      " [0.40269932 0.3986667  0.36461428]\n",
      " [0.42358562 0.40987647 0.4040638 ]\n",
      " [0.48742592 0.44800088 0.4303597 ]\n",
      " [0.4859001  0.48342195 0.46834773]\n",
      " [0.4472525  0.4446807  0.43818676]\n",
      " [0.46538377 0.42036235 0.40666467]\n",
      " [0.49320224 0.47169712 0.46055797]\n",
      " [0.49318352 0.46718758 0.46215913]\n",
      " [0.49597162 0.46584734 0.45878544]\n",
      " [0.34435076 0.3266797  0.3173244 ]\n",
      " [0.47190857 0.41293254 0.39519563]\n",
      " [0.40374184 0.36214277 0.35247308]\n",
      " [0.43981263 0.4061059  0.39542985]\n",
      " [0.34816056 0.33054066 0.30999392]\n",
      " [0.38240686 0.37261915 0.3621137 ]\n",
      " [0.45108512 0.4476762  0.44479197]\n",
      " [0.5128733  0.46564463 0.44112724]\n",
      " [0.54511714 0.4652726  0.44619423]\n",
      " [0.43620452 0.38515937 0.36579785]\n",
      " [0.4655982  0.4330203  0.42699963]\n",
      " [0.33185413 0.3301904  0.31513184]\n",
      " [0.4426566  0.4421195  0.4365607 ]\n",
      " [0.46381873 0.4505877  0.43372247]\n",
      " [0.54603714 0.47887215 0.43153864]\n",
      " [0.5123023  0.43507695 0.40592518]\n",
      " [0.50025594 0.4571121  0.45224288]\n",
      " [0.34778118 0.34426734 0.34303853]\n",
      " [0.46872783 0.39670548 0.38263202]\n",
      " [0.38207114 0.36246508 0.34462744]\n",
      " [0.43838543 0.4378701  0.4033499 ]\n",
      " [0.35395816 0.33239445 0.3280786 ]\n",
      " [0.5050235  0.49814337 0.48637813]\n",
      " [0.38790718 0.36901274 0.36529   ]\n",
      " [0.3477165  0.34263304 0.3279763 ]\n",
      " [0.4831719  0.4463259  0.4192363 ]\n",
      " [0.36427948 0.36373094 0.36372736]\n",
      " [0.48601282 0.45968536 0.43842435]\n",
      " [0.5205001  0.51482964 0.47965303]\n",
      " [0.4614725  0.41657904 0.41284177]\n",
      " [0.48846117 0.46592554 0.45028773]\n",
      " [0.48298162 0.47890085 0.47701988]\n",
      " [0.45002314 0.43494734 0.42326772]\n",
      " [0.546131   0.46499738 0.45291558]\n",
      " [0.5409855  0.50793326 0.4876684 ]\n",
      " [0.5247976  0.46211505 0.4593253 ]\n",
      " [0.5054523  0.483089   0.4819103 ]\n",
      " [0.458611   0.43269485 0.42037347]\n",
      " [0.514926   0.50802326 0.49055374]\n",
      " [0.45611265 0.4505888  0.44895226]\n",
      " [0.53689027 0.5355166  0.5340581 ]\n",
      " [0.5094662  0.48343164 0.47450396]\n",
      " [0.5357876  0.44963634 0.4393365 ]\n",
      " [0.46595553 0.4258757  0.4121536 ]\n",
      " [0.44449735 0.43418887 0.42871863]\n",
      " [0.43752223 0.4367644  0.41658205]\n",
      " [0.4401709  0.4253924  0.42354375]\n",
      " [0.43282616 0.42493603 0.42016625]\n",
      " [0.5205531  0.4908602  0.48563772]\n",
      " [0.45323554 0.43954736 0.42852587]\n",
      " [0.4799471  0.40500793 0.38388512]\n",
      " [0.3391864  0.3191486  0.31568322]\n",
      " [0.35581085 0.35377654 0.3500581 ]\n",
      " [0.35358697 0.32088435 0.31701875]\n",
      " [0.31249905 0.30106747 0.2702111 ]\n",
      " [0.28639412 0.2772738  0.2731249 ]\n",
      " [0.39788306 0.37500802 0.3583663 ]\n",
      " [0.2946459  0.27995166 0.2732296 ]\n",
      " [0.437032   0.433967   0.4299157 ]\n",
      " [0.33967903 0.3202113  0.2960524 ]\n",
      " [0.30102056 0.29671845 0.29262972]\n",
      " [0.3344743  0.2995842  0.29529813]\n",
      " [0.33062086 0.32574877 0.31852853]\n",
      " [0.39671165 0.39459795 0.38648593]\n",
      " [0.411432   0.38062084 0.3661326 ]\n",
      " [0.3925411  0.38592103 0.37393403]\n",
      " [0.41035813 0.4066692  0.39977726]\n",
      " [0.36257613 0.3621233  0.35467717]\n",
      " [0.44385317 0.43178827 0.41964063]\n",
      " [0.4931029  0.49184328 0.48041958]\n",
      " [0.4032797  0.3927648  0.38670588]\n",
      " [0.4638986  0.41937894 0.4092151 ]\n",
      " [0.41774324 0.41475064 0.39710706]\n",
      " [0.57041967 0.5525094  0.54946405]\n",
      " [0.47134638 0.45111173 0.44732487]\n",
      " [0.39848223 0.37048405 0.36564046]\n",
      " [0.40722033 0.39815804 0.38027892]\n",
      " [0.5489919  0.54394835 0.5418951 ]\n",
      " [0.52652085 0.5065489  0.5030612 ]\n",
      " [0.53693867 0.53614473 0.52417576]\n",
      " [0.54286486 0.5310258  0.5293989 ]\n",
      " [0.5490128  0.5204611  0.515295  ]\n",
      " [0.5736252  0.55342895 0.5487724 ]\n",
      " [0.49364972 0.4719066  0.45724812]\n",
      " [0.4762774  0.46344918 0.45730907]\n",
      " [0.46434927 0.45474553 0.4533086 ]\n",
      " [0.39484793 0.38665015 0.37161705]\n",
      " [0.5710533  0.5486664  0.5463343 ]\n",
      " [0.49388352 0.48685813 0.4830438 ]]\n",
      "Indices: [[94 26 79]\n",
      " [77 78 94]\n",
      " [94 26  1]\n",
      " [53 94 19]\n",
      " [79 89 26]\n",
      " [26 60 59]\n",
      " [79 19 90]\n",
      " [79 90 19]\n",
      " [82 83 84]\n",
      " [22 14 26]\n",
      " [86 84 89]\n",
      " [56 59 60]\n",
      " [56 26 60]\n",
      " [63 60 26]\n",
      " [79 90 60]\n",
      " [26 53 60]\n",
      " [26 53 94]\n",
      " [26 94 53]\n",
      " [89 84 67]\n",
      " [89 84 79]\n",
      " [89 84 87]\n",
      " [49 28 50]\n",
      " [84 81 87]\n",
      " [89 90 84]\n",
      " [84 89 86]\n",
      " [79 67 89]\n",
      " [79 67 89]\n",
      " [79 67 84]\n",
      " [79 77 67]\n",
      " [77 74 79]\n",
      " [78 79 91]\n",
      " [79 84 67]\n",
      " [79 67 66]\n",
      " [79 67 66]\n",
      " [79 78 77]\n",
      " [84 86 87]\n",
      " [89 79 87]\n",
      " [78 87 88]\n",
      " [77 89 26]\n",
      " [63 20 56]\n",
      " [63 79 59]\n",
      " [22 94 67]\n",
      " [19 75 79]\n",
      " [79 19 57]\n",
      " [77 95 76]\n",
      " [78 75 79]\n",
      " [78 77 79]\n",
      " [75 79 77]\n",
      " [75 77 79]\n",
      " [75 79 19]\n",
      " [66 26 67]\n",
      " [56 26 60]\n",
      " [56 26 60]\n",
      " [26 46 27]\n",
      " [26 60 53]\n",
      " [26 53 60]\n",
      " [63 26 56]\n",
      " [57 60 79]\n",
      " [53 26 57]\n",
      " [26 57 79]\n",
      " [56 26 60]\n",
      " [26 57 77]\n",
      " [26 57 77]\n",
      " [89 86 84]\n",
      " [79 60 39]\n",
      " [60 79 78]\n",
      " [60 26 56]\n",
      " [19 90 78]\n",
      " [78 77 74]\n",
      " [90 60 39]\n",
      " [74 78 77]\n",
      " [90 86 34]\n",
      " [94 92 45]\n",
      " [26 53 19]\n",
      " [51 49 60]\n",
      " [61 77 53]\n",
      " [65 79 60]\n",
      " [60 56 53]\n",
      " [19 84 78]\n",
      " [19 78 53]\n",
      " [60 19 39]\n",
      " [38 60 56]\n",
      " [49 50 88]\n",
      " [53 57 59]\n",
      " [94 19 77]\n",
      " [38 77 19]\n",
      " [89 79 18]\n",
      " [78 26 79]\n",
      " [78 19 79]\n",
      " [19 78 79]\n",
      " [87 78 86]\n",
      " [ 1 94 26]\n",
      " [26 89 60]\n",
      " [26 94 76]\n",
      " [26 81 84]\n",
      " [ 1 26 60]\n",
      " [ 1 79 53]\n",
      " [ 1 60 53]\n",
      " [60 53 26]\n",
      " [60 53 57]\n",
      " [60 53  1]\n",
      " [67 66 19]\n",
      " [26 93 20]\n",
      " [86 78 94]\n",
      " [26 81 78]\n",
      " [ 1 94 60]\n",
      " [77 95 78]]\n",
      "Prepared 107 matches\n",
      "Creating similarity links in Neo4j...\n",
      "matches:  dict_items([('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1104', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1531', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1588', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:856', np.float32(0.5528637)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:788', np.float32(0.5468018)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:763', np.float32(0.5363688))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1589', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1590', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1591', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:788', np.float32(0.51821434))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1592', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:841', np.float32(0.51728094))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1593', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1594', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1595', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1596', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1597', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1598', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1599', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1600', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1601', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1602', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1603', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1604', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1605', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1606', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1607', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1608', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1609', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1610', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1611', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:841', np.float32(0.5128733))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1612', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:841', np.float32(0.54511714))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1613', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1614', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1615', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1616', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1617', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1618', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:841', np.float32(0.54603714))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1619', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:841', np.float32(0.5123023))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1620', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:841', np.float32(0.50025594))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1621', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1622', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1623', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1624', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1625', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1626', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:825', np.float32(0.5050235))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1627', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1628', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1629', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1630', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1631', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1632', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:840', np.float32(0.5205001)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:839', np.float32(0.51482964))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1633', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1634', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1635', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1636', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1637', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:818', np.float32(0.546131))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1638', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:818', np.float32(0.5409855)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:788', np.float32(0.50793326))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1639', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:788', np.float32(0.5247976))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1640', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:788', np.float32(0.5054523))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1641', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1642', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:825', np.float32(0.514926)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:788', np.float32(0.50802326))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1643', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1644', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:815', np.float32(0.53689027)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:788', np.float32(0.5355166)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:819', np.float32(0.5340581))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1645', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:788', np.float32(0.5094662))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1646', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:818', np.float32(0.5357876))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1647', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1648', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1649', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1650', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1651', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1652', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:822', np.float32(0.5205531))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1653', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1654', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1655', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1656', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1657', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1658', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1659', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1660', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1661', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1662', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1663', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1664', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1665', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1666', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1667', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1668', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1669', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1670', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1671', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1672', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1673', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1674', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1675', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1676', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1677', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:763', np.float32(0.57041967)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:856', np.float32(0.5525094)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:788', np.float32(0.54946405))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1678', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1679', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1680', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1681', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:763', np.float32(0.5489919)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:788', np.float32(0.54394835)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:822', np.float32(0.5418951))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1682', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:763', np.float32(0.52652085)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:841', np.float32(0.5065489)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:815', np.float32(0.5030612))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1683', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:763', np.float32(0.53693867)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:822', np.float32(0.53614473)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:815', np.float32(0.52417576))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1684', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:822', np.float32(0.54286486)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:815', np.float32(0.5310258)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:788', np.float32(0.5293989))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1685', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:822', np.float32(0.5490128)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:815', np.float32(0.5204611)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:819', np.float32(0.515295))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1686', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:822', np.float32(0.5736252)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:815', np.float32(0.55342895)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:763', np.float32(0.5487724))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1687', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1688', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1689', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1690', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1691', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:763', np.float32(0.5710533)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:856', np.float32(0.5486664)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:822', np.float32(0.5463343))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1692', [])])\n",
      "Found 96 similar chunks for each chunk in graph graph_id_a\n",
      "Scores: [[0.43354437 0.398375   0.3898226 ]\n",
      " [0.5710533  0.57041967 0.5489919 ]\n",
      " [0.49169344 0.486844   0.4507264 ]\n",
      " [0.40193933 0.38087198 0.3659932 ]\n",
      " [0.34826002 0.3455066  0.34371442]\n",
      " [0.39201614 0.38820568 0.38414052]\n",
      " [0.25597832 0.23819613 0.22776724]\n",
      " [0.21559764 0.20787421 0.17758211]\n",
      " [0.22278577 0.21222392 0.19698808]\n",
      " [0.3470093  0.31743333 0.3173478 ]\n",
      " [0.46411702 0.44794676 0.4459234 ]\n",
      " [0.34125796 0.3361481  0.3241872 ]\n",
      " [0.4550977  0.42509973 0.419209  ]\n",
      " [0.4143463  0.4086594  0.4040704 ]\n",
      " [0.3986667  0.32563505 0.31825173]\n",
      " [0.34303936 0.32357374 0.31962705]\n",
      " [0.3632839  0.3557002  0.353256  ]\n",
      " [0.32385793 0.31957006 0.31866872]\n",
      " [0.46703845 0.45942783 0.45670548]\n",
      " [0.51160204 0.5094175  0.5049111 ]\n",
      " [0.48957384 0.48343512 0.46832323]\n",
      " [0.3835141  0.33866793 0.33801234]\n",
      " [0.45624483 0.45515436 0.45426825]\n",
      " [0.4178164  0.4163338  0.41294166]\n",
      " [0.36296785 0.36174253 0.34978944]\n",
      " [0.3352743  0.33010712 0.3273183 ]\n",
      " [0.54946405 0.5468018  0.5442125 ]\n",
      " [0.4593253  0.44343108 0.43661097]\n",
      " [0.4061059  0.37567735 0.3724908 ]\n",
      " [0.3012634  0.30114964 0.29928035]\n",
      " [0.3617444  0.35250327 0.35068017]\n",
      " [0.38207802 0.37468612 0.37205368]\n",
      " [0.4408403  0.4144202  0.40948564]\n",
      " [0.3807075  0.3775132  0.3722862 ]\n",
      " [0.399005   0.39724338 0.38993457]\n",
      " [0.38703594 0.32687506 0.31937355]\n",
      " [0.40504357 0.4027152  0.4025113 ]\n",
      " [0.37432078 0.36660606 0.3616201 ]\n",
      " [0.4363459  0.4278384  0.40238523]\n",
      " [0.42354375 0.40562534 0.40228   ]\n",
      " [0.40334252 0.3935854  0.39345136]\n",
      " [0.33889496 0.30809346 0.29985106]\n",
      " [0.32359087 0.32286578 0.3195425 ]\n",
      " [0.35588935 0.3529924  0.331957  ]\n",
      " [0.3224949  0.31246197 0.3044129 ]\n",
      " [0.29344577 0.2904181  0.28935054]\n",
      " [0.46211505 0.43806243 0.43331605]\n",
      " [0.41028905 0.40754205 0.40275425]\n",
      " [0.35958102 0.3553431  0.35221624]\n",
      " [0.43981263 0.41590428 0.411432  ]\n",
      " [0.39542985 0.38459098 0.38062084]\n",
      " [0.4134112  0.40703613 0.3997074 ]\n",
      " [0.42940277 0.42780873 0.42202693]\n",
      " [0.55342907 0.53689027 0.53102595]\n",
      " [0.4785274  0.47838202 0.4773273 ]\n",
      " [0.47799733 0.45044887 0.44155145]\n",
      " [0.546131   0.5409855  0.5357876 ]\n",
      " [0.5340581  0.5337147  0.5174956 ]\n",
      " [0.43032727 0.4094401  0.40885216]\n",
      " [0.48637813 0.46597046 0.45816538]\n",
      " [0.5736252  0.5490128  0.5463343 ]\n",
      " [0.3448529  0.33413792 0.32712135]\n",
      " [0.40038714 0.39931452 0.3969412 ]\n",
      " [0.514926   0.5050235  0.49972314]\n",
      " [0.3993468  0.3913953  0.391347  ]\n",
      " [0.44044393 0.437032   0.42862263]\n",
      " [0.47190666 0.4500231  0.43153873]\n",
      " [0.4936496  0.47887215 0.46564472]\n",
      " [0.32889983 0.3163246  0.29978666]\n",
      " [0.35644135 0.32862693 0.31624165]\n",
      " [0.39175624 0.38236278 0.3761898 ]\n",
      " [0.3365199  0.3170631  0.30667934]\n",
      " [0.36210376 0.35857552 0.35545376]\n",
      " [0.45255217 0.45089853 0.4496418 ]\n",
      " [0.45652154 0.4202129  0.38515526]\n",
      " [0.48846126 0.48298153 0.4614725 ]\n",
      " [0.47612047 0.45092824 0.44966808]\n",
      " [0.5148297  0.49388352 0.47097898]\n",
      " [0.5206953  0.52050006 0.5145761 ]\n",
      " [0.546037   0.5451171  0.51728106]\n",
      " [0.45930576 0.45613664 0.44684032]\n",
      " [0.40017205 0.39815792 0.39052784]\n",
      " [0.51925015 0.51731443 0.48386988]\n",
      " [0.53197396 0.52717096 0.46421883]\n",
      " [0.5212151  0.50638527 0.48371804]\n",
      " [0.4597641  0.45130384 0.43463668]\n",
      " [0.5165963  0.49431384 0.47470966]\n",
      " [0.46846431 0.45574123 0.45555   ]\n",
      " [0.50021756 0.48819762 0.4728516 ]\n",
      " [0.47319996 0.47224188 0.47190863]\n",
      " [0.4730366  0.46886098 0.46703103]\n",
      " [0.48414204 0.48209855 0.46442482]\n",
      " [0.53459966 0.5176028  0.5052897 ]\n",
      " [0.463449   0.45472944 0.4468911 ]\n",
      " [0.5528639  0.5525094  0.5486664 ]\n",
      " [0.5462887  0.546031   0.51563823]]\n",
      "Indices: [[102  87  92]\n",
      " [105  91  95]\n",
      " [ 91 105  95]\n",
      " [  2  95  97]\n",
      " [105   2  91]\n",
      " [105  95  91]\n",
      " [102 105  91]\n",
      " [ 84   3  74]\n",
      " [ 83  73 102]\n",
      " [ 21  91 105]\n",
      " [ 58  98  59]\n",
      " [ 97  51  95]\n",
      " [ 46  96  48]\n",
      " [ 91 105 100]\n",
      " [  9   2  43]\n",
      " [ 43   9   6]\n",
      " [105  95   2]\n",
      " [100 101  43]\n",
      " [ 54  58 105]\n",
      " [ 91 100 105]\n",
      " [105  91   2]\n",
      " [102 105   2]\n",
      " [105  95   2]\n",
      " [  2 105 102]\n",
      " [ 30  50 102]\n",
      " [ 95 105   2]\n",
      " [ 91   2 105]\n",
      " [ 53 102 105]\n",
      " [ 21 105  91]\n",
      " [ 95 105 100]\n",
      " [  2  97  95]\n",
      " [100  64  99]\n",
      " [100  95   2]\n",
      " [ 14   2  40]\n",
      " [ 13 102  14]\n",
      " [ 50  97  95]\n",
      " [105  50  98]\n",
      " [ 40  13 102]\n",
      " [ 12  13  55]\n",
      " [ 64  58  55]\n",
      " [ 14  46  58]\n",
      " [ 49 102  66]\n",
      " [105   6  49]\n",
      " [ 64   6  14]\n",
      " [ 50 102  98]\n",
      " [ 74  82   3]\n",
      " [ 53  54  95]\n",
      " [ 54  95  15]\n",
      " [ 21  74  56]\n",
      " [ 21 100  82]\n",
      " [ 21  97  82]\n",
      " [ 56  51  52]\n",
      " [ 58  91  98]\n",
      " [100  58  98]\n",
      " [ 91 105  54]\n",
      " [ 58  59  98]\n",
      " [ 51  52  60]\n",
      " [ 58 100 105]\n",
      " [ 45  91 105]\n",
      " [ 40  54   5]\n",
      " [100  99 105]\n",
      " [ 83 100 102]\n",
      " [100  91 105]\n",
      " [ 56  40  58]\n",
      " [ 33  76  58]\n",
      " [ 58  76  56]\n",
      " [101  50  32]\n",
      " [101  32  25]\n",
      " [ 51  97  95]\n",
      " [102   5  66]\n",
      " [ 56  54  51]\n",
      " [ 74  49  51]\n",
      " [102  56  97]\n",
      " [ 66  98  97]\n",
      " [ 46  48  96]\n",
      " [ 48  49  47]\n",
      " [106  87  91]\n",
      " [ 46 106  91]\n",
      " [ 91  46 105]\n",
      " [ 32  26   6]\n",
      " [105  91  58]\n",
      " [ 31  94  96]\n",
      " [105  91  95]\n",
      " [ 91 105 100]\n",
      " [ 91 105 100]\n",
      " [ 91 105  30]\n",
      " [ 91 105 100]\n",
      " [ 91  87 105]\n",
      " [ 91 105  87]\n",
      " [ 96  91  19]\n",
      " [100   6  58]\n",
      " [105  91 100]\n",
      " [  2 100 105]\n",
      " [102  87  91]\n",
      " [  2  91 105]\n",
      " [105  91  95]]\n",
      "Prepared 96 reverse matches\n",
      "Creating reverse similarity links...\n",
      "matches:  dict_items([('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:762', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:763', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1691', np.float32(0.5710533)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1677', np.float32(0.57041967)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1681', np.float32(0.5489919))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:764', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:765', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:766', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:767', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:768', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:769', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:770', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:771', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:772', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:773', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:774', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:775', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:776', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:777', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:778', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:779', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:780', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:781', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1677', np.float32(0.51160204)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1686', np.float32(0.5094175)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1691', np.float32(0.5049111))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:782', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:783', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:784', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:785', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:786', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:787', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:788', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1677', np.float32(0.54946405)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1588', np.float32(0.5468018)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1691', np.float32(0.5442125))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:789', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:790', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:791', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:792', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:793', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:794', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:795', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:796', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:797', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:798', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:799', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:800', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:801', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:802', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:803', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:804', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:805', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:806', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:807', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:808', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:809', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:810', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:811', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:812', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:813', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:814', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:815', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1686', np.float32(0.55342907)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1644', np.float32(0.53689027)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1684', np.float32(0.53102595))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:816', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:817', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:818', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1637', np.float32(0.546131)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1638', np.float32(0.5409855)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1646', np.float32(0.5357876))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:819', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1644', np.float32(0.5340581)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1686', np.float32(0.5337147)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1691', np.float32(0.5174956))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:820', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:821', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:822', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1686', np.float32(0.5736252)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1685', np.float32(0.5490128)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1691', np.float32(0.5463343))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:823', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:824', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:825', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1642', np.float32(0.514926)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1626', np.float32(0.5050235))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:826', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:827', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:828', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:829', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:830', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:831', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:832', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:833', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:834', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:835', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:836', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:837', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:838', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:839', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1632', np.float32(0.5148297))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:840', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1677', np.float32(0.5206953)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1632', np.float32(0.52050006)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1691', np.float32(0.5145761))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:841', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1618', np.float32(0.546037)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1612', np.float32(0.5451171)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1592', np.float32(0.51728106))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:842', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:843', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:844', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1691', np.float32(0.51925015)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1677', np.float32(0.51731443))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:845', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1677', np.float32(0.53197396)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1691', np.float32(0.52717096))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:846', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1677', np.float32(0.5212151)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1691', np.float32(0.50638527))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:847', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:848', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1677', np.float32(0.5165963))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:849', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:850', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1677', np.float32(0.50021756))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:851', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:852', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:853', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:854', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1588', np.float32(0.53459966)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1686', np.float32(0.5176028)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1691', np.float32(0.5052897))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:855', []), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:856', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1588', np.float32(0.5528639)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1677', np.float32(0.5525094)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1691', np.float32(0.5486664))]), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:857', [('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1691', np.float32(0.5462887)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1677', np.float32(0.546031)), ('4:d3d77976-d4c2-4ea7-b3a3-5e6effb68005:1681', np.float32(0.51563823))])])\n",
      "âœ… Created 97 cross-graph similarity links\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "import neo4j\n",
    "from typing import Dict\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import faiss\n",
    "from rdflib import Graph\n",
    "\n",
    "from neo4j_graphrag.llm import LLMInterface\n",
    "from neo4j_graphrag.embeddings import OpenAIEmbeddings\n",
    "from neo4j_graphrag.experimental.pipeline.kg_builder import SimpleKGPipeline\n",
    "from neo4j_graphrag.llm import OpenAILLM\n",
    "\n",
    "\n",
    "URI = \"neo4j://localhost:7687\"\n",
    "USERNAME = \"neo4j\"\n",
    "PASSWORD = \"\"\n",
    "AUTH = (USERNAME, PASSWORD)\n",
    "\n",
    "def clear_database(driver):\n",
    "    with driver.session() as session:\n",
    "        session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "        print(\"Database cleaned.\")\n",
    "\n",
    "def initialize_vector_index(driver):\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            session.run(\"\"\"\n",
    "            CALL db.index.vector.createNodeIndex(\n",
    "                'legal_text_embeddings',\n",
    "                'Paragraph',\n",
    "                'embedding',\n",
    "                1536,\n",
    "                'cosine'\n",
    "            )\n",
    "            \"\"\")\n",
    "            print(\"Vectorial index created.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to create index: {e}\")\n",
    "\n",
    "def fetch_chunks(tx, graph_id):\n",
    "    \"\"\"\n",
    "    Fetch chunks with embeddings from a specific knowledge graph.\n",
    "\n",
    "    Args:\n",
    "        tx: Neo4j transaction\n",
    "        graph_id: Identifier for the knowledge graph\n",
    "\n",
    "    Returns:\n",
    "        List of tuples containing (node_id, embedding)\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    MATCH (c:Chunk)\n",
    "    WHERE c.graph_id = $graph_id AND c.embedding IS NOT NULL\n",
    "    RETURN elementId(c) AS id, c.embedding AS embedding, c.text AS text\n",
    "    \"\"\"\n",
    "    result = tx.run(query, graph_id=graph_id)\n",
    "\n",
    "    # Since embeddings are already lists in Neo4j, we can use them directly\n",
    "    return [(r[\"id\"], r[\"embedding\"], r[\"text\"]) for r in result]\n",
    "\n",
    "def create_similarity_links(tx, matches):\n",
    "    \"\"\"\n",
    "    Create SIMILAR relationships between chunks based on similarity scores.\n",
    "\n",
    "    Args:\n",
    "        tx: Neo4j transaction\n",
    "        matches: Dictionary mapping source node IDs to list of (target_id, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"matches: \", matches.items())\n",
    "    batch = []\n",
    "    for src_id, targets in matches.items():\n",
    "        for tgt_id, score in targets:\n",
    "            if score >= 0.5:  # Similarity threshold\n",
    "                batch.append({\"src\": src_id, \"tgt\": tgt_id, \"score\": float(score)})\n",
    "\n",
    "    if batch:\n",
    "        tx.run(\"\"\"\n",
    "            UNWIND $batch AS row\n",
    "            MATCH (a:Chunk), (b:Chunk)\n",
    "            WHERE elementId(a) = row.src AND elementId(b) = row.tgt\n",
    "            MERGE (a)-[r:SIMILAR]->(b)\n",
    "            SET r.score = row.score\n",
    "        \"\"\", batch=batch)\n",
    "\n",
    "    return len(batch)\n",
    "\n",
    "# Run the pipeline for processing legal documents\n",
    "async def define_and_run_pipeline(\n",
    "    neo4j_driver: neo4j.Driver,\n",
    "    llm: LLMInterface,\n",
    "    embedder,\n",
    "    neo4j_schema\n",
    ") -> Dict:\n",
    "\n",
    "    legal_path = Path(\"legal_corpus/\")\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    structure_splitter = None\n",
    "    for file_path in legal_path.glob(\"*.txt\"):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            print(\"Processing file:\", file_path.name)\n",
    "            text = file.read()\n",
    "            if file_path.name == \"1Constitution.txt\" or file_path.name == \"1Constitution_small.txt\":\n",
    "                structure_splitter = ConstitutionSplitter(llm=llm, graph_id=\"Constitution\")\n",
    "            if file_path.name == \"2Prospectus.txt\" or file_path.name == \"2Prospectus_small.txt\":\n",
    "                structure_splitter = ProspectusSplitter(llm=llm, graph_id=\"Prospectus\")\n",
    "            if file_path.name == \"3Agreement.txt\" or file_path.name == \"3Agreement_small.txt\":\n",
    "                structure_splitter = ShareholdersAgreementSplitter(llm=llm, graph_id=\"Agreement\")\n",
    "\n",
    "            structure_pipeline = SimpleKGPipeline(\n",
    "                llm=llm,\n",
    "                driver=neo4j_driver,\n",
    "                text_splitter=structure_splitter, \n",
    "                #text_splitter=FixedSizeSplitter(chunk_size=2500, chunk_overlap=10),\n",
    "                #embedder=SentenceTransformerEmbeddings(model='all-MiniLM-L6-v2'),\n",
    "                embedder=embedder,\n",
    "                entities=list(neo4j_schema.entities.values()),\n",
    "                relations=list(neo4j_schema.relations.values()),\n",
    "                potential_schema=neo4j_schema.potential_schema,\n",
    "                on_error=\"IGNORE\",\n",
    "                from_pdf=False\n",
    "            )\n",
    "\n",
    "            asyncio.run(structure_pipeline.run_async(text=text))\n",
    "\n",
    "def merge_graphs(driver, graph_id_a, graph_id_b):\n",
    "    stats = {\"relationships_created\": 0}\n",
    "\n",
    "    with driver.session() as session:\n",
    "        # Fetch chunks from both graphs\n",
    "        print(f\"Fetching chunks from graph {graph_id_a}\")\n",
    "        chunks_a = session.execute_read(fetch_chunks, graph_id_a)\n",
    "        print(f\"Found {len(chunks_a)} chunks with embeddings\")\n",
    "\n",
    "        print(f\"Fetching chunks from graph {graph_id_b}\")\n",
    "        chunks_b = session.execute_read(fetch_chunks, graph_id_b)\n",
    "        print(f\"Found {len(chunks_b)} chunks with embeddings\")\n",
    "\n",
    "        if not chunks_a or not chunks_b:\n",
    "            print(\"Warning: One or both graphs have no chunks with embeddings\")\n",
    "            #return stats\n",
    "\n",
    "        # Extract IDs and embeddings\n",
    "        ids_a, emb_a, texts_a = zip(*chunks_a) if chunks_a else ([], [], [])\n",
    "        ids_b, emb_b, texts_b = zip(*chunks_b) if chunks_b else ([], [], [])\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        emb_a = np.array(emb_a).astype('float32')\n",
    "        emb_b = np.array(emb_b).astype('float32')\n",
    "\n",
    "        # Handle empty arrays\n",
    "        if emb_a.size == 0 or emb_b.size == 0:\n",
    "            print(\"Warning: No embeddings found in one or both graphs\")\n",
    "            #return stats\n",
    "\n",
    "        # Normalize for cosine similarity\n",
    "        faiss.normalize_L2(emb_a)\n",
    "        faiss.normalize_L2(emb_b)\n",
    "\n",
    "        # Build similarity index\n",
    "        d = emb_a.shape[1]  # Embedding dimension\n",
    "        print(f\"Building FAISS index with {len(ids_a)} vectors of dimension {d}...\")\n",
    "        index = faiss.IndexFlatIP(d)  # Inner product = cosine similarity for normalized vectors\n",
    "        index.add(emb_a)\n",
    "\n",
    "        # Perform similarity search\n",
    "        print(f\"Finding top {3} similar chunks for each chunk in graph graph_id_b\")\n",
    "        k = min(3, len(ids_a))  # Make sure k isn't larger than available chunks\n",
    "        scores, indices = index.search(emb_b, k)\n",
    "        print(f\"Found {len(indices)} similar chunks for each chunk in graph graph_id_b\")\n",
    "        print(f\"Scores: {scores}\")\n",
    "        print(f\"Indices: {indices}\")\n",
    "\n",
    "        # Prepare matches\n",
    "        matches = {}\n",
    "        for i in range(len(ids_b)):\n",
    "            matches[ids_b[i]] = [(ids_a[indices[i][j]], scores[i][j])\n",
    "                                for j in range(k)\n",
    "                                if scores[i][j] >= 0.5]\n",
    "\n",
    "        print(f\"Prepared {len(matches)} matches\")\n",
    "\n",
    "        # Store links in Neo4j\n",
    "        print(\"Creating similarity links in Neo4j...\")\n",
    "        stats[\"relationships_created\"] = session.execute_write(\n",
    "            create_similarity_links, matches\n",
    "        )\n",
    "\n",
    "        # Also create links in the reverse direction (B â†’ A)\n",
    "        # Build reverse index\n",
    "        index_reverse = faiss.IndexFlatIP(d)\n",
    "        index_reverse.add(emb_b)\n",
    "\n",
    "        # Search A chunks in B\n",
    "        scores_reverse, indices_reverse = index_reverse.search(emb_a, k)\n",
    "        print(f\"Found {len(indices_reverse)} similar chunks for each chunk in graph graph_id_a\")\n",
    "        print(f\"Scores: {scores_reverse}\")\n",
    "        print(f\"Indices: {indices_reverse}\")\n",
    "\n",
    "        # Prepare reverse matches\n",
    "        matches_reverse = {}\n",
    "        for i in range(len(ids_a)):\n",
    "            matches_reverse[ids_a[i]] = [(ids_b[indices_reverse[i][j]], scores_reverse[i][j])\n",
    "                                      for j in range(k)\n",
    "                                      if scores_reverse[i][j] >= 0.5]\n",
    "\n",
    "        print(f\"Prepared {len(matches_reverse)} reverse matches\")\n",
    "\n",
    "        # Store reverse links\n",
    "        print(\"Creating reverse similarity links...\")\n",
    "        stats[\"relationships_created\"] += session.execute_write(\n",
    "            create_similarity_links, matches_reverse\n",
    "        )\n",
    "\n",
    "        # Verify links in database\n",
    "        count_query = \"\"\"\n",
    "        MATCH ()-[r:SIMILAR]->()\n",
    "        RETURN count(r) as count\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = session.run(count_query).single()\n",
    "            stats[\"total_similar_links\"] = result[\"count\"] if result else 0\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Couldn't count total relationships: {e}\")\n",
    "            stats[\"total_similar_links\"] = 0\n",
    "\n",
    "        print(f\"âœ… Created {stats['relationships_created']} cross-graph similarity links\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    g = Graph()\n",
    "    \n",
    "    # Get the schema from the ontology file\n",
    "    neo4j_schema = getSchemaFromOnto(g.parse(\"ontology.ttl\"))\n",
    "    # Set up the llm\n",
    "    llm = OpenAILLM(\n",
    "        api_key=\"hide\",\n",
    "        model_name=\"gpt-4o-mini\",\n",
    "        #model_name=\"gpt-4.1\",\n",
    "        model_params={\n",
    "            \"max_tokens\": 5000,\n",
    "            \"response_format\": {\"type\": \"json_object\"},\n",
    "            \"temperature\": 0,\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    embedder = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=\"hide\")\n",
    "    \n",
    "    with neo4j.GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "        print(\"Clearing database...\")\n",
    "        clear_database(driver)\n",
    "\n",
    "        print(\"Initializing vector index...\")\n",
    "        initialize_vector_index(driver)\n",
    "\n",
    "        print(\"Building knowledge graph...\")\n",
    "        res = await define_and_run_pipeline(driver, llm, embedder, neo4j_schema)\n",
    "\n",
    "        # Merge graphs A => B => C\n",
    "        print(\"Merging graphs...\")\n",
    "        merge_graphs(driver, \"Constitution\", \"Prospectus\")\n",
    "        merge_graphs(driver, \"Prospectus\", \"Agreement\")\n",
    "\n",
    "res = await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ce898d",
   "metadata": {},
   "source": [
    "# Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e991a870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 22:29:41,507 - INFO - Starting taxonomy extraction process\n",
      "2025-06-10 22:29:45,708 - INFO - Optimized 498 SIMILAR relationships\n",
      "2025-06-10 22:29:45,826 - INFO - Set default weights for 0 SIMILAR relationships\n",
      "2025-06-10 22:29:46,223 - INFO - Created graph projection: legal-taxonomy-graph-1749587385827 with 2301 nodes and 8196 relationships\n",
      "2025-06-10 22:29:47,474 - INFO - Detected communities with 380 nodes\n",
      "2025-06-10 22:29:49,093 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 22:29:49,101 - INFO - Generated category name for community 325: 'Investor Share Redemption Laws'\n",
      "2025-06-10 22:29:50,254 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 22:29:50,256 - INFO - Generated category name for community 415: 'Share Conversion Regulations'\n",
      "2025-06-10 22:29:51,015 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 22:29:51,018 - INFO - Generated category name for community 561: 'Investment Fund Legal Documentation'\n",
      "2025-06-10 22:29:52,295 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 22:29:52,297 - INFO - Generated category name for community 582: 'Corporate Insurance Policies'\n",
      "2025-06-10 22:29:53,555 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 22:29:53,557 - INFO - Generated category name for community 618: 'Company Asset Management Laws'\n",
      "2025-06-10 22:29:54,366 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 22:29:54,367 - INFO - Generated category name for community 759: 'Investment Prospectus Documentation'\n",
      "2025-06-10 22:29:55,258 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 22:29:55,260 - INFO - Generated category name for community 949: 'Investment Partnership Agreements'\n",
      "2025-06-10 22:29:56,706 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 22:29:56,714 - INFO - Generated category name for community 1109: 'Equity Issuance Legal Texts'\n",
      "2025-06-10 22:29:57,514 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 22:29:57,517 - INFO - Generated category name for community 1151: 'Investment Risk Disclosures'\n",
      "2025-06-10 22:29:58,377 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 22:29:58,381 - INFO - Generated category name for community 1214: 'Bank Resolution Risks Law'\n",
      "2025-06-10 22:29:59,146 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 22:29:59,147 - INFO - Generated category name for community 1230: 'Investment and Shareholding Regulations'\n",
      "2025-06-10 22:30:00,376 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 22:30:00,378 - INFO - Generated category name for community 1253: 'ESG Integration Policies'\n",
      "2025-06-10 22:30:01,579 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 22:30:01,581 - INFO - Generated category name for community 1353: 'Investment Subscription Procedures'\n",
      "2025-06-10 22:30:02,562 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 22:30:02,572 - INFO - Generated category name for community 1373: 'Shareholder Rights and Transactions'\n",
      "2025-06-10 22:30:03,477 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 22:30:03,479 - INFO - Generated category name for community 1392: 'Corporate Finance and Taxation Law'\n",
      "2025-06-10 22:30:04,517 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 22:30:04,519 - INFO - Generated category name for community 1397: 'Securities Conversion Regulations'\n",
      "2025-06-10 22:30:05,711 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 22:30:05,712 - INFO - Generated category name for community 1414: 'Financial Law Documentation'\n",
      "2025-06-10 22:30:06,439 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 22:30:06,441 - INFO - Generated category name for community 1482: 'Corporate Meeting Regulations'\n",
      "2025-06-10 22:30:07,347 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 22:30:07,348 - INFO - Generated category name for community 1491: 'Corporate Structure Legislation'\n",
      "2025-06-10 22:30:08,127 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 22:30:08,129 - INFO - Generated category name for community 1709: 'Corporate Acquisition Agreements'\n",
      "2025-06-10 22:30:09,037 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 22:30:09,038 - INFO - Generated category name for community 1801: 'Equity Securities Regulations'\n",
      "2025-06-10 22:30:09,761 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 22:30:09,764 - INFO - Generated category name for community 1831: 'Contractual Terms and Conditions'\n",
      "2025-06-10 22:30:10,467 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 22:30:10,469 - INFO - Generated category name for community 2107: 'Share Transfer Agreements'\n",
      "2025-06-10 22:30:11,701 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 22:30:11,709 - INFO - Generated category name for community 2135: 'Default Management Procedures'\n",
      "2025-06-10 22:30:11,821 - INFO - Created 24 taxonomy category nodes with LLM-generated names\n",
      "2025-06-10 22:30:13,028 - INFO - Created 370 BELONGS_TO relationships\n",
      "2025-06-10 22:30:13,082 - INFO - Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE CONSTRAINT taxonomy_category_id IF NOT EXISTS FOR (e:TaxonomyCategory) REQUIRE (e.id) IS UNIQUE` has no effect.} {description: `CONSTRAINT taxonomy_category_id FOR (e:TaxonomyCategory) REQUIRE (e.id) IS UNIQUE` already exists.} {position: None} for query: '\\n            CREATE CONSTRAINT taxonomy_category_id IF NOT EXISTS\\n            FOR (t:TaxonomyCategory) REQUIRE t.id IS UNIQUE\\n            '\n",
      "2025-06-10 22:30:13,089 - INFO - Created constraint on TaxonomyCategory.id\n",
      "2025-06-10 22:30:13,100 - INFO - Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE RANGE INDEX taxonomy_category_name IF NOT EXISTS FOR (e:TaxonomyCategory) ON (e.name)` has no effect.} {description: `RANGE INDEX taxonomy_category_name FOR (e:TaxonomyCategory) ON (e.name)` already exists.} {position: None} for query: '\\n            CREATE INDEX taxonomy_category_name IF NOT EXISTS\\n            FOR (t:TaxonomyCategory) ON (t.name)\\n            '\n",
      "2025-06-10 22:30:13,101 - INFO - Created index on TaxonomyCategory.name\n",
      "2025-06-10 22:30:13,103 - INFO - Taxonomy extraction completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Hierarchical Taxonomy :\n",
      "* Investment Prospectus Documentation (30 nodes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 22:30:13,524 - INFO - Starting LLM-based taxonomy hierarchy rebuild\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Investment Risk Disclosures (30 nodes)\n",
      "* Investor Share Redemption Laws (28 nodes)\n",
      "* Investment Subscription Procedures (26 nodes)\n",
      "* Investment and Shareholding Regulations (24 nodes)\n",
      "* Equity Issuance Legal Texts (20 nodes)\n",
      "* Investment Fund Legal Documentation (19 nodes)\n",
      "* Company Asset Management Laws (18 nodes)\n",
      "* Corporate Acquisition Agreements (18 nodes)\n",
      "* Equity Securities Regulations (17 nodes)\n",
      "* Corporate Finance and Taxation Law (16 nodes)\n",
      "* Corporate Meeting Regulations (16 nodes)\n",
      "* Bank Resolution Risks Law (10 nodes)\n",
      "* ESG Integration Policies (10 nodes)\n",
      "* Corporate Structure Legislation (10 nodes)\n",
      "* Contractual Terms and Conditions (10 nodes)\n",
      "* Default Management Procedures (10 nodes)\n",
      "* Share Conversion Regulations (9 nodes)\n",
      "* Shareholder Rights and Transactions (9 nodes)\n",
      "* Investment Partnership Agreements (8 nodes)\n",
      "* Corporate Insurance Policies (8 nodes)\n",
      "* Securities Conversion Regulations (8 nodes)\n",
      "* Financial Law Documentation (8 nodes)\n",
      "* Share Transfer Agreements (8 nodes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 22:30:40,109 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 22:30:40,113 - INFO - Successfully generated hierarchy structure with LLM\n",
      "2025-06-10 22:30:40,219 - INFO - Created 7 new parent taxonomy categories\n",
      "2025-06-10 22:30:40,335 - INFO - Created 29 SUBCATEGORY_OF relationships\n",
      "2025-06-10 22:30:40,561 - INFO - Updated sizes for 7 parent categories\n",
      "2025-06-10 22:30:40,561 - INFO - Successfully created LLM-generated hierarchical taxonomy\n",
      "2025-06-10 22:30:40,562 - INFO - Successfully rebuilt taxonomy with LLM-generated hierarchy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Hierarchical Taxonomy :\n",
      "* Legal Document Categories (342 nodes)\n",
      "  * Investment Law and Regulations (137 nodes)\n",
      "    * Investment Prospectus Documentation (30 nodes)\n",
      "    * Investment Risk Disclosures (30 nodes)\n",
      "    * Investment Subscription Procedures (26 nodes)\n",
      "    * Investment and Shareholding Regulations (24 nodes)\n",
      "    * Investment Fund Legal Documentation (19 nodes)\n",
      "    * Investment Partnership Agreements (8 nodes)\n",
      "  * Corporate Law and Governance (86 nodes)\n",
      "    * Company Asset Management Laws (18 nodes)\n",
      "    * Corporate Acquisition Agreements (18 nodes)\n",
      "    * Corporate Finance and Taxation Law (16 nodes)\n",
      "    * Corporate Meeting Regulations (16 nodes)\n",
      "    * Corporate Structure Legislation (10 nodes)\n",
      "    * Corporate Insurance Policies (8 nodes)\n",
      "  * Securities and Equity Law (71 nodes)\n",
      "    * Equity Issuance Legal Texts (20 nodes)\n",
      "    * Equity Securities Regulations (17 nodes)\n",
      "    * Share Conversion Regulations (9 nodes)\n",
      "    * Shareholder Rights and Transactions (9 nodes)\n",
      "    * Securities Conversion Regulations (8 nodes)\n",
      "    * Share Transfer Agreements (8 nodes)\n",
      "  * Contractual Law and Regulations (20 nodes)\n",
      "    * Contractual Terms and Conditions (10 nodes)\n",
      "    * Default Management Procedures (10 nodes)\n",
      "  * Financial Law and Regulations (18 nodes)\n",
      "    * Bank Resolution Risks Law (10 nodes)\n",
      "    * Financial Law Documentation (8 nodes)\n",
      "  * Environmental, Social, and Governance Policies (10 nodes)\n",
      "    * ESG Integration Policies (10 nodes)\n",
      "* Investor Share Redemption Laws (28 nodes)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from neo4j import GraphDatabase\n",
    "import re\n",
    "import logging\n",
    "import random\n",
    "import json\n",
    "from datetime import datetime\n",
    "import openai\n",
    "from typing import Dict, List\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class LegalTaxonomyExtractor:\n",
    "    def __init__(self, uri, username, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "        \n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "        \n",
    "    def run_query(self, query, params=None):\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(query, params or {})\n",
    "            return list(result)\n",
    "            \n",
    "    def optimize_similarity_relationships(self):\n",
    "        \"\"\"Ensure similarity relationships have weight properties.\"\"\"\n",
    "        query = \"\"\"\n",
    "        MATCH (a)-[r:SIMILAR]->(b)\n",
    "        WHERE r.score IS NOT NULL AND r.weight IS NULL\n",
    "        SET r.weight = r.score\n",
    "        RETURN count(r) as updated\n",
    "        \"\"\"\n",
    "        result = self.run_query(query)\n",
    "        logger.info(f\"Optimized {result[0]['updated']} SIMILAR relationships\")\n",
    "        \n",
    "        # For relationships that might not have a score\n",
    "        query = \"\"\"\n",
    "        MATCH (a)-[r:SIMILAR]->(b)\n",
    "        WHERE r.score IS NULL AND r.weight IS NULL\n",
    "        SET r.weight = 0.5, r.score = 0.5\n",
    "        RETURN count(r) as updated\n",
    "        \"\"\"\n",
    "        result = self.run_query(query)\n",
    "        logger.info(f\"Set default weights for {result[0]['updated']} SIMILAR relationships\")\n",
    "        \n",
    "    def create_graph_projection(self, graph_name=\"legal-taxonomy-graph\"):\n",
    "        \"\"\"Create an in-memory graph projection for community detection.\"\"\"\n",
    "        if graph_name == \"legal-taxonomy-graph\":\n",
    "            timestamp = int(datetime.now().timestamp() * 1000)\n",
    "            graph_name = f\"legal-taxonomy-graph-{timestamp}\"\n",
    "        \n",
    "        # First, check if a graph with this name already exists\n",
    "        check_query = \"\"\"\n",
    "        CALL gds.graph.exists($graph_name) YIELD exists\n",
    "        RETURN exists\n",
    "        \"\"\"\n",
    "        result = self.run_query(check_query, {\"graph_name\": graph_name})\n",
    "        \n",
    "        if result and result[0][\"exists\"]:\n",
    "            # If graph exists, drop it first\n",
    "            drop_query = \"\"\"\n",
    "            CALL gds.graph.drop($graph_name)\n",
    "            YIELD graphName\n",
    "            RETURN graphName\n",
    "            \"\"\"\n",
    "            self.run_query(drop_query, {\"graph_name\": graph_name})\n",
    "            logger.info(f\"Dropped existing graph projection: {graph_name}\")\n",
    "        \n",
    "        # Create new graph projection\n",
    "        query = \"\"\"\n",
    "        CALL gds.graph.project(\n",
    "            $graph_name,\n",
    "            '*',\n",
    "            {\n",
    "                relType: {\n",
    "                    type: '*',\n",
    "                    orientation: 'UNDIRECTED',\n",
    "                    properties: {}\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        YIELD graphName, nodeCount, relationshipCount\n",
    "        RETURN graphName, nodeCount, relationshipCount\n",
    "        \"\"\"\n",
    "        result = self.run_query(query, {\"graph_name\": graph_name})\n",
    "        \n",
    "        if result:\n",
    "            logger.info(f\"Created graph projection: {result[0]['graphName']} with {result[0]['nodeCount']} nodes and {result[0]['relationshipCount']} relationships\")\n",
    "            return graph_name\n",
    "        else:\n",
    "            logger.error(\"Failed to create graph projection\")\n",
    "            return None\n",
    "        \n",
    "    def run_community_detection(self, graph_name=\"legal-taxonomy-graph\", limit=42, community_node_limit=10):\n",
    "        \"\"\"Run Louvain community detection algorithm on the projected graph.\"\"\"\n",
    "        query = \"\"\"\n",
    "        CALL gds.louvain.stream($graph_name, {\n",
    "            relationshipWeightProperty: null,\n",
    "            includeIntermediateCommunities: true,\n",
    "            seedProperty: ''\n",
    "        })\n",
    "        YIELD nodeId, communityId AS community, intermediateCommunityIds AS communities\n",
    "        WITH gds.util.asNode(nodeId) AS node, community, communities\n",
    "        WITH community, communities, collect(node) AS nodes\n",
    "        WITH community, communities, nodes, size(nodes) AS size\n",
    "        ORDER BY size DESC\n",
    "        LIMIT toInteger($limit)\n",
    "        UNWIND nodes[0..$community_node_limit] AS node\n",
    "        RETURN \n",
    "            node.id AS nodeId,\n",
    "            node.text AS text,\n",
    "            community,\n",
    "            communities AS hierarchicalCommunities,\n",
    "            size\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self.run_query(query, {\n",
    "            \"graph_name\": graph_name,\n",
    "            \"limit\": limit,\n",
    "            \"community_node_limit\": community_node_limit\n",
    "        })\n",
    "        \n",
    "        logger.info(f\"Detected communities with {len(result)} nodes\")\n",
    "        \n",
    "        community_data = []\n",
    "        for record in result:\n",
    "            community_data.append({\n",
    "                'nodeId': record['nodeId'],\n",
    "                'text': record['text'] if record['text'] else '',\n",
    "                'community': record['community'],\n",
    "                'hierarchicalCommunities': record['hierarchicalCommunities'],\n",
    "                'size': record['size']\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(community_data)\n",
    "    \n",
    "    def generate_category_name(self, client, texts, community_id):\n",
    "        # Use a sample of texts to avoid token limits\n",
    "        sample_size = min(5, len(texts))\n",
    "        sample_texts = random.sample(texts, sample_size) if len(texts) > sample_size else texts\n",
    "                        \n",
    "        # Prepare prompt for the LLM\n",
    "        prompt = f\"\"\"Below are {sample_size} legal text snippets that belong to the same category. \n",
    "        Please generate a concise, specific legal category name (2-5 words) that accurately captures \n",
    "        what these texts have in common:\n",
    "\n",
    "        {'-' * 40}\n",
    "        {'\\n'.join([f\"{i+1}. {text[:300]}...\" if len(text) > 300 else f\"{i+1}. {text}\" for i, text in enumerate(sample_texts)])}\n",
    "        {'-' * 40}\n",
    "        \n",
    "        Respond with ONLY the category name, nothing else.\"\"\"\n",
    "        \n",
    "        # Call the LLM API\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a legal taxonomy expert. Generate concise, specific category names for groups of legal texts.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=20\n",
    "        )\n",
    "        \n",
    "        # Extract and clean the category name\n",
    "        category_name = response.choices[0].message.content.strip()\n",
    "        # Remove quotes if present\n",
    "        category_name = category_name.strip('\"\\'')\n",
    "            \n",
    "        logger.info(f\"Generated category name for community {community_id}: '{category_name}'\")\n",
    "        return category_name\n",
    "    \n",
    "    \n",
    "    def create_taxonomy_categories(self, community_df, min_community_size=3):\n",
    "        \"\"\"Create taxonomy category nodes from communities using LLM for labeling.\"\"\"\n",
    "        \n",
    "        communities = community_df.groupby('community')\n",
    "        categories = []\n",
    "        try:\n",
    "            openai_api_key = \"hide\"\n",
    "            client = openai.OpenAI(api_key=openai_api_key)\n",
    "            \n",
    "            # Process each community\n",
    "            for community_id, group in communities:\n",
    "                if len(group) < min_community_size:\n",
    "                    continue\n",
    "                    \n",
    "                community_texts = group['text'].tolist()\n",
    "                community_texts = [t for t in community_texts if t and isinstance(t, str)]\n",
    "                \n",
    "                if not community_texts:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Generate category name using LLM\n",
    "                    category_name = self.generate_category_name(client, community_texts, community_id)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error generating category name with LLM: {e}\")\n",
    "                    # Fallback to keyword extraction\n",
    "                    #keywords = self.extract_keywords(community_texts)\n",
    "                    category_name = \"not found\"\n",
    "                \n",
    "                categories.append({\n",
    "                    'id': f'category-{community_id}',\n",
    "                    'name': category_name,\n",
    "                    'size': len(group)\n",
    "                })\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error setting up LLM: {e}\")\n",
    "        \n",
    "        # Create category nodes in neo4j\n",
    "        if categories:\n",
    "            params = {\"categories\": categories}\n",
    "            query = \"\"\"\n",
    "            UNWIND $categories AS category\n",
    "            MERGE (t:TaxonomyCategory {id: category.id})\n",
    "            SET t.name = category.name, \n",
    "                t.size = category.size,\n",
    "                t.keywords = split(category.name, ' ')\n",
    "            RETURN count(t) as created\n",
    "            \"\"\"\n",
    "            result = self.run_query(query, params)\n",
    "            logger.info(f\"Created {result[0]['created']} taxonomy category nodes with LLM-generated names\")\n",
    "        \n",
    "        return categories\n",
    "    \n",
    "    def link_nodes_to_categories(self, community_df):\n",
    "        \"\"\"Link nodes to their taxonomy categories.\"\"\"\n",
    "        \n",
    "        # Prepare relationships data\n",
    "        relationships = []\n",
    "        for _, row in community_df.iterrows():\n",
    "            relationships.append({\n",
    "                'nodeId': row['nodeId'],\n",
    "                'categoryId': f\"category-{row['community']}\"\n",
    "            })\n",
    "        \n",
    "        # Create relationships : link nodes to categories\n",
    "        if relationships:\n",
    "            params = {\"relationships\": relationships}\n",
    "            query = \"\"\"\n",
    "            UNWIND $relationships AS rel\n",
    "            MATCH (d {id: rel.nodeId})\n",
    "            MATCH (t:TaxonomyCategory {id: rel.categoryId})\n",
    "            MERGE (d)-[:BELONGS_TO]->(t)\n",
    "            RETURN count(*) as created\n",
    "            \"\"\"\n",
    "            result = self.run_query(query, params)\n",
    "            logger.info(f\"Created {result[0]['created']} BELONGS_TO relationships\")\n",
    "    \n",
    "    def create_constraints_and_indexes(self):\n",
    "        \"\"\"Create constraints and indexes.\"\"\"\n",
    "        # Create unique constraint on TaxonomyCategory.id if it doesn't exist\n",
    "        try:\n",
    "            query = \"\"\"\n",
    "            CREATE CONSTRAINT taxonomy_category_id IF NOT EXISTS\n",
    "            FOR (t:TaxonomyCategory) REQUIRE t.id IS UNIQUE\n",
    "            \"\"\"\n",
    "            self.run_query(query)\n",
    "            logger.info(\"Created constraint on TaxonomyCategory.id\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not create constraint: {e}\")\n",
    "        \n",
    "        # Create index on TaxonomyCategory.name for text search\n",
    "        try:\n",
    "            query = \"\"\"\n",
    "            CREATE INDEX taxonomy_category_name IF NOT EXISTS\n",
    "            FOR (t:TaxonomyCategory) ON (t.name)\n",
    "            \"\"\"\n",
    "            self.run_query(query)\n",
    "            logger.info(\"Created index on TaxonomyCategory.name\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not create index: {e}\")\n",
    "    \n",
    "    def extract_taxonomy(self):\n",
    "        logger.info(\"Starting taxonomy extraction process\")\n",
    "        \n",
    "        self.optimize_similarity_relationships()\n",
    "        \n",
    "        graph_name = self.create_graph_projection()\n",
    "        if not graph_name:\n",
    "            return False\n",
    "        \n",
    "        community_df = self.run_community_detection(graph_name)\n",
    "        \n",
    "        categories = self.create_taxonomy_categories(community_df)\n",
    "        \n",
    "        self.link_nodes_to_categories(community_df)\n",
    "        \n",
    "        self.create_constraints_and_indexes()\n",
    "        \n",
    "        logger.info(\"Taxonomy extraction completed\")\n",
    "        return True\n",
    "            \n",
    "    def print_hierarchical_taxonomy(self):\n",
    "        \"\"\"Print all taxonomy categories in a hierarchical bullet point format,\n",
    "        ensuring all categories are displayed.\"\"\"\n",
    "        \n",
    "        # Get all taxonomy categories\n",
    "        query = \"\"\"\n",
    "            MATCH (t:TaxonomyCategory)\n",
    "            RETURN t.id as id, t.name as name, t.size as size\n",
    "            ORDER BY t.size DESC\n",
    "        \"\"\"\n",
    "        all_categories = self.run_query(query)\n",
    "        \n",
    "        # Track categories already displayed\n",
    "        displayed_categories = set()\n",
    "        \n",
    "        print(\"Generated Hierarchical Taxonomy :\")\n",
    "        \n",
    "        # First, find all root categories (those that have no parents)\n",
    "        query = \"\"\"\n",
    "            MATCH (t:TaxonomyCategory)\n",
    "            WHERE NOT (t)-[:SUBCATEGORY_OF]->()\n",
    "            RETURN t.id as id, t.name as name, t.size as size\n",
    "            ORDER BY t.size DESC\n",
    "        \"\"\"\n",
    "        root_categories = self.run_query(query)\n",
    "        \n",
    "        # Process each root category and its hierarchy\n",
    "        for r in root_categories:\n",
    "            category_id = r['id']\n",
    "            category_name = r['name']\n",
    "            size = r['size']\n",
    "            \n",
    "            # Print the root category\n",
    "            print(f\"* {category_name} ({size} nodes)\")\n",
    "            displayed_categories.add(category_id)\n",
    "            \n",
    "            # Recursively print subcategories\n",
    "            self._print_subcategories(category_id, 1, displayed_categories)\n",
    "\n",
    "    def _print_subcategories(self, category_id, depth, visited_nodes):\n",
    "        \"\"\"Recursively print subcategories of a given category.\"\"\"\n",
    "        # Get all categories related to this one\n",
    "        query = \"\"\"\n",
    "            MATCH (child:TaxonomyCategory)-[:SUBCATEGORY_OF]->(parent:TaxonomyCategory {id: $category_id})\n",
    "            RETURN child.id as id, child.name as name, child.size as size\n",
    "            ORDER BY child.size DESC\n",
    "        \"\"\"\n",
    "        result = self.run_query(query, {\"category_id\": category_id})\n",
    "        \n",
    "        if not result:\n",
    "            # No subcategories found for this category\n",
    "            return\n",
    "        \n",
    "        # Print each related category with proper indentation\n",
    "        for r in result:\n",
    "            sub_id = r['id']\n",
    "            sub_name = r['name']\n",
    "            size = r['size']\n",
    "            \n",
    "            # Create indentation\n",
    "            indent = \"  \" * depth\n",
    "            \n",
    "            # Check for cycles\n",
    "            if sub_id in visited_nodes:\n",
    "                #print(f\"{indent}* {sub_name} ({size} nodes) [CYCLE DETECTED]\")\n",
    "                continue\n",
    "            \n",
    "            # Print subcategory\n",
    "            print(f\"{indent}* {sub_name} ({size} nodes)\")\n",
    "            visited_nodes.add(sub_id)\n",
    "            \n",
    "            # Recursively print its subcategories\n",
    "            self._print_subcategories(sub_id, depth + 1, visited_nodes)\n",
    "        \n",
    "    def generate_hierarchical_taxonomy_with_llm(self, client=None):\n",
    "        \"\"\"Generate a proper hierarchical taxonomy using LLM to organize categories.\"\"\"\n",
    "        \n",
    "        if client is None:\n",
    "            try:\n",
    "                openai_api_key = \"hide\"\n",
    "                client = openai.OpenAI(api_key=openai_api_key)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error setting up OpenAI client: {e}\")\n",
    "                return False\n",
    "        \n",
    "        # Get all existing categories\n",
    "        query = \"\"\"\n",
    "        MATCH (t:TaxonomyCategory)\n",
    "        RETURN t.id as id, t.name as name, t.size as size\n",
    "        ORDER BY t.size DESC\n",
    "        \"\"\"\n",
    "        result = self.run_query(query)\n",
    "        \n",
    "        if not result:\n",
    "            logger.error(\"No taxonomy categories found\")\n",
    "            return False\n",
    "        \n",
    "        # Prepare category names for LLM\n",
    "        category_names = [r['name'] for r in result]\n",
    "        category_info = {r['name']: {'id': r['id'], 'size': r['size']} for r in result}\n",
    "        \n",
    "        # Generate hierarchy structure using LLM\n",
    "        hierarchy_structure = self._generate_taxonomy_structure(client, category_names)\n",
    "        \n",
    "        if not hierarchy_structure:\n",
    "            logger.error(\"Failed to generate hierarchy structure\")\n",
    "            return False\n",
    "        \n",
    "        # Create the hierarchy in Neo4j\n",
    "        success = self._create_llm_generated_hierarchy(hierarchy_structure, category_info)\n",
    "        \n",
    "        return success\n",
    "    \n",
    "    def _generate_taxonomy_structure(self, client, category_names: List[str]) -> Dict:\n",
    "        \"\"\"Use LLM to generate a hierarchical taxonomy structure.\"\"\"\n",
    "        \n",
    "        categories_text = '\\n'.join([f\"* {name}\" for name in category_names])\n",
    "        \n",
    "        prompt = f\"\"\"Below are legal category names that should be grouped under a tree-like structure (taxonomy):\n",
    "\n",
    "        {categories_text}\n",
    "\n",
    "        Please generate a hierarchical taxonomy with several levels. Create logical parent categories that group related subcategories together. \n",
    "\n",
    "        Requirements:\n",
    "        1. Create a root category that encompasses all legal categories\n",
    "        2. Create 4-6 main parent categories under the root\n",
    "        3. Group the existing categories under appropriate parent categories\n",
    "        4. You can create additional intermediate levels if needed\n",
    "        5. Each existing category should appear exactly once in the hierarchy\n",
    "        6. Use clear, professional legal terminology for parent category names\n",
    "\n",
    "        Return the result as a JSON structure with this format:\n",
    "        {{\n",
    "        \"root\": {{\n",
    "            \"name\": \"Root Category Name\",\n",
    "            \"children\": {{\n",
    "            \"Parent Category 1\": {{\n",
    "                \"name\": \"Parent Category 1 Full Name\",\n",
    "                \"children\": {{\n",
    "                \"Subcategory 1\": {{\"name\": \"Existing Category Name\"}},\n",
    "                \"Subcategory 2\": {{\"name\": \"Another Existing Category Name\"}}\n",
    "                }}\n",
    "            }},\n",
    "            \"Parent Category 2\": {{\n",
    "                \"name\": \"Parent Category 2 Full Name\", \n",
    "                \"children\": {{\n",
    "                \"Subcategory 3\": {{\"name\": \"Yet Another Existing Category Name\"}}\n",
    "                }}\n",
    "            }}\n",
    "            }}\n",
    "        }}\n",
    "        }}\n",
    "\n",
    "        Make sure every existing category from the input list appears exactly once in the JSON structure.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a legal taxonomy expert. Create well-structured hierarchical taxonomies for legal document categorization. Always return valid JSON.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.3,\n",
    "                max_tokens=2000\n",
    "            )\n",
    "            \n",
    "            response_text = response.choices[0].message.content.strip()\n",
    "            \n",
    "            json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_text = json_match.group()\n",
    "                hierarchy_structure = json.loads(json_text)\n",
    "                logger.info(\"Successfully generated hierarchy structure with LLM\")\n",
    "                return hierarchy_structure\n",
    "            else:\n",
    "                logger.error(\"No JSON structure found in LLM response\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating taxonomy structure with LLM: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _create_llm_generated_hierarchy(self, hierarchy_structure: Dict, category_info: Dict) -> bool:\n",
    "        \"\"\"Create the LLM-generated hierarchy in Neo4j.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            new_categories = []\n",
    "            category_mappings = []\n",
    "            \n",
    "            def process_hierarchy_level(node_data, parent_id=None, level=0):\n",
    "                \"\"\"Recursively process hierarchy levels.\"\"\"\n",
    "                \n",
    "                if isinstance(node_data, dict) and 'name' in node_data:\n",
    "                    category_name = node_data['name']\n",
    "                    \n",
    "                    # Generate ID for this category\n",
    "                    if category_name in category_info:\n",
    "                        # Existing leaf category\n",
    "                        category_id = category_info[category_name]['id']\n",
    "                        size = category_info[category_name]['size']\n",
    "                    else:\n",
    "                        # New parent category\n",
    "                        category_id = f\"category-parent-{len(new_categories)}\"\n",
    "                        size = 0 \n",
    "                        new_categories.append({\n",
    "                            'id': category_id,\n",
    "                            'name': category_name,\n",
    "                            'size': size,\n",
    "                            'is_parent': True\n",
    "                        })\n",
    "                    \n",
    "                    # Record parent-child relationship\n",
    "                    if parent_id:\n",
    "                        category_mappings.append({\n",
    "                            'child_id': category_id,\n",
    "                            'parent_id': parent_id\n",
    "                        })\n",
    "                    \n",
    "                    # Process children if they exist\n",
    "                    if 'children' in node_data and node_data['children']:\n",
    "                        for child_key, child_data in node_data['children'].items():\n",
    "                            process_hierarchy_level(child_data, category_id, level + 1)\n",
    "                    \n",
    "                    return category_id\n",
    "                \n",
    "                return None\n",
    "            \n",
    "            # Process the hierarchy starting from root\n",
    "            if 'root' in hierarchy_structure:\n",
    "                root_data = hierarchy_structure['root']\n",
    "                process_hierarchy_level(root_data)\n",
    "            else:\n",
    "                logger.error(\"No root found in hierarchy structure\")\n",
    "                return False\n",
    "            \n",
    "            # Create new parent categories in Neo4j\n",
    "            if new_categories:\n",
    "                params = {\"categories\": new_categories}\n",
    "                query = \"\"\"\n",
    "                UNWIND $categories AS category\n",
    "                MERGE (t:TaxonomyCategory {id: category.id})\n",
    "                SET t.name = category.name, \n",
    "                    t.size = category.size,\n",
    "                    t.keywords = split(category.name, ' '),\n",
    "                    t.is_parent = category.is_parent\n",
    "                RETURN count(t) as created\n",
    "                \"\"\"\n",
    "                result = self.run_query(query, params)\n",
    "                logger.info(f\"Created {result[0]['created']} new parent taxonomy categories\")\n",
    "            \n",
    "            # Create hierarchy relationships in Neo4j\n",
    "            if category_mappings:\n",
    "                params = {\"mappings\": category_mappings}\n",
    "                query = \"\"\"\n",
    "                UNWIND $mappings AS mapping\n",
    "                MATCH (child:TaxonomyCategory {id: mapping.child_id})\n",
    "                MATCH (parent:TaxonomyCategory {id: mapping.parent_id})\n",
    "                MERGE (child)-[:SUBCATEGORY_OF]->(parent)\n",
    "                RETURN count(*) as created\n",
    "                \"\"\"\n",
    "                result = self.run_query(query, params)\n",
    "                logger.info(f\"Created {result[0]['created']} SUBCATEGORY_OF relationships\")\n",
    "            \n",
    "            # Update parent category sizes based on their children\n",
    "            self._update_parent_category_sizes()\n",
    "            \n",
    "            logger.info(\"Successfully created LLM-generated hierarchical taxonomy\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating LLM-generated hierarchy: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _update_parent_category_sizes(self):\n",
    "        \"\"\"Update parent category sizes based on the sum of their children's sizes.\"\"\"\n",
    "        query = \"\"\"\n",
    "        MATCH (parent:TaxonomyCategory)<-[:SUBCATEGORY_OF*]-(leaf:TaxonomyCategory)\n",
    "        WHERE NOT ()-[:SUBCATEGORY_OF]->(leaf)\n",
    "        WITH parent, sum(leaf.size) as total_size\n",
    "        SET parent.size = total_size\n",
    "        RETURN parent.name as name, total_size\n",
    "        \"\"\"\n",
    "        result = self.run_query(query)\n",
    "        logger.info(f\"Updated sizes for {len(result)} parent categories\")\n",
    "    \n",
    "    \n",
    "    def build_hierarchical_taxonomy(self):\n",
    "        \"\"\"Complete process to rebuild taxonomy with LLM-generated hierarchy.\"\"\"\n",
    "        logger.info(\"Starting LLM-based taxonomy hierarchy rebuild\")\n",
    "        \n",
    "        success = self.generate_hierarchical_taxonomy_with_llm()\n",
    "        \n",
    "        if success:\n",
    "            logger.info(\"Successfully rebuilt taxonomy with LLM-generated hierarchy\")\n",
    "            self.print_hierarchical_taxonomy()\n",
    "        else:\n",
    "            logger.error(\"Failed to rebuild taxonomy with LLM hierarchy\")\n",
    "        \n",
    "        return success\n",
    "        \n",
    "\n",
    "def main():\n",
    "    NEO4J_URI = \"neo4j://localhost:7687\"\n",
    "    NEO4J_USER = \"neo4j\"\n",
    "    NEO4J_PASSWORD = \"\"\n",
    "    \n",
    "    try:\n",
    "        extractor = LegalTaxonomyExtractor(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)\n",
    "        \n",
    "        extractor.extract_taxonomy()\n",
    "\n",
    "        extractor.print_hierarchical_taxonomy()\n",
    "        \n",
    "        extractor.build_hierarchical_taxonomy()\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {e}\")\n",
    "    finally:\n",
    "        if 'extractor' in locals():\n",
    "            extractor.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e33764",
   "metadata": {},
   "source": [
    "# GraphRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fWWNu8FHgtU",
   "metadata": {
    "id": "4fWWNu8FHgtU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 22:31:35,971 - INFO - Creating vector index named 'legal_corpus'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard DROP INDEX failed: {code: Neo.DatabaseError.Schema.IndexDropFailed} {message: Unable to drop index called `legal_corpus`. There is no such index.}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 22:31:36,596 - WARNING - Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated function: `id`.} {position: line: 4, column: 20, offset: 90} for query: '\\n            MATCH (c:Chunk)\\n            WHERE c.embedding IS NOT NULL\\n            RETURN id(c) AS node_id, c.embedding AS embedding\\n            '\n",
      "C:\\Users\\ArnaudCrucifix\\AppData\\Local\\Temp\\ipykernel_21080\\1158364686.py:83: DeprecationWarning: 'upsert_vector' is deprecated and will be removed in a future version, please use 'upsert_vectors' instead.\n",
      "  upsert_vector(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 338 embeddings to vector store 'legal_corpus'...\n",
      "Successfully added 338 embeddings to the vector store.\n"
     ]
    }
   ],
   "source": [
    "from neo4j_graphrag.indexes import create_vector_index, upsert_vector\n",
    "import neo4j\n",
    "\n",
    "VECTOR_STORE_NAME = \"legal_corpus\"\n",
    "DIMENSION = 1536\n",
    "\n",
    "def drop_vector_index(driver, index_name):\n",
    "    \"\"\"\n",
    "    Drop a vector index from Neo4j database using various methods\n",
    "    depending on Neo4j version.\n",
    "    \n",
    "    Args:\n",
    "        driver: Neo4j driver instance\n",
    "        index_name: Name of the vector index to drop\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            try:\n",
    "                session.run(f\"DROP INDEX {index_name}\")\n",
    "                print(f\"Vector index '{index_name}' dropped successfully using standard DROP INDEX.\")\n",
    "                return True\n",
    "            except Exception as e1:\n",
    "                print(f\"Standard DROP INDEX failed: {e1}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error dropping vector index '{index_name}': {e}\")\n",
    "        return False\n",
    "\n",
    "def get_chunk_embeddings(driver) -> list:\n",
    "    \"\"\"\n",
    "    Retrieve all Chunk nodes along with their embedding properties.\n",
    "    Returns a list of tuples (node_id, embedding).\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\n",
    "            \"\"\"\n",
    "            MATCH (c:Chunk)\n",
    "            WHERE c.embedding IS NOT NULL\n",
    "            RETURN id(c) AS node_id, c.embedding AS embedding\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        embeddings_data = [\n",
    "            (record[\"node_id\"], record[\"embedding\"]) for record in result\n",
    "        ]\n",
    "\n",
    "        return embeddings_data\n",
    "\n",
    "def main():\n",
    "    URI = \"neo4j://localhost:7687\"\n",
    "    USERNAME = \"neo4j\"\n",
    "    PASSWORD = \"hide\"\n",
    "    AUTH = (USERNAME, PASSWORD)\n",
    "\n",
    "    driver = neo4j.GraphDatabase.driver(URI, auth=AUTH)\n",
    "\n",
    "    drop_vector_index(driver, VECTOR_STORE_NAME)\n",
    "\n",
    "    create_vector_index(\n",
    "        driver,\n",
    "        name=VECTOR_STORE_NAME,\n",
    "        label=\"Legal_corpus\",\n",
    "        embedding_property=\"vectorProperty\",\n",
    "        dimensions=DIMENSION,\n",
    "        similarity_fn=\"cosine\",\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Fetch all chunk nodes along with their embeddings from the knowledge graph.\n",
    "        embeddings_data = get_chunk_embeddings(driver)\n",
    "\n",
    "        if not embeddings_data:\n",
    "            print(\"No Chunk nodes with embeddings found.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Adding {len(embeddings_data)} embeddings to vector store '{VECTOR_STORE_NAME}'...\")\n",
    "\n",
    "        successful_ops_counter = 0\n",
    "\n",
    "        # Process and upsert each node's embedding into the vector store.\n",
    "        # This transfers embeddings data from standard node properties into a dedicated Neo4j vector index\n",
    "        for node_id, embedding in embeddings_data:\n",
    "            upsert_vector(\n",
    "                driver,\n",
    "                node_id=node_id,\n",
    "                embedding_property=\"embedding\",\n",
    "                vector=embedding,\n",
    "            )\n",
    "            successful_ops_counter += 1\n",
    "\n",
    "        print(f\"Successfully added {successful_ops_counter} embeddings to the vector store.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        \n",
    "    finally:\n",
    "        driver.close()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QC-tEU51Hsgh",
   "metadata": {
    "id": "QC-tEU51Hsgh"
   },
   "outputs": [],
   "source": [
    "from neo4j_graphrag.embeddings import OpenAIEmbeddings\n",
    "from neo4j_graphrag.generation import GraphRAG\n",
    "from neo4j_graphrag.llm import OpenAILLM\n",
    "from neo4j_graphrag.retrievers import VectorRetriever\n",
    "\n",
    "URI = \"neo4j://localhost:7687\"\n",
    "USERNAME = \"neo4j\"\n",
    "PASSWORD = \"hide\"\n",
    "AUTH = (USERNAME, PASSWORD)\n",
    "INDEX_NAME = \"legal_corpus\"\n",
    "DATABASE = \"neo4j\"\n",
    "\n",
    "driver = neo4j.GraphDatabase.driver(URI, auth=AUTH)\n",
    "\n",
    "# Create an Embedder object to convert the user's question into a vector. The same embedding model used for knowledge graph creation is used here.\n",
    "embedder = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=\"hide\")\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = VectorRetriever(driver, INDEX_NAME, embedder)\n",
    "\n",
    "# Set up the LLM for generating answers based on the retrieved knowledge graph leal texts.\n",
    "llm = OpenAILLM(\n",
    "    api_key=\"hide\",\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    model_params={\n",
    "        \"temperature\": 0,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Initialize the RAG pipeline\n",
    "rag = GraphRAG(retriever=retriever, llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce5fe92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 22:33:50,486 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 22:33:58,094 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Investment Risk Disclosures are essential documents or statements provided by investment firms to inform investors about the potential risks associated with various investment products. These disclosures aim to ensure that investors are aware of the risks they may face, allowing them to make informed decisions. Key aspects of Investment Risk Disclosures include:\n",
      "\n",
      "1. **Types of Risks**: They typically outline various types of risks, such as market risk, credit risk, liquidity risk, operational risk, and specific risks related to particular investment products (e.g., stocks, bonds, mutual funds).\n",
      "\n",
      "2. **Regulatory Requirements**: Many jurisdictions require investment firms to provide risk disclosures as part of their compliance with financial regulations. This is to protect investors and promote transparency in the financial markets.\n",
      "\n",
      "3. **Clear Language**: Disclosures should be written in clear and understandable language, avoiding jargon that may confuse investors. The goal is to ensure that all investors, regardless of their experience level, can comprehend the risks involved.\n",
      "\n",
      "4. **Specificity**: Effective risk disclosures should be specific to the investment product being offered. For example, a disclosure for a high-yield bond fund would differ from that of a conservative equity fund, highlighting the unique risks associated with each.\n",
      "\n",
      "5. **Updates and Revisions**: Investment Risk Disclosures should be regularly updated to reflect changes in market conditions, regulatory requirements, or the investment strategy of the product.\n",
      "\n",
      "6. **Investor Acknowledgment**: Some firms may require investors to acknowledge that they have read and understood the risk disclosures before proceeding with an investment.\n",
      "\n",
      "7. **Educational Resources**: In addition to disclosures, firms may provide educational resources to help investors better understand the risks and how to manage them.\n",
      "\n",
      "Overall, Investment Risk Disclosures play a crucial role in promoting informed investment decisions and fostering trust between investors and financial institutions.\n"
     ]
    }
   ],
   "source": [
    "# Query the graph\n",
    "#query_text = \"Tell me something about risks\"\n",
    "#query_text = \"What is the legal form of FinaStream Fund?\"\n",
    "#query_text = \"What assets can the company invest in?\"\n",
    "#query_text = \"Can you give the different types of shares?\"\n",
    "query_text = \"Give all you know about the Investment Risk Disclosures?\"\n",
    "#query_text = \"How are the Tolaxis Single Asset Funds legally organised?\"\n",
    "#query_text = \"Provide the paragraph that explains under what conditions an investorâ€™s shares can be forcibly redeemed by the company.\"\n",
    "#query_text = \"Provide and show the paragraph that defines the different share classes and their specific rights. Dont forget to give the provision number.\"\n",
    "#query_text = \"What can you say about the fith article of the Shareholders from the FinaStream Fund in the constitution? Is it related to anything in the offering document?\"\n",
    "#query_text = \"Can you say about the FinaStream Fund? Give many details related in documents as possible and give the answer only from those\"\n",
    "response = rag.search(query_text=query_text, retriever_config={\"top_k\": 5})\n",
    "print(response.answer)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
