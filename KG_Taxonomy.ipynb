{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "iICFRAFzXeGj",
   "metadata": {
    "id": "iICFRAFzXeGj"
   },
   "source": [
    "# ðŸ“Š Fund Review Automation\n",
    "\n",
    "This notebook performs the full pipeline:\n",
    "- Connect to Neo4j: Interface with a graph database to store and query legal content as a knowledge graph.\n",
    "\n",
    "- Construct a Legal Knowledge Graph: Transform unstructured legal documents (e.g., constitutions, prospectuses, shareholder agreements) into a structured graph using custom document splitters, entity/relation extraction, and semantic linking.\n",
    "\n",
    "- Run Louvain Community Detection: Identify hierarchical legal taxonomies by discovering clusters of semantically or referentially related legal content.\n",
    "\n",
    "- Label and Analyze Communities: Use generative AI to assign interpretable labels to communities of legal texts, forming a multidimensional taxonomy for fund review reports.\n",
    "\n",
    "- GraphRAG : Use a first version of GraphRAG to query the knowledge graph and retrieve legal texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2262444d",
   "metadata": {},
   "source": [
    "## Constitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5000124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.experimental.components.text_splitters.base import TextSplitter\n",
    "from neo4j_graphrag.experimental.components.types import TextChunk, TextChunks\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "import asyncio\n",
    "\n",
    "@dataclass\n",
    "class HierarchyLevel:\n",
    "    \"\"\"Defines a hierarchy level with its regular expression extraction and metadata key.\"\"\"\n",
    "    name: str\n",
    "    pattern: str\n",
    "    metadata_key: str\n",
    "    title_key: Optional[str] = None\n",
    "    strip_header_pattern: Optional[str] = None\n",
    "\n",
    "class ConstitutionSplitter(TextSplitter):\n",
    "    \"\"\"Splitter for constitution documents.\"\"\"\n",
    "\n",
    "    def __init__(self, llm, graph_id: str, overlap_percentage: float = 0.2) -> None:\n",
    "        self.llm = llm\n",
    "        self.graph_id = graph_id\n",
    "        #self.overlap_percentage = max(0.0, min(1.0, overlap_percentage))\n",
    "        \n",
    "        # Define the hierarchy levels in order : section -> article\n",
    "        self.hierarchy_levels = [\n",
    "            HierarchyLevel(\n",
    "                name=\"section\",\n",
    "                pattern=r'^Section\\s+(\\d+)\\s+[â€“\\-]\\s+([^\\n]+)',\n",
    "                metadata_key=\"section_num\",\n",
    "                title_key=\"section_title\",\n",
    "                strip_header_pattern=r'^Section\\s+{num}\\s+[â€“\\-]\\s+{title}\\n?'\n",
    "            ),\n",
    "            HierarchyLevel(\n",
    "                name=\"article\", \n",
    "                pattern=r'^(?:[ \\t]*)(\\d+)\\s+[â€“\\-]\\s+([^\\n]+)',\n",
    "                metadata_key=\"article_num\",\n",
    "                title_key=\"article_title\",\n",
    "                strip_header_pattern=r'^{num}\\s+[â€“\\-]\\s+{title}\\n?'\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        model_name = self.llm.model_name\n",
    "        try:\n",
    "            self.tokenizer = tiktoken.encoding_for_model(model_name)\n",
    "        except KeyError:\n",
    "            self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    def split_text(self, text: str) -> TextChunks:\n",
    "        \"\"\"Synchronous wrapper for the main async processing method.\"\"\"\n",
    "        try:\n",
    "            loop = asyncio.get_running_loop()\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "        return asyncio.run(self.run(text.strip()))\n",
    "\n",
    "    async def run(self, text: str) -> TextChunks:\n",
    "        \"\"\"Main processing method using recursive hierarchy processing.\"\"\"\n",
    "        \n",
    "        cleaned_text = re.sub(r'\\\\n?', '', text).strip()\n",
    "        \n",
    "        #header_info = self._extract_header(cleaned_text)\n",
    "        header_info = {\n",
    "            \"document_id\": self.graph_id,\n",
    "            \"source\": \"legal_document\",\n",
    "            \"document_type\": \"legal_constitution\",\n",
    "            \"document_title\": f\"Legal Document {self.graph_id}\"\n",
    "        }\n",
    "        base_metadata = {**header_info, \"graph_id\": self.graph_id}\n",
    "        \n",
    "        chunks = []\n",
    "        chunk_index = [0]  # Use list to allow modification in nested calls\n",
    "        \n",
    "        \n",
    "        # Process hierarchical content recursively\n",
    "        self._process_hierarchy_level(\n",
    "            content=cleaned_text,\n",
    "            level_index=0,\n",
    "            metadata=base_metadata,\n",
    "            chunks=chunks,\n",
    "            chunk_index=chunk_index\n",
    "        )\n",
    "        \n",
    "        return TextChunks(chunks=chunks)\n",
    "\n",
    "\n",
    "    def _process_hierarchy_level(self, content: str, level_index: int, metadata: dict, \n",
    "                                chunks: List, chunk_index: List[int]):\n",
    "        \"\"\"\n",
    "        Process recursively a hierarchy level. \n",
    "        Args:\n",
    "            content: Text content to process at this level\n",
    "            level_index: Current hierarchy level (0=section, 1=article)\n",
    "            metadata: Accumulated metadata from parent levels\n",
    "            chunks: List of chunks\n",
    "            chunk_index: Current chunk index\n",
    "        \"\"\"\n",
    "        \n",
    "        # Base case: no more hierarchy levels to process\n",
    "        if level_index >= len(self.hierarchy_levels):\n",
    "            if content.strip():\n",
    "                chunks.append(self._create_chunk(chunk_index[0], content, metadata))\n",
    "                chunk_index[0] += 1\n",
    "            return\n",
    "        \n",
    "        current_level = self.hierarchy_levels[level_index]\n",
    "        items = self._extract_items_at_level(content, current_level)\n",
    "        \n",
    "        # If no items found at this level, try the next level or create chunk\n",
    "        if not items:\n",
    "            self._process_hierarchy_level(content, level_index + 1, metadata, chunks, chunk_index)\n",
    "            return\n",
    "        \n",
    "        # Process each item at this level\n",
    "        for item_id, item_title, item_content in items:\n",
    "            # Build metadata for this level, {**metadata} to do a shallow copy\n",
    "            current_metadata = {**metadata}\n",
    "            current_metadata[current_level.metadata_key] = item_id\n",
    "            if current_level.title_key and item_title:\n",
    "                current_metadata[current_level.title_key] = item_title\n",
    "            current_metadata[\"chunk_type\"] = current_level.name\n",
    "            \n",
    "            # Strip header if needed\n",
    "            processed_content = self._strip_level_header(\n",
    "                item_content, current_level, item_id, item_title\n",
    "            )\n",
    "            \n",
    "            # Recursively process the next level\n",
    "            self._process_hierarchy_level(\n",
    "                content=processed_content,\n",
    "                level_index=level_index + 1,\n",
    "                metadata=current_metadata,\n",
    "                chunks=chunks,\n",
    "                chunk_index=chunk_index\n",
    "            )\n",
    "\n",
    "    def _extract_items_at_level(self, content: str, level: HierarchyLevel) -> List[Tuple[str, Optional[str], str]]:\n",
    "        \"\"\"\n",
    "        Extract items at a specific hierarchy level. An item can be a section or an article.\n",
    "        Args:\n",
    "            content: Text content to search for items\n",
    "            level: HierarchyLevel object defining the pattern and metadata keys\n",
    "        \n",
    "        Returns:\n",
    "            List of (item_id, item_title, item_content) tuples\n",
    "        \"\"\"\n",
    "        \n",
    "        matches = list(re.finditer(level.pattern, content, re.MULTILINE))\n",
    "        if not matches:\n",
    "            return []\n",
    "        \n",
    "        items = []\n",
    "        for i, match in enumerate(matches):\n",
    "            # Extract ID and title based on pattern groups\n",
    "            if level.title_key:\n",
    "                item_id, item_title = match.group(1), match.group(2).strip() # group(1) is ID, group(2) is title\n",
    "            else:  # Only has ID (paragraphs)\n",
    "                if match.lastindex >= 1:\n",
    "                    item_id = match.group(1)\n",
    "                else:\n",
    "                    match.group('id')\n",
    "                item_title = None\n",
    "            \n",
    "            # Extract content block\n",
    "            start_pos = match.start()\n",
    "            if i < len(matches) - 1:\n",
    "                # Get content until the next match\n",
    "                end_pos = matches[i + 1].start()\n",
    "            else:\n",
    "                end_pos = len(content)\n",
    "                \n",
    "            item_content = content[start_pos:end_pos].strip()\n",
    "            \n",
    "            if item_content:\n",
    "                items.append((item_id, item_title, item_content))\n",
    "        \n",
    "        return items\n",
    "\n",
    "    def _strip_level_header(self, content: str, level: HierarchyLevel, \n",
    "                           item_id: str, item_title: Optional[str]) -> str:\n",
    "        \"\"\"Strip the header from content at a specific level.\"\"\"\n",
    "        if not level.strip_header_pattern:\n",
    "            # For enumerated items, strip the leading pattern\n",
    "            pattern = level.pattern\n",
    "            return re.sub(pattern, '', content, count=1, flags=re.MULTILINE).strip()\n",
    "        \n",
    "        # For sections/articles with titles\n",
    "        if item_title:\n",
    "            pattern = level.strip_header_pattern.format(\n",
    "                num=re.escape(item_id),\n",
    "                title=re.escape(item_title)\n",
    "            )\n",
    "        else:\n",
    "            pattern = level.strip_header_pattern.format(num=re.escape(item_id))\n",
    "        \n",
    "        return re.sub(pattern, '', content, count=1, flags=re.MULTILINE).strip()\n",
    "\n",
    "    def _extract_header(self, text: str) -> Dict[str, str]:\n",
    "        \"\"\"Extract document header information with required fields for lexical graph.\"\"\"\n",
    "        header = {\n",
    "            \"document_id\": self.graph_id,\n",
    "            \"source\": \"legal_document\",\n",
    "            \"document_type\": \"legal_constitution\"\n",
    "        }\n",
    "        \n",
    "        return header\n",
    "\n",
    "    def _create_chunk(self, index: int, text: str, metadata: dict) -> TextChunk:\n",
    "        \"\"\"Creates a TextChunk with cleaned text and hierarchical metadata.\"\"\"\n",
    "        \n",
    "        # Clean the text\n",
    "        clean_text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Determine level\n",
    "        chunk_type = metadata.get(\"chunk_type\", \"content\")\n",
    "        level_mapping = {\n",
    "            \"document_root\": \"document\",\n",
    "            \"section\": \"section\",\n",
    "            \"article\": \"article\", \n",
    "        }\n",
    "        level = level_mapping.get(chunk_type, chunk_type)\n",
    "        \n",
    "        # Build hierarchical path\n",
    "        path_parts = self._build_hierarchical_path(metadata, level)\n",
    "        \n",
    "        final_metadata = {\n",
    "            **metadata,\n",
    "            \"level\": level,\n",
    "            \"hierarchical_path\": \" â†’ \".join(path_parts) if path_parts else \"Document Content\",\n",
    "        }\n",
    "        \n",
    "        return TextChunk(index=index, text=clean_text, metadata=final_metadata)\n",
    "\n",
    "    def _build_hierarchical_path(self, metadata: dict, level: str) -> List[str]:\n",
    "        \"\"\"Build the hierarchical path for a chunk.\"\"\"\n",
    "        path_parts = []\n",
    "        \n",
    "        if level == \"document\" and \"document_title\" in metadata:\n",
    "            path_parts.append(metadata[\"document_title\"])\n",
    "        else:\n",
    "            if \"section_title\" in metadata:\n",
    "                path_parts.append(f\"Section {metadata.get('section_num','?')} â€“ {metadata.get('section_title','Untitled Section')}\")\n",
    "            if \"article_title\" in metadata:\n",
    "                path_parts.append(f\"Article {metadata.get('article_num','?')} â€“ {metadata.get('article_title','Untitled Article')}\")\n",
    "        \n",
    "        return path_parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554f75a2",
   "metadata": {},
   "source": [
    "# Prospectus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XzObjqtykvUe",
   "metadata": {
    "executionInfo": {
     "elapsed": 82,
     "status": "ok",
     "timestamp": 1747302409602,
     "user": {
      "displayName": "Arnaud Crucifix",
      "userId": "14208215153378642511"
     },
     "user_tz": -120
    },
    "id": "XzObjqtykvUe"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import asyncio\n",
    "from typing import List, Dict\n",
    "import tiktoken\n",
    "from neo4j_graphrag.experimental.components.text_splitters.base import TextSplitter\n",
    "from neo4j_graphrag.experimental.components.types import TextChunk, TextChunks\n",
    "\n",
    "class ProspectusSplitter(TextSplitter):\n",
    "    \"\"\"Splitter for prospectus documents.\"\"\"\n",
    "\n",
    "    def __init__(self, llm, graph_id: str, overlap_percentage: float = 0.2) -> None:\n",
    "        self.llm = llm\n",
    "        self.graph_id = graph_id\n",
    "        self.overlap_percentage = overlap_percentage\n",
    "        if not 0.0 <= overlap_percentage <= 1.0:\n",
    "            raise ValueError(\"overlap_percentage must be between 0.0 and 1.0\")\n",
    "\n",
    "        # Get the encoding for the model being used\n",
    "        model_name = self.llm.model_name\n",
    "        try:\n",
    "            self.tokenizer = tiktoken.encoding_for_model(model_name)\n",
    "        except KeyError:\n",
    "            # Fallback to cl100k_base encoding (used by many OpenAI models)\n",
    "            self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "        # Document structure regular expressions\n",
    "        self.patterns = {\n",
    "            # Major sections with all-caps titles (e.g., \"UNDERSTANDING INVESTMENT POWERS\")\n",
    "            'major_section': r'(?:\\n|\\A)([A-Z][A-Z\\s]+[A-Z])(?:\\n|$)(.*?)(?=(?:\\n|\\A)[A-Z][A-Z\\s]+[A-Z](?:\\n|$)|\\Z)',\n",
    "\n",
    "            # Sub-sections with Title Case (e.g., \"What to Know About Investment Policy\")\n",
    "            'sub_section': r'(?:\\n|\\A)([A-Z][a-zA-Z\\s]+[a-zA-Z])[:\\.\\n](.*?)(?=(?:\\n|\\A)[A-Z][a-zA-Z\\s]+[a-zA-Z][:\\.\\n]|\\Z)',\n",
    "\n",
    "            # Tables that often begin with headers and have structured data\n",
    "            'table': r'(?:\\n|\\A)([A-Za-z\\s]+)\\n((?:[^\\n]+\\n)+?(?=\\n|\\Z))',\n",
    "\n",
    "            # Definitions (e.g., \"Class: Limited partner interests...\")\n",
    "            'definition': r'(?:\\n|\\A)([A-Za-z\\s]+)\\.\\s+(.*?)(?=(?:\\n|\\A)[A-Za-z\\s]+\\.\\s+|\\Z)',\n",
    "\n",
    "            # Bullet points\n",
    "            'bullet_points': r'(?:\\n|\\A)(?:â€¢|\\*|\\-)\\s+(.*?)(?=(?:\\n|\\A)(?:â€¢|\\*|\\-)|\\Z)',\n",
    "\n",
    "            # For numbered items\n",
    "            'numbered_item': r'(?:\\n|\\A)(\\d+\\.)\\s+(.*?)(?=(?:\\n|\\A)\\d+\\.|\\Z)',\n",
    "        }\n",
    "\n",
    "    async def run(self, text: str) -> TextChunks:\n",
    "        # Identify the document type and extract metadata\n",
    "        document_metadata = self._extract_document_metadata(text)\n",
    "\n",
    "        # Extract major sections from the document\n",
    "        major_sections = self._extract_major_sections(text)\n",
    "\n",
    "        # If no clear major sections found, fall back to sub-sections\n",
    "        if not major_sections:\n",
    "            major_sections = self._extract_sub_sections(text)\n",
    "\n",
    "        # If still no clear structure, fallback to paragraphs\n",
    "        if not major_sections:\n",
    "            major_sections = self._extract_paragraphs(text)\n",
    "\n",
    "        \n",
    "        all_chunks = []\n",
    "        # Process each section into chunks\n",
    "        for section_idx, (section_title, section_content) in enumerate(major_sections):\n",
    "            # Add metadata about the section\n",
    "            metadata = {\n",
    "                **document_metadata,\n",
    "                \"graph_id\": self.graph_id,\n",
    "                \"section_title\": section_title,\n",
    "                \"section_index\": section_idx,\n",
    "            }\n",
    "\n",
    "            # Tokenize the section\n",
    "            tokens = self.tokenizer.encode(section_content)\n",
    "            max_tokens = min(\n",
    "                4096,  # Default context window\n",
    "                self.llm.model_params.get(\"max_tokens\", 3000) * 3,\n",
    "            )\n",
    "\n",
    "            # If the section fits in one chunk\n",
    "            if len(tokens) <= max_tokens:\n",
    "                all_chunks.append(\n",
    "                    TextChunk(\n",
    "                        index=len(all_chunks),\n",
    "                        text=section_content,\n",
    "                        metadata=metadata\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                # Need to split the section, try by sub-sections first\n",
    "                sub_sections = self._extract_sub_sections(section_content)\n",
    "\n",
    "                if sub_sections:\n",
    "                    # Process each sub-section\n",
    "                    current_chunk_text = \"\"\n",
    "                    current_chunk_tokens = 0\n",
    "                    subsection_metadata = metadata.copy()\n",
    "                    subsection_metadata[\"subsections\"] = []\n",
    "\n",
    "                    for subsec_title, subsec_content in sub_sections:\n",
    "                        subsec_tokens = self.tokenizer.encode(subsec_content)\n",
    "\n",
    "                        # If adding this subsection would exceed max tokens, create a new chunk\n",
    "                        if current_chunk_tokens + len(subsec_tokens) > max_tokens and current_chunk_text:\n",
    "                            all_chunks.append(\n",
    "                                TextChunk(\n",
    "                                    index=len(all_chunks),\n",
    "                                    text=current_chunk_text,\n",
    "                                    metadata=subsection_metadata\n",
    "                                )\n",
    "                            )\n",
    "                            current_chunk_text = f\"[Continued from {section_title}]\\n\\n\" # Indicate continuation from prev chunk\n",
    "                            current_chunk_tokens = len(self.tokenizer.encode(current_chunk_text))\n",
    "                            subsection_metadata = metadata.copy()\n",
    "                            subsection_metadata[\"continued\"] = True\n",
    "                            subsection_metadata[\"subsections\"] = []\n",
    "\n",
    "                        # Add the subsection to the current chunk\n",
    "                        current_chunk_text += subsec_content + \"\\n\\n\"\n",
    "                        current_chunk_tokens += len(subsec_tokens) + len(self.tokenizer.encode(\"\\n\\n\"))\n",
    "\n",
    "                        # Track subsection information\n",
    "                        subsection_metadata[\"subsections\"].append(subsec_title)\n",
    "\n",
    "                    # Add the final chunk if there is a content\n",
    "                    if current_chunk_text:\n",
    "                        all_chunks.append(\n",
    "                            TextChunk(\n",
    "                                index=len(all_chunks),\n",
    "                                text=current_chunk_text,\n",
    "                                metadata=subsection_metadata\n",
    "                            )\n",
    "                        )\n",
    "                else:\n",
    "                    # No subsection found : try bullet points or fall back to overlapping chunks\n",
    "                    bullet_points = self._extract_bullet_points(section_content)\n",
    "\n",
    "                    if bullet_points and len(bullet_points) > 1:\n",
    "                        # Process bullet points as chunks\n",
    "                        current_chunk_text = \"\"\n",
    "                        current_chunk_tokens = 0\n",
    "                        bullet_metadata = metadata.copy()\n",
    "                        bullet_metadata[\"bullet_points\"] = True\n",
    "\n",
    "                        for bullet_title, bullet_content in bullet_points:\n",
    "                            bullet_tokens = self.tokenizer.encode(bullet_content)\n",
    "\n",
    "                            # If adding this bullet exceed max tokens, create a new chunk\n",
    "                            if current_chunk_tokens + len(bullet_tokens) > max_tokens and current_chunk_text:\n",
    "                                all_chunks.append(\n",
    "                                    TextChunk(\n",
    "                                        index=len(all_chunks),\n",
    "                                        text=current_chunk_text,\n",
    "                                        metadata=bullet_metadata\n",
    "                                    )\n",
    "                                )\n",
    "                                current_chunk_text = f\"[Continued from {section_title} - Bullet Points]\\n\\n\"\n",
    "                                current_chunk_tokens = len(self.tokenizer.encode(current_chunk_text))\n",
    "                                bullet_metadata = metadata.copy()\n",
    "                                bullet_metadata[\"continued\"] = True\n",
    "                                bullet_metadata[\"bullet_points\"] = True\n",
    "\n",
    "                            # Add the bullet to the current chunk\n",
    "                            current_chunk_text += bullet_content + \"\\n\\n\"\n",
    "                            current_chunk_tokens += len(bullet_tokens) + len(self.tokenizer.encode(\"\\n\\n\"))\n",
    "\n",
    "                        # Add the final chunk if there's content\n",
    "                        if current_chunk_text:\n",
    "                            all_chunks.append(\n",
    "                                TextChunk(\n",
    "                                    index=len(all_chunks),\n",
    "                                    text=current_chunk_text,\n",
    "                                    metadata=bullet_metadata\n",
    "                                )\n",
    "                            )\n",
    "                    else:\n",
    "                        # Fall back to overlapping chunks\n",
    "                        chunk_size = int(max_tokens * (1 - self.overlap_percentage))\n",
    "                        overlap_size = int(max_tokens * self.overlap_percentage)\n",
    "\n",
    "                        for i in range(0, len(tokens), chunk_size):\n",
    "                            start_index = max(0, i)\n",
    "                            end_index = min(i + chunk_size + overlap_size, len(tokens))\n",
    "                            current_chunk_tokens = tokens[start_index:end_index]\n",
    "                            current_chunk_text = self.tokenizer.decode(current_chunk_tokens)\n",
    "\n",
    "                            chunk_metadata = metadata.copy()\n",
    "                            if i > 0:\n",
    "                                chunk_metadata[\"continued\"] = True\n",
    "                                current_chunk_text = f\"[Continued from {section_title}]\\n\\n{current_chunk_text}\"\n",
    "\n",
    "                            all_chunks.append(\n",
    "                                TextChunk(\n",
    "                                    index=len(all_chunks),\n",
    "                                    text=current_chunk_text,\n",
    "                                    metadata=chunk_metadata\n",
    "                                )\n",
    "                            )\n",
    "\n",
    "        # Identify and mark special chunks like tables, definitions.\n",
    "        all_chunks = self._mark_special_chunks(all_chunks)\n",
    "\n",
    "        return TextChunks(chunks=all_chunks)\n",
    "\n",
    "    def _extract_document_metadata(self, text: str) -> Dict:\n",
    "        \"\"\"Extract key metadata from the document.\"\"\"\n",
    "        \n",
    "        metadata = {\n",
    "            \"document_type\": \"Offering Document\",\n",
    "            \"document_title\": \"\"\n",
    "        }\n",
    "\n",
    "        # Try to extract the document title\n",
    "        title_pattern = r'(?:\\A|\\n)([^\\n]{5,150}(?:Fund|Offering|Prospectus|Document)[^\\n]{0,50})'\n",
    "        title_match = re.search(title_pattern, text[:1000], re.IGNORECASE)\n",
    "        if title_match:\n",
    "            metadata[\"document_title\"] = title_match.group(1).strip()\n",
    "\n",
    "        # Try to extract the fund name\n",
    "        fund_pattern = r'(?:\\A|\\n)([A-Za-z0-9\\s]+(?:Fund|Partnership|Trust|Company))'\n",
    "        fund_match = re.search(fund_pattern, text[:1000], re.IGNORECASE)\n",
    "        if fund_match:\n",
    "            metadata[\"fund_name\"] = fund_match.group(1).strip()\n",
    "\n",
    "        return metadata\n",
    "\n",
    "    def _extract_major_sections(self, text: str) -> List[tuple]:\n",
    "        \"\"\"Extract major sections from the document.\"\"\"\n",
    "        \n",
    "        matches = re.finditer(self.patterns['major_section'], text, re.DOTALL)\n",
    "        sections = []\n",
    "\n",
    "        prev_end = 0\n",
    "        for match in matches:\n",
    "            # If there is a content before the first section, add it\n",
    "            if prev_end == 0 and match.start() > 0:\n",
    "                intro_content = text[:match.start()].strip()\n",
    "                if intro_content:\n",
    "                    sections.append((\"Introduction\", intro_content))\n",
    "\n",
    "            section_title = match.group(1).strip()\n",
    "            section_content = match.group(0).strip()\n",
    "\n",
    "            sections.append((section_title, section_content))\n",
    "            prev_end = match.end()\n",
    "\n",
    "        # If there's a content after the last section, add it\n",
    "        if prev_end < len(text) and text[prev_end:].strip():\n",
    "            sections.append((\"Additional information\", text[prev_end:].strip()))\n",
    "\n",
    "        return sections\n",
    "\n",
    "    def _extract_sub_sections(self, text: str) -> List[tuple]:\n",
    "        \"\"\"Extract sub-sections from a section of text.\"\"\"\n",
    "        \n",
    "        matches = re.finditer(self.patterns['sub_section'], text, re.DOTALL)\n",
    "        sub_sections = []\n",
    "\n",
    "        for match in matches:\n",
    "            subsec_title = match.group(1).strip()\n",
    "            subsec_content = match.group(0).strip()\n",
    "\n",
    "            sub_sections.append((subsec_title, subsec_content))\n",
    "\n",
    "        return sub_sections\n",
    "    \n",
    "    \n",
    "    def _extract_paragraphs(self, text: str) -> List[tuple]:\n",
    "        \"\"\"Extract paragraphs when no clear section structure is found.\"\"\"\n",
    "        \n",
    "        paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "        result = []\n",
    "\n",
    "        for i, para in enumerate(paragraphs):\n",
    "            if para.strip():\n",
    "                # Try to a title from the first line\n",
    "                lines = para.strip().split('\\n')\n",
    "                if lines and len(lines[0]) < 100:\n",
    "                    title = lines[0].strip()\n",
    "                else:\n",
    "                    words = para.strip().split()[:5]\n",
    "                    \n",
    "                    if words:\n",
    "                        title = \" \".join(words) + \"...\"\n",
    "                    else:\n",
    "                        f\"Paragraph {i+1}\"\n",
    "\n",
    "                result.append((title, para.strip()))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _extract_bullet_points(self, text: str) -> List[tuple]:\n",
    "        \"\"\"Extract bullet points from a text section.\"\"\"\n",
    "        \n",
    "        matches = re.finditer(self.patterns['bullet_points'], text, re.DOTALL)\n",
    "        bullets = []\n",
    "\n",
    "        for i, match in enumerate(matches):\n",
    "            bullet_content = match.group(0).strip()\n",
    "            bullet_text = match.group(1).strip()\n",
    "\n",
    "            words = bullet_text.split()\n",
    "            title = \" \".join(words[:min(5, len(words))]) + \"...\"\n",
    "\n",
    "            bullets.append((f\"Bullet {i+1}: {title}\", bullet_content))\n",
    "\n",
    "        return bullets\n",
    "\n",
    "\n",
    "    def _mark_special_chunks(self, chunks: List[TextChunk]) -> List[TextChunk]:\n",
    "        \"\"\"Identify chunks that contain tables, definitions, or numbered items.\"\"\"\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            # Check for tables\n",
    "            table_matches = re.findall(r'(?:\\n|\\A)([A-Za-z\\s]+)\\n((?:[^\\n]+\\n)+?(?=\\n|\\Z))', chunk.text, re.DOTALL)\n",
    "            if table_matches:\n",
    "                if chunk.metadata is None:\n",
    "                    chunk.metadata = {}\n",
    "                chunk.metadata[\"contains_tables\"] = True\n",
    "                chunk.metadata[\"table_titles\"] = [title.strip() for title, _ in table_matches]\n",
    "\n",
    "            # Check for definitions\n",
    "            definitions = []\n",
    "            for match in re.finditer(self.patterns['definition'], chunk.text, re.DOTALL):\n",
    "                term = match.group(1)\n",
    "                definition = match.group(2)\n",
    "                if term and definition:\n",
    "                    definitions.append((term.strip(), definition.strip()))\n",
    "\n",
    "            if definitions:\n",
    "                if chunk.metadata is None:\n",
    "                    chunk.metadata = {}\n",
    "                chunk.metadata[\"contains_definitions\"] = True\n",
    "                chunk.metadata[\"definitions\"] = [term for term, _ in definitions]\n",
    "\n",
    "            # Check for numbered items\n",
    "            numbered_items = re.findall(self.patterns['numbered_item'], chunk.text, re.DOTALL)\n",
    "            if numbered_items:\n",
    "                if chunk.metadata is None:\n",
    "                    chunk.metadata = {}\n",
    "                chunk.metadata[\"contains_numbered_items\"] = True\n",
    "                chunk.metadata[\"item_count\"] = len(numbered_items)\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def split_text(self, text: str) -> TextChunks:\n",
    "        \"\"\"Synchronously calls the async run method.\"\"\"\n",
    "        return asyncio.run(self.run(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749ca0b9",
   "metadata": {},
   "source": [
    "# Shareholders Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8afba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import re\n",
    "import asyncio\n",
    "import tiktoken\n",
    "\n",
    "@dataclass\n",
    "class HierarchyLevel:\n",
    "    \"\"\"Defines a hierarchy level with its regular expression extraction and metadata key.\"\"\"\n",
    "    name: str\n",
    "    pattern: str\n",
    "    metadata_key: str\n",
    "    title_key: Optional[str] = None\n",
    "    strip_header_pattern: Optional[str] = None\n",
    "    level_number: int = 0\n",
    "\n",
    "\n",
    "class ShareholdersAgreementSplitter(TextSplitter):\n",
    "    \"\"\"Improved splitter for shareholders documents using recursive hierarchy processing.\"\"\"\n",
    "\n",
    "    def __init__(self, llm, graph_id: str, overlap_percentage: float = 0.2) -> None:\n",
    "        self.llm = llm\n",
    "        self.graph_id = graph_id\n",
    "        self.overlap_percentage = overlap_percentage\n",
    "        if not 0.0 <= overlap_percentage <= 1.0:\n",
    "            raise ValueError(\"overlap_percentage must be between 0.0 and 1.0\")\n",
    "\n",
    "        # Define the hierarchy levels in order of nesting\n",
    "        self.hierarchy_levels = [\n",
    "            # Level 1: Main numbered clauses (e.g., \"1. Definitions and Interpretation\")\n",
    "            HierarchyLevel(\n",
    "                name=\"main_clause\",\n",
    "                pattern=r'^(\\d+)\\.\\s+([A-Z][A-Za-z0-9\\s\\-\\(\\),;&\\']+?)(?:\\n|$)',\n",
    "                metadata_key=\"main_clause_num\",\n",
    "                title_key=\"main_clause_title\",\n",
    "                strip_header_pattern=r'^{num}\\.\\s+{title}\\n?',\n",
    "                level_number=1\n",
    "            ),\n",
    "            # Level 2: First-level sub-clauses (e.g., \"1.1 Definitions\")\n",
    "            HierarchyLevel(\n",
    "                name=\"sub_clause\",\n",
    "                pattern=r'^(\\d+\\.\\d+)\\s+([A-Z][A-Za-z0-9\\s\\-\\(\\),;&\\']+?)(?:\\n|$)',\n",
    "                metadata_key=\"sub_clause_num\",\n",
    "                title_key=\"sub_clause_title\",\n",
    "                strip_header_pattern=r'^{num}\\s+{title}\\n?',\n",
    "                level_number=2\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Regular expressions for recitals, schedules and annexures\n",
    "        self.special_patterns = {\n",
    "            'recital': r'^([A-Z])\\.\\s+(.*)',\n",
    "            'schedule': r'^Schedule\\s+(\\d+)\\s*([A-Za-z0-9\\s\\-\\(\\)]*)',\n",
    "            'annexure': r'^(?:Annexure|ANNEXURE)\\s+([A-Z])[:\\s]*([^\\n]*)',\n",
    "            'schedule_part': r'^PART\\s+([A-Z]+)'\n",
    "        }\n",
    "\n",
    "        # TOC patterns for filtering\n",
    "        self.toc_patterns = [\n",
    "            r'^(\\d+(?:\\.\\d+)*)\\.\\s+([^\\n]+?)\\.{2,}\\s*\\d+\\s*$',\n",
    "            r'^(\\d+(?:\\.\\d+)*)\\.\\s+([A-Za-z][A-Za-z0-9\\s\\-\\(\\),;&\\']+?)\\s+\\d+\\s*$',\n",
    "            r'^(\\d+(?:\\.\\d+)*)\\.\\s+([^\\t\\n]+?)\\t+\\d+\\s*$',\n",
    "            r'^(\\d+(?:\\.\\d+)*)\\.\\s+([A-Za-z][^\\n]+?)\\s{2,}\\d+\\s*$',\n",
    "            r'^([ivxlcdm]+)\\.\\s+([A-Za-z][^\\n]+?)\\s+\\d+\\s*$',\n",
    "            r'^([A-Z])\\.\\s+([A-Za-z][^\\n]+?)\\s+\\d+\\s*$'\n",
    "        ]\n",
    "\n",
    "        model_name = self.llm.model_name\n",
    "        try:\n",
    "            self.tokenizer = tiktoken.encoding_for_model(model_name)\n",
    "        except KeyError:\n",
    "            self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    def split_text(self, text: str) -> TextChunks:\n",
    "        \"\"\"Synchronous wrapper for the main async processing method.\"\"\"\n",
    "        return asyncio.run(self.run(text))\n",
    "\n",
    "    async def run(self, text: str) -> TextChunks:\n",
    "        \"\"\"Main processing method using recursive hierarchy processing.\"\"\"\n",
    "        \n",
    "        # Normalize line endings and remove TOC\n",
    "        cleaned_text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "        filtered_text = self._extract_non_toc_text(cleaned_text)\n",
    "        \n",
    "        # Extract header information\n",
    "        header_info = self._extract_document_header(filtered_text)\n",
    "        base_metadata = {**header_info, \"graph_id\": self.graph_id}\n",
    "        \n",
    "        chunks = []\n",
    "        chunk_index = [0]\n",
    "        \n",
    "        # First, process recitals separately\n",
    "        recitals_text, main_content = self._separate_recitals(filtered_text)\n",
    "        if recitals_text:\n",
    "            self._process_recitals(recitals_text, base_metadata, chunks, chunk_index)\n",
    "        \n",
    "        # Then process main hierarchical content\n",
    "        if main_content:\n",
    "            self._process_hierarchy_level(\n",
    "                content=main_content,\n",
    "                level_index=0,\n",
    "                metadata=base_metadata.copy(),\n",
    "                chunks=chunks,\n",
    "                chunk_index=chunk_index\n",
    "            )\n",
    "        \n",
    "        # Finally, process schedules and annexures\n",
    "        schedules_text = self._extract_schedules_content(filtered_text)\n",
    "        if schedules_text:\n",
    "            self._process_schedules(schedules_text, base_metadata, chunks, chunk_index)\n",
    "        \n",
    "        return TextChunks(chunks=chunks)\n",
    "\n",
    "    def _process_hierarchy_level(self, content: str, level_index: int, metadata: dict, \n",
    "                                chunks: List, chunk_index: List[int]):\n",
    "        \"\"\"\n",
    "        Recursively process a hierarchy level.\n",
    "        Args:\n",
    "            content: Text content to process at this level\n",
    "            level_index: Current hierarchy level index\n",
    "            metadata: Accumulated metadata from parent levels\n",
    "            chunks: List of chunks to append to\n",
    "            chunk_index: Current chunk index counter\n",
    "        \"\"\"\n",
    "        \n",
    "        # Base case: no more hierarchy levels to process\n",
    "        if level_index >= len(self.hierarchy_levels):\n",
    "            if content.strip():\n",
    "                chunk = self._create_chunk(chunk_index[0], content, metadata)\n",
    "                if self._should_split_chunk(chunk):\n",
    "                    sub_chunks = self._split_large_content(content, metadata, chunk_index[0])\n",
    "                    chunks.extend(sub_chunks)\n",
    "                    chunk_index[0] += len(sub_chunks)\n",
    "                else:\n",
    "                    chunks.append(chunk)\n",
    "                    chunk_index[0] += 1\n",
    "            return\n",
    "        \n",
    "        current_level = self.hierarchy_levels[level_index]\n",
    "        items = self._extract_items_at_level(content, current_level)\n",
    "        \n",
    "        # If no items found at this level, try the next level or create chunk\n",
    "        if not items:\n",
    "            self._process_hierarchy_level(content, level_index + 1, metadata, chunks, chunk_index)\n",
    "            return\n",
    "        \n",
    "        # Process each item at this level\n",
    "        for item_id, item_title, item_content in items:\n",
    "            # Build metadata for this level\n",
    "            current_metadata = {**metadata}\n",
    "            current_metadata[current_level.metadata_key] = item_id\n",
    "            if current_level.title_key and item_title:\n",
    "                current_metadata[current_level.title_key] = item_title\n",
    "            current_metadata[\"chunk_type\"] = current_level.name\n",
    "            current_metadata[\"level\"] = current_level.level_number\n",
    "            \n",
    "            # Add hierarchical path\n",
    "            current_metadata[\"hierarchical_path\"] = self._build_hierarchical_path(current_metadata)\n",
    "            \n",
    "            # Strip header if needed\n",
    "            processed_content = self._strip_level_header(\n",
    "                item_content, current_level, item_id, item_title\n",
    "            )\n",
    "            \n",
    "            # Recursively process the next level\n",
    "            self._process_hierarchy_level(\n",
    "                content=processed_content,\n",
    "                level_index=level_index + 1,\n",
    "                metadata=current_metadata,\n",
    "                chunks=chunks,\n",
    "                chunk_index=chunk_index\n",
    "            )\n",
    "\n",
    "    def _extract_items_at_level(self, content: str, level: HierarchyLevel) -> List[Tuple[str, Optional[str], str]]:\n",
    "        \"\"\"Extract items at a specific hierarchy level.\"\"\"\n",
    "        \n",
    "        matches = list(re.finditer(level.pattern, content, re.MULTILINE))\n",
    "        if not matches:\n",
    "            return []\n",
    "        \n",
    "        items = []\n",
    "        for i, match in enumerate(matches):\n",
    "            # Skip if this looks like a TOC entry\n",
    "            if self._is_toc_entry(match.group(0)):\n",
    "                continue\n",
    "                \n",
    "            # Extract ID and title based on pattern groups\n",
    "            if level.title_key:\n",
    "                item_id = match.group(1)\n",
    "                item_title = match.group(2).strip() if match.group(2) else None\n",
    "            else:\n",
    "                item_id = match.group(1)\n",
    "                item_title = None\n",
    "            \n",
    "            # Extract content block\n",
    "            start_pos = match.start()\n",
    "            if i < len(matches) - 1:\n",
    "                # Find next non-TOC match\n",
    "                next_match = None\n",
    "                for j in range(i + 1, len(matches)):\n",
    "                    if not self._is_toc_entry(matches[j].group(0)):\n",
    "                        next_match = matches[j]\n",
    "                        break\n",
    "                end_pos = next_match.start() if next_match else len(content)\n",
    "            else:\n",
    "                end_pos = len(content)\n",
    "                \n",
    "            item_content = content[start_pos:end_pos].strip()\n",
    "            \n",
    "            if item_content:\n",
    "                items.append((item_id, item_title, item_content))\n",
    "        \n",
    "        return items\n",
    "\n",
    "    def _strip_level_header(self, content: str, level: HierarchyLevel, \n",
    "                           item_id: str, item_title: Optional[str]) -> str:\n",
    "        \"\"\"Strip the header from content at a specific level.\"\"\"\n",
    "        if not level.strip_header_pattern:\n",
    "            return content\n",
    "        \n",
    "        if item_title and '{title}' in level.strip_header_pattern:\n",
    "            pattern = level.strip_header_pattern.format(\n",
    "                num=re.escape(item_id),\n",
    "                title=re.escape(item_title),\n",
    "                id=re.escape(item_id)\n",
    "            )\n",
    "        else:\n",
    "            pattern = level.strip_header_pattern.format(\n",
    "                num=re.escape(item_id),\n",
    "                id=re.escape(item_id)\n",
    "            )\n",
    "        \n",
    "        return re.sub(pattern, '', content, count=1, flags=re.MULTILINE).strip()\n",
    "\n",
    "    def _process_recitals(self, recitals_text: str, base_metadata: dict, \n",
    "                         chunks: List, chunk_index: List[int]):\n",
    "        \"\"\"Process recitals section.\"\"\"\n",
    "        recital_matches = list(re.finditer(self.special_patterns['recital'], recitals_text, re.MULTILINE))\n",
    "        \n",
    "        for match in recital_matches:\n",
    "            recital_id = match.group(1)\n",
    "            recital_content = match.group(2).strip()\n",
    "            \n",
    "            metadata = {\n",
    "                **base_metadata,\n",
    "                \"chunk_type\": \"recital\",\n",
    "                \"level\": 1,\n",
    "                \"recital_id\": recital_id,\n",
    "                \"hierarchical_path\": f\"Recital {recital_id}\"\n",
    "            }\n",
    "            \n",
    "            chunk = self._create_chunk(chunk_index[0], f\"Recital {recital_id}: {recital_content}\", metadata)\n",
    "            chunks.append(chunk)\n",
    "            chunk_index[0] += 1\n",
    "\n",
    "    def _process_schedules(self, schedules_text: str, base_metadata: dict, \n",
    "                          chunks: List, chunk_index: List[int]):\n",
    "        \"\"\"Process schedules and annexures.\"\"\"\n",
    "        schedule_sections = self._extract_schedule_sections(schedules_text)\n",
    "        \n",
    "        for section in schedule_sections:\n",
    "            metadata = {\n",
    "                **base_metadata,\n",
    "                \"chunk_type\": section[\"type\"],\n",
    "                \"level\": 1,\n",
    "                f\"{section['type']}_id\": section[\"id\"],\n",
    "                f\"{section['type']}_title\": section.get(\"title\", \"\"),\n",
    "                \"hierarchical_path\": f\"{section['type'].title()} {section['id']}\"\n",
    "            }\n",
    "            \n",
    "            if self._should_split_chunk_content(section[\"content\"]):\n",
    "                sub_chunks = self._split_large_content(section[\"content\"], metadata, chunk_index[0])\n",
    "                chunks.extend(sub_chunks)\n",
    "                chunk_index[0] += len(sub_chunks)\n",
    "            else:\n",
    "                chunk = self._create_chunk(chunk_index[0], section[\"content\"], metadata)\n",
    "                chunks.append(chunk)\n",
    "                chunk_index[0] += 1\n",
    "\n",
    "    def _separate_recitals(self, text: str) -> Tuple[str, str]:\n",
    "        \"\"\"Separate recitals from main content.\"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        recitals_lines = []\n",
    "        main_content_start = 0\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            if re.match(self.special_patterns['recital'], line.strip()):\n",
    "                recitals_lines.append(line)\n",
    "            elif re.match(r'^\\d+\\.\\s+', line.strip()) and recitals_lines:\n",
    "                main_content_start = i\n",
    "                break\n",
    "        \n",
    "        if recitals_lines:\n",
    "            recitals_text = '\\n'.join(recitals_lines)\n",
    "        else: \n",
    "            \"\"\n",
    "        main_content = '\\n'.join(lines[main_content_start:]) if main_content_start > 0 else text\n",
    "        \n",
    "        return recitals_text, main_content\n",
    "\n",
    "    def _extract_schedules_content(self, text: str) -> str:\n",
    "        \"\"\"Extract schedules and annexures content.\"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        schedule_start = None\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            if re.match(self.special_patterns['schedule'], line.strip()) or \\\n",
    "               re.match(self.special_patterns['annexure'], line.strip()):\n",
    "                schedule_start = i\n",
    "                break\n",
    "        \n",
    "        return '\\n'.join(lines[schedule_start:]) if schedule_start is not None else \"\"\n",
    "\n",
    "    def _extract_schedule_sections(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Extract individual schedule/annexure sections.\"\"\"\n",
    "        sections = []\n",
    "        lines = text.split('\\n')\n",
    "        current_section = None\n",
    "        \n",
    "        for line in lines:\n",
    "            line_stripped = line.strip()\n",
    "            \n",
    "            # Check for schedule\n",
    "            schedule_match = re.match(self.special_patterns['schedule'], line_stripped)\n",
    "            if schedule_match:\n",
    "                if current_section:\n",
    "                    sections.append(current_section)\n",
    "                current_section = {\n",
    "                    'type': 'schedule',\n",
    "                    'id': schedule_match.group(1),\n",
    "                    'title': schedule_match.group(2).strip() if schedule_match.group(2) else \"\",\n",
    "                    'content': line\n",
    "                }\n",
    "                continue\n",
    "            \n",
    "            # Check for annexure\n",
    "            annexure_match = re.match(self.special_patterns['annexure'], line_stripped)\n",
    "            if annexure_match:\n",
    "                if current_section:\n",
    "                    sections.append(current_section)\n",
    "                current_section = {\n",
    "                    'type': 'annexure',\n",
    "                    'id': annexure_match.group(1),\n",
    "                    'title': annexure_match.group(2).strip() if annexure_match.group(2) else \"\",\n",
    "                    'content': line\n",
    "                }\n",
    "                continue\n",
    "            \n",
    "            # Add content to current section\n",
    "            if current_section and line_stripped:\n",
    "                current_section['content'] += '\\n' + line\n",
    "        \n",
    "        if current_section:\n",
    "            sections.append(current_section)\n",
    "        \n",
    "        return sections\n",
    "\n",
    "    def _extract_non_toc_text(self, text: str) -> str:\n",
    "        \"\"\"Extract text excluding the Table of Contents section.\"\"\"\n",
    "        toc_start, toc_end = self._identify_toc_boundaries(text)\n",
    "        \n",
    "        if toc_start is None:\n",
    "            return text\n",
    "        \n",
    "        lines = text.split('\\n')\n",
    "        \n",
    "        if toc_end is not None:\n",
    "            before_toc = lines[:toc_start] if toc_start > 0 else []\n",
    "            after_toc = lines[toc_end + 1:] if toc_end + 1 < len(lines) else []\n",
    "            filtered_lines = before_toc + after_toc\n",
    "        else:\n",
    "            before_toc = lines[:toc_start] if toc_start > 0 else []\n",
    "            first_clause = self._find_first_real_clause_after_toc(lines, toc_start)\n",
    "            if first_clause is not None:\n",
    "                after_toc = lines[first_clause:]\n",
    "                filtered_lines = before_toc + after_toc\n",
    "            else:\n",
    "                filtered_lines = before_toc\n",
    "        \n",
    "        return '\\n'.join(filtered_lines)\n",
    "\n",
    "    def _identify_toc_boundaries(self, text: str) -> Tuple[Optional[int], Optional[int]]:\n",
    "        \"\"\"Identify the start and end boundaries of the Table of Contents.\"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        toc_start = None\n",
    "        toc_end = None\n",
    "        \n",
    "        # Look for TOC start\n",
    "        for i, line in enumerate(lines):\n",
    "            if re.search(r'(?i)table\\s+of\\s+contents?|^contents?$', line.strip()):\n",
    "                toc_start = i\n",
    "                break\n",
    "        \n",
    "        if toc_start is not None:\n",
    "            consecutive_toc_lines = 0\n",
    "            for i in range(toc_start + 1, min(toc_start + 100, len(lines))):\n",
    "                line_stripped = lines[i].strip()\n",
    "                if not line_stripped:\n",
    "                    continue\n",
    "                \n",
    "                if self._is_toc_entry(line_stripped):\n",
    "                    consecutive_toc_lines += 1\n",
    "                    toc_end = i\n",
    "                else:\n",
    "                    if consecutive_toc_lines > 0 and self._is_clause(line_stripped):\n",
    "                        break\n",
    "                    consecutive_toc_lines = 0\n",
    "        \n",
    "        return toc_start, toc_end\n",
    "\n",
    "    def _is_toc_entry(self, line: str) -> bool:\n",
    "        \"\"\"Check if a line appears to be a Table of Contents entry.\"\"\"\n",
    "        return any(re.match(pattern, line) for pattern in self.toc_patterns)\n",
    "\n",
    "    def _is_clause(self, line: str) -> bool:\n",
    "        \"\"\"Check if a line is a clause.\"\"\"\n",
    "        clause_patterns = [\n",
    "            r'^(\\d+)\\.\\s+([A-Z][A-Za-z0-9\\s\\-\\(\\),;&\\']+?)$',\n",
    "            r'^(\\d+\\.\\d+)\\s+([A-Z][A-Za-z0-9\\s\\-\\(\\),;&\\']+?)$',\n",
    "            r'^([A-Z])\\.\\s+([A-Za-z][^\\d]*?)$'\n",
    "        ]\n",
    "        \n",
    "        for pattern in clause_patterns:\n",
    "            match = re.match(pattern, line)\n",
    "            if match and not re.search(r'\\s+\\d+$', match.group(2) if len(match.groups()) > 1 else match.group(1)):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _find_first_real_clause_after_toc(self, lines: List[str], toc_start: int) -> Optional[int]:\n",
    "        \"\"\"Find the first real clause after TOC start.\"\"\"\n",
    "        for i in range(toc_start + 1, len(lines)):\n",
    "            if self._is_clause(lines[i].strip()):\n",
    "                return i\n",
    "        return None\n",
    "\n",
    "    def _extract_document_header(self, text: str) -> Dict[str, str]:\n",
    "        \"\"\"Extract document header information.\"\"\"\n",
    "        return {\n",
    "            \"document_type\": \"ShareholdersAgreement\",\n",
    "            \"document_title\": f\"Shareholders Agreement {self.graph_id}\",\n",
    "            \"source\": \"legal_document\"\n",
    "        }\n",
    "\n",
    "    def _build_hierarchical_path(self, metadata: dict) -> str:\n",
    "        \"\"\"Build the hierarchical path for a chunk.\"\"\"\n",
    "        path_parts = []\n",
    "        \n",
    "        if \"main_clause_num\" in metadata and \"main_clause_title\" in metadata:\n",
    "            path_parts.append(f\"Clause {metadata['main_clause_num']} â€“ {metadata['main_clause_title']}\")\n",
    "        \n",
    "        if \"sub_clause_num\" in metadata and \"sub_clause_title\" in metadata:\n",
    "            path_parts.append(f\"Sub-clause {metadata['sub_clause_num']} â€“ {metadata['sub_clause_title']}\")\n",
    "        \n",
    "        return \" â†’ \".join(path_parts) if path_parts else \"Document Content\"\n",
    "\n",
    "    def _create_chunk(self, index: int, text: str, metadata: dict) -> TextChunk:\n",
    "        \"\"\"Create a TextChunk with cleaned text and metadata.\"\"\"\n",
    "        clean_text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        final_metadata = {\n",
    "            **metadata,\n",
    "            \"component_type\": self._determine_component_type(metadata)\n",
    "        }\n",
    "        \n",
    "        return TextChunk(index=index, text=clean_text, metadata=final_metadata)\n",
    "\n",
    "    def _determine_component_type(self, metadata: dict) -> str:\n",
    "        \"\"\"Determine the component type based on metadata.\"\"\"\n",
    "        chunk_type = metadata.get(\"chunk_type\", \"content\")\n",
    "        \n",
    "        component_mapping = {\n",
    "            \"main_clause\": \"MainClause\",\n",
    "            \"sub_clause\": \"SubClause\", \n",
    "            \"recital\": \"Recital\",\n",
    "            \"schedule\": \"Schedule\",\n",
    "            \"annexure\": \"Annexure\"\n",
    "        }\n",
    "        \n",
    "        return component_mapping.get(chunk_type, \"Content\")\n",
    "\n",
    "    def _should_split_chunk(self, chunk: TextChunk) -> bool:\n",
    "        \"\"\"Check if a chunk should be split due to size.\"\"\"\n",
    "        tokens = self.tokenizer.encode(chunk.text)\n",
    "        max_tokens = min(4096, self.llm.model_params.get(\"max_tokens\", 3000) * 3)\n",
    "        return len(tokens) > max_tokens\n",
    "\n",
    "    def _should_split_chunk_content(self, content: str) -> bool:\n",
    "        \"\"\"Check if content should be split due to size.\"\"\"\n",
    "        tokens = self.tokenizer.encode(content)\n",
    "        max_tokens = min(4096, self.llm.model_params.get(\"max_tokens\", 3000) * 3)\n",
    "        return len(tokens) > max_tokens\n",
    "\n",
    "    def _split_large_content(self, content: str, metadata: dict, start_index: int) -> List[TextChunk]:\n",
    "        \"\"\"Split large content into smaller chunks with overlap.\"\"\"\n",
    "        tokens = self.tokenizer.encode(content)\n",
    "        max_tokens = min(4096, self.llm.model_params.get(\"max_tokens\", 3000) * 3)\n",
    "        \n",
    "        chunk_size = int(max_tokens * (1 - self.overlap_percentage))\n",
    "        overlap_size = int(max_tokens * self.overlap_percentage)\n",
    "        \n",
    "        chunks = []\n",
    "        for i in range(0, len(tokens), chunk_size):\n",
    "            start_idx = max(0, i)\n",
    "            end_idx = min(i + chunk_size + overlap_size, len(tokens))\n",
    "            chunk_tokens = tokens[start_idx:end_idx]\n",
    "            chunk_text = self.tokenizer.decode(chunk_tokens)\n",
    "            \n",
    "            chunk_metadata = metadata.copy()\n",
    "            if i > 0:\n",
    "                chunk_metadata[\"continued\"] = True\n",
    "                chunk_text = f\"[Continued from {metadata.get('hierarchical_path', 'previous section')}]\\n\\n{chunk_text}\"\n",
    "            \n",
    "            chunks.append(self._create_chunk(start_index + len(chunks), chunk_text, chunk_metadata))\n",
    "        \n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43909668",
   "metadata": {},
   "source": [
    "# Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c5bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/jbarrasa/goingmeta/blob/main/session31/python/utils.py\n",
    "\n",
    "from neo4j_graphrag.experimental.components.schema import (\n",
    "    SchemaBuilder,\n",
    "    SchemaConfig,\n",
    "    SchemaEntity,\n",
    "    SchemaProperty,\n",
    "    SchemaRelation,\n",
    ")\n",
    "from rdflib.namespace import OWL, RDF, RDFS\n",
    "from rdflib import Graph\n",
    "\n",
    "def getLocalPart(uri):\n",
    "    pos = uri.rfind(\"#\")\n",
    "    if pos < 0:\n",
    "        pos = uri.rfind(\"/\")\n",
    "    if pos < 0:\n",
    "        pos = uri.rindex(\":\")\n",
    "    return uri[pos + 1 :]\n",
    "\n",
    "def getPropertiesForClass(g, cat):\n",
    "    props = []\n",
    "    for dtp in g.subjects(RDFS.domain, cat):\n",
    "        if (dtp, RDF.type, OWL.DatatypeProperty) in g:\n",
    "            propName = getLocalPart(dtp)\n",
    "            propDesc = next(g.objects(dtp, RDFS.comment), \"\")\n",
    "            props.append(SchemaProperty(name=propName, type=\"STRING\", description=propDesc))\n",
    "    return props\n",
    "\n",
    "def getSchemaFromOnto(g) -> SchemaConfig:\n",
    "    schema_builder = SchemaBuilder()\n",
    "    classes = {}\n",
    "    entities = []\n",
    "    rels = []\n",
    "    triples = []\n",
    "\n",
    "    for cat in g.subjects(RDF.type, OWL.Class):\n",
    "        classes[cat] = None\n",
    "        label = getLocalPart(cat)\n",
    "        props = getPropertiesForClass(g, cat)\n",
    "        entities.append(SchemaEntity(label=label, description=next(g.objects(cat, RDFS.comment), \"\"), properties=props))\n",
    "\n",
    "    for cat in g.objects(None, RDFS.domain):\n",
    "        if cat not in classes.keys():\n",
    "            classes[cat] = None\n",
    "            label = getLocalPart(cat)\n",
    "            props = getPropertiesForClass(g, cat)\n",
    "            entities.append(SchemaEntity(label=label, description=next(g.objects(cat, RDFS.comment), \"\"), properties=props))\n",
    "\n",
    "    for cat in g.objects(None, RDFS.range):\n",
    "        if not (cat.startswith(\"http://www.w3.org/2001/XMLSchema#\") or cat in classes.keys()):\n",
    "            classes[cat] = None\n",
    "            label = getLocalPart(cat)\n",
    "            props = getPropertiesForClass(g, cat)\n",
    "            entities.append(SchemaEntity(label=label, description=next(g.objects(cat, RDFS.comment), \"\"), properties=props))\n",
    "\n",
    "    for op in g.subjects(RDF.type, OWL.ObjectProperty):\n",
    "        relname = getLocalPart(op)\n",
    "        rels.append(SchemaRelation(label=relname, properties=[], description=next(g.objects(op, RDFS.comment), \"\")))\n",
    "\n",
    "    for op in g.subjects(RDF.type, OWL.ObjectProperty):\n",
    "        relname = getLocalPart(op)\n",
    "        doms = [getLocalPart(dom) for dom in g.objects(op, RDFS.domain) if dom in classes.keys()]\n",
    "        rans = [getLocalPart(ran) for ran in g.objects(op, RDFS.range) if ran in classes.keys()]\n",
    "        for d in doms:\n",
    "            for r in rans:\n",
    "                triples.append((d, relname, r))\n",
    "\n",
    "    return schema_builder.create_schema_model(entities=entities, relations=rels, potential_schema=triples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757e0642",
   "metadata": {},
   "source": [
    "# Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aa1c9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29261,
     "status": "ok",
     "timestamp": 1747311696979,
     "user": {
      "displayName": "Arnaud Crucifix",
      "userId": "14208215153378642511"
     },
     "user_tz": -120
    },
    "id": "77aa1c9a",
    "outputId": "b237d0ab-0fc0-443a-e4e4-cc062485a6d7"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "import neo4j\n",
    "from typing import Dict\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import faiss\n",
    "from rdflib import Graph\n",
    "\n",
    "from neo4j_graphrag.llm import LLMInterface\n",
    "from neo4j_graphrag.embeddings import OpenAIEmbeddings\n",
    "from neo4j_graphrag.experimental.pipeline.kg_builder import SimpleKGPipeline\n",
    "from neo4j_graphrag.llm import OpenAILLM\n",
    "\n",
    "\n",
    "URI = \"neo4j://localhost:7687\"\n",
    "USERNAME = \"neo4j\"\n",
    "PASSWORD = \"hide\"\n",
    "AUTH = (USERNAME, PASSWORD)\n",
    "\n",
    "def clear_database(driver):\n",
    "    with driver.session() as session:\n",
    "        session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "        print(\"Database cleaned.\")\n",
    "\n",
    "def initialize_vector_index(driver):\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            session.run(\"\"\"\n",
    "            CALL db.index.vector.createNodeIndex(\n",
    "                'legal_text_embeddings',\n",
    "                'Paragraph',\n",
    "                'embedding',\n",
    "                1536,\n",
    "                'cosine'\n",
    "            )\n",
    "            \"\"\")\n",
    "            print(\"Vectorial index created.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to create index: {e}\")\n",
    "\n",
    "def fetch_chunks(tx, graph_id):\n",
    "    \"\"\"\n",
    "    Fetch chunks with embeddings from a specific knowledge graph.\n",
    "\n",
    "    Args:\n",
    "        tx: Neo4j transaction\n",
    "        graph_id: Identifier for the knowledge graph\n",
    "\n",
    "    Returns:\n",
    "        List of tuples containing (node_id, embedding)\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    MATCH (c:Chunk)\n",
    "    WHERE c.graph_id = $graph_id AND c.embedding IS NOT NULL\n",
    "    RETURN elementId(c) AS id, c.embedding AS embedding, c.text AS text\n",
    "    \"\"\"\n",
    "    result = tx.run(query, graph_id=graph_id)\n",
    "\n",
    "    # Since embeddings are already lists in Neo4j, we can use them directly\n",
    "    return [(r[\"id\"], r[\"embedding\"], r[\"text\"]) for r in result]\n",
    "\n",
    "def create_similarity_links(tx, matches):\n",
    "    \"\"\"\n",
    "    Create SIMILAR relationships between chunks based on similarity scores.\n",
    "\n",
    "    Args:\n",
    "        tx: Neo4j transaction\n",
    "        matches: Dictionary mapping source node IDs to list of (target_id, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"matches: \", matches.items())\n",
    "    batch = []\n",
    "    for src_id, targets in matches.items():\n",
    "        for tgt_id, score in targets:\n",
    "            if score >= 0.5:  # Similarity threshold\n",
    "                batch.append({\"src\": src_id, \"tgt\": tgt_id, \"score\": float(score)})\n",
    "\n",
    "    if batch:\n",
    "        tx.run(\"\"\"\n",
    "            UNWIND $batch AS row\n",
    "            MATCH (a:Chunk), (b:Chunk)\n",
    "            WHERE elementId(a) = row.src AND elementId(b) = row.tgt\n",
    "            MERGE (a)-[r:SIMILAR]->(b)\n",
    "            SET r.score = row.score\n",
    "        \"\"\", batch=batch)\n",
    "\n",
    "    return len(batch)\n",
    "\n",
    "# Run the pipeline for processing legal documents\n",
    "async def define_and_run_pipeline(\n",
    "    neo4j_driver: neo4j.Driver,\n",
    "    llm: LLMInterface,\n",
    "    embedder,\n",
    "    neo4j_schema\n",
    ") -> Dict:\n",
    "\n",
    "    legal_path = Path(\"legal_corpus/\")\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    structure_splitter = None\n",
    "    for file_path in legal_path.glob(\"*.txt\"):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            print(\"Processing file:\", file_path.name)\n",
    "            text = file.read()\n",
    "            if file_path.name == \"1Constitution.txt\" or file_path.name == \"1Constitution_small.txt\":\n",
    "                structure_splitter = ConstitutionSplitter(llm=llm, graph_id=\"Constitution\")\n",
    "            if file_path.name == \"2Prospectus.txt\" or file_path.name == \"2Prospectus_small.txt\":\n",
    "                structure_splitter = ProspectusSplitter(llm=llm, graph_id=\"Prospectus\")\n",
    "            if file_path.name == \"3Agreement.txt\" or file_path.name == \"3Agreement_small.txt\":\n",
    "                structure_splitter = ShareholdersAgreementSplitter(llm=llm, graph_id=\"Agreement\")\n",
    "\n",
    "            structure_pipeline = SimpleKGPipeline(\n",
    "                llm=llm,\n",
    "                driver=neo4j_driver,\n",
    "                text_splitter=structure_splitter, \n",
    "                #text_splitter=FixedSizeSplitter(chunk_size=2500, chunk_overlap=10),\n",
    "                #embedder=SentenceTransformerEmbeddings(model='all-MiniLM-L6-v2'),\n",
    "                embedder=embedder,\n",
    "                entities=list(neo4j_schema.entities.values()),\n",
    "                relations=list(neo4j_schema.relations.values()),\n",
    "                potential_schema=neo4j_schema.potential_schema,\n",
    "                on_error=\"IGNORE\",\n",
    "                from_pdf=False\n",
    "            )\n",
    "\n",
    "            asyncio.run(structure_pipeline.run_async(text=text))\n",
    "\n",
    "def merge_graphs(driver, graph_id_a, graph_id_b):\n",
    "    stats = {\"relationships_created\": 0}\n",
    "\n",
    "    with driver.session() as session:\n",
    "        # Fetch chunks from both graphs\n",
    "        print(f\"Fetching chunks from graph {graph_id_a}\")\n",
    "        chunks_a = session.execute_read(fetch_chunks, graph_id_a)\n",
    "        print(f\"Found {len(chunks_a)} chunks with embeddings\")\n",
    "\n",
    "        print(f\"Fetching chunks from graph {graph_id_b}\")\n",
    "        chunks_b = session.execute_read(fetch_chunks, graph_id_b)\n",
    "        print(f\"Found {len(chunks_b)} chunks with embeddings\")\n",
    "\n",
    "        if not chunks_a or not chunks_b:\n",
    "            print(\"Warning: One or both graphs have no chunks with embeddings\")\n",
    "            #return stats\n",
    "\n",
    "        # Extract IDs and embeddings\n",
    "        ids_a, emb_a, texts_a = zip(*chunks_a) if chunks_a else ([], [], [])\n",
    "        ids_b, emb_b, texts_b = zip(*chunks_b) if chunks_b else ([], [], [])\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        emb_a = np.array(emb_a).astype('float32')\n",
    "        emb_b = np.array(emb_b).astype('float32')\n",
    "\n",
    "        # Handle empty arrays\n",
    "        if emb_a.size == 0 or emb_b.size == 0:\n",
    "            print(\"Warning: No embeddings found in one or both graphs\")\n",
    "            #return stats\n",
    "\n",
    "        # Normalize for cosine similarity\n",
    "        faiss.normalize_L2(emb_a)\n",
    "        faiss.normalize_L2(emb_b)\n",
    "\n",
    "        # Build similarity index\n",
    "        d = emb_a.shape[1]  # Embedding dimension\n",
    "        print(f\"Building FAISS index with {len(ids_a)} vectors of dimension {d}...\")\n",
    "        index = faiss.IndexFlatIP(d)  # Inner product = cosine similarity for normalized vectors\n",
    "        index.add(emb_a)\n",
    "\n",
    "        # Perform similarity search\n",
    "        print(f\"Finding top {3} similar chunks for each chunk in graph graph_id_b\")\n",
    "        k = min(3, len(ids_a))  # Make sure k isn't larger than available chunks\n",
    "        scores, indices = index.search(emb_b, k)\n",
    "        print(f\"Found {len(indices)} similar chunks for each chunk in graph graph_id_b\")\n",
    "        print(f\"Scores: {scores}\")\n",
    "        print(f\"Indices: {indices}\")\n",
    "\n",
    "        # Prepare matches\n",
    "        matches = {}\n",
    "        for i in range(len(ids_b)):\n",
    "            matches[ids_b[i]] = [(ids_a[indices[i][j]], scores[i][j])\n",
    "                                for j in range(k)\n",
    "                                if scores[i][j] >= 0.5]\n",
    "\n",
    "        print(f\"Prepared {len(matches)} matches\")\n",
    "\n",
    "        # Store links in Neo4j\n",
    "        print(\"Creating similarity links in Neo4j...\")\n",
    "        stats[\"relationships_created\"] = session.execute_write(\n",
    "            create_similarity_links, matches\n",
    "        )\n",
    "\n",
    "        # Also create links in the reverse direction (B â†’ A)\n",
    "        # Build reverse index\n",
    "        index_reverse = faiss.IndexFlatIP(d)\n",
    "        index_reverse.add(emb_b)\n",
    "\n",
    "        # Search A chunks in B\n",
    "        scores_reverse, indices_reverse = index_reverse.search(emb_a, k)\n",
    "        print(f\"Found {len(indices_reverse)} similar chunks for each chunk in graph graph_id_a\")\n",
    "        print(f\"Scores: {scores_reverse}\")\n",
    "        print(f\"Indices: {indices_reverse}\")\n",
    "\n",
    "        # Prepare reverse matches\n",
    "        matches_reverse = {}\n",
    "        for i in range(len(ids_a)):\n",
    "            matches_reverse[ids_a[i]] = [(ids_b[indices_reverse[i][j]], scores_reverse[i][j])\n",
    "                                      for j in range(k)\n",
    "                                      if scores_reverse[i][j] >= 0.5]\n",
    "\n",
    "        print(f\"Prepared {len(matches_reverse)} reverse matches\")\n",
    "\n",
    "        # Store reverse links\n",
    "        print(\"Creating reverse similarity links...\")\n",
    "        stats[\"relationships_created\"] += session.execute_write(\n",
    "            create_similarity_links, matches_reverse\n",
    "        )\n",
    "\n",
    "        # Verify links in database\n",
    "        count_query = \"\"\"\n",
    "        MATCH ()-[r:SIMILAR]->()\n",
    "        RETURN count(r) as count\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = session.run(count_query).single()\n",
    "            stats[\"total_similar_links\"] = result[\"count\"] if result else 0\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Couldn't count total relationships: {e}\")\n",
    "            stats[\"total_similar_links\"] = 0\n",
    "\n",
    "        print(f\"âœ… Created {stats['relationships_created']} cross-graph similarity links\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    g = Graph()\n",
    "    \n",
    "    # Get the schema from the ontology file\n",
    "    neo4j_schema = getSchemaFromOnto(g.parse(\"ontology.ttl\"))\n",
    "    # Set up the llm\n",
    "    llm = OpenAILLM(\n",
    "        api_key=\"mykey\",\n",
    "        model_name=\"gpt-4o-mini\",\n",
    "        #model_name=\"gpt-4.1\",\n",
    "        model_params={\n",
    "            \"max_tokens\": 5000,\n",
    "            \"response_format\": {\"type\": \"json_object\"},\n",
    "            \"temperature\": 0,\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    embedder = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=\"mykey\")\n",
    "    \n",
    "    with neo4j.GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "        print(\"Clearing database...\")\n",
    "        clear_database(driver)\n",
    "\n",
    "        print(\"Initializing vector index...\")\n",
    "        initialize_vector_index(driver)\n",
    "\n",
    "        print(\"Building knowledge graph...\")\n",
    "        res = await define_and_run_pipeline(driver, llm, embedder, neo4j_schema)\n",
    "\n",
    "        # Merge graphs A => B => C\n",
    "        print(\"Merging graphs...\")\n",
    "        merge_graphs(driver, \"Constitution\", \"Prospectus\")\n",
    "        merge_graphs(driver, \"Prospectus\", \"Agreement\")\n",
    "\n",
    "res = await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ce898d",
   "metadata": {},
   "source": [
    "# Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e991a870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from neo4j import GraphDatabase\n",
    "import re\n",
    "import logging\n",
    "import random\n",
    "import json\n",
    "from datetime import datetime\n",
    "import openai\n",
    "from typing import Dict, List\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class LegalTaxonomyExtractor:\n",
    "    def __init__(self, uri, username, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "        \n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "        \n",
    "    def run_query(self, query, params=None):\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(query, params or {})\n",
    "            return list(result)\n",
    "            \n",
    "    def optimize_similarity_relationships(self):\n",
    "        \"\"\"Ensure similarity relationships have weight properties.\"\"\"\n",
    "        query = \"\"\"\n",
    "        MATCH (a)-[r:SIMILAR]->(b)\n",
    "        WHERE r.score IS NOT NULL AND r.weight IS NULL\n",
    "        SET r.weight = r.score\n",
    "        RETURN count(r) as updated\n",
    "        \"\"\"\n",
    "        result = self.run_query(query)\n",
    "        logger.info(f\"Optimized {result[0]['updated']} SIMILAR relationships\")\n",
    "        \n",
    "        # For relationships that might not have a score\n",
    "        query = \"\"\"\n",
    "        MATCH (a)-[r:SIMILAR]->(b)\n",
    "        WHERE r.score IS NULL AND r.weight IS NULL\n",
    "        SET r.weight = 0.5, r.score = 0.5\n",
    "        RETURN count(r) as updated\n",
    "        \"\"\"\n",
    "        result = self.run_query(query)\n",
    "        logger.info(f\"Set default weights for {result[0]['updated']} SIMILAR relationships\")\n",
    "        \n",
    "    def create_graph_projection(self, graph_name=\"legal-taxonomy-graph\"):\n",
    "        \"\"\"Create an in-memory graph projection for community detection.\"\"\"\n",
    "        if graph_name == \"legal-taxonomy-graph\":\n",
    "            timestamp = int(datetime.now().timestamp() * 1000)\n",
    "            graph_name = f\"legal-taxonomy-graph-{timestamp}\"\n",
    "        \n",
    "        # First, check if a graph with this name already exists\n",
    "        check_query = \"\"\"\n",
    "        CALL gds.graph.exists($graph_name) YIELD exists\n",
    "        RETURN exists\n",
    "        \"\"\"\n",
    "        result = self.run_query(check_query, {\"graph_name\": graph_name})\n",
    "        \n",
    "        if result and result[0][\"exists\"]:\n",
    "            # If graph exists, drop it first\n",
    "            drop_query = \"\"\"\n",
    "            CALL gds.graph.drop($graph_name)\n",
    "            YIELD graphName\n",
    "            RETURN graphName\n",
    "            \"\"\"\n",
    "            self.run_query(drop_query, {\"graph_name\": graph_name})\n",
    "            logger.info(f\"Dropped existing graph projection: {graph_name}\")\n",
    "        \n",
    "        # Create new graph projection\n",
    "        query = \"\"\"\n",
    "        CALL gds.graph.project(\n",
    "            $graph_name,\n",
    "            '*',\n",
    "            {\n",
    "                relType: {\n",
    "                    type: '*',\n",
    "                    orientation: 'UNDIRECTED',\n",
    "                    properties: {}\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        YIELD graphName, nodeCount, relationshipCount\n",
    "        RETURN graphName, nodeCount, relationshipCount\n",
    "        \"\"\"\n",
    "        result = self.run_query(query, {\"graph_name\": graph_name})\n",
    "        \n",
    "        if result:\n",
    "            logger.info(f\"Created graph projection: {result[0]['graphName']} with {result[0]['nodeCount']} nodes and {result[0]['relationshipCount']} relationships\")\n",
    "            return graph_name\n",
    "        else:\n",
    "            logger.error(\"Failed to create graph projection\")\n",
    "            return None\n",
    "        \n",
    "    def run_community_detection(self, graph_name=\"legal-taxonomy-graph\", limit=42, community_node_limit=10):\n",
    "        \"\"\"Run Louvain community detection algorithm on the projected graph.\"\"\"\n",
    "        query = \"\"\"\n",
    "        CALL gds.louvain.stream($graph_name, {\n",
    "            relationshipWeightProperty: null,\n",
    "            includeIntermediateCommunities: true,\n",
    "            seedProperty: ''\n",
    "        })\n",
    "        YIELD nodeId, communityId AS community, intermediateCommunityIds AS communities\n",
    "        WITH gds.util.asNode(nodeId) AS node, community, communities\n",
    "        WITH community, communities, collect(node) AS nodes\n",
    "        WITH community, communities, nodes, size(nodes) AS size\n",
    "        ORDER BY size DESC\n",
    "        LIMIT toInteger($limit)\n",
    "        UNWIND nodes[0..$community_node_limit] AS node\n",
    "        RETURN \n",
    "            node.id AS nodeId,\n",
    "            node.text AS text,\n",
    "            community,\n",
    "            communities AS hierarchicalCommunities,\n",
    "            size\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self.run_query(query, {\n",
    "            \"graph_name\": graph_name,\n",
    "            \"limit\": limit,\n",
    "            \"community_node_limit\": community_node_limit\n",
    "        })\n",
    "        \n",
    "        logger.info(f\"Detected communities with {len(result)} nodes\")\n",
    "        \n",
    "        community_data = []\n",
    "        for record in result:\n",
    "            community_data.append({\n",
    "                'nodeId': record['nodeId'],\n",
    "                'text': record['text'] if record['text'] else '',\n",
    "                'community': record['community'],\n",
    "                'hierarchicalCommunities': record['hierarchicalCommunities'],\n",
    "                'size': record['size']\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(community_data)\n",
    "    \n",
    "    def generate_category_name(self, client, texts, community_id):\n",
    "        # Use a sample of texts to avoid token limits\n",
    "        sample_size = min(5, len(texts))\n",
    "        sample_texts = random.sample(texts, sample_size) if len(texts) > sample_size else texts\n",
    "                        \n",
    "        # Prepare prompt for the LLM\n",
    "        prompt = f\"\"\"Below are {sample_size} legal text snippets that belong to the same category. \n",
    "        Please generate a concise, specific legal category name (2-5 words) that accurately captures \n",
    "        what these texts have in common:\n",
    "\n",
    "        {'-' * 40}\n",
    "        {'\\n'.join([f\"{i+1}. {text[:300]}...\" if len(text) > 300 else f\"{i+1}. {text}\" for i, text in enumerate(sample_texts)])}\n",
    "        {'-' * 40}\n",
    "        \n",
    "        Respond with ONLY the category name, nothing else.\"\"\"\n",
    "        \n",
    "        # Call the LLM API\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a legal taxonomy expert. Generate concise, specific category names for groups of legal texts.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=20\n",
    "        )\n",
    "        \n",
    "        # Extract and clean the category name\n",
    "        category_name = response.choices[0].message.content.strip()\n",
    "        # Remove quotes if present\n",
    "        category_name = category_name.strip('\"\\'')\n",
    "            \n",
    "        logger.info(f\"Generated category name for community {community_id}: '{category_name}'\")\n",
    "        return category_name\n",
    "    \n",
    "    \n",
    "    def create_taxonomy_categories(self, community_df, min_community_size=3):\n",
    "        \"\"\"Create taxonomy category nodes from communities using LLM for labeling.\"\"\"\n",
    "        \n",
    "        communities = community_df.groupby('community')\n",
    "        categories = []\n",
    "        try:\n",
    "            openai_api_key = \"mykey\"\n",
    "            client = openai.OpenAI(api_key=openai_api_key)\n",
    "            \n",
    "            # Process each community\n",
    "            for community_id, group in communities:\n",
    "                if len(group) < min_community_size:\n",
    "                    continue\n",
    "                    \n",
    "                community_texts = group['text'].tolist()\n",
    "                community_texts = [t for t in community_texts if t and isinstance(t, str)]\n",
    "                \n",
    "                if not community_texts:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Generate category name using LLM\n",
    "                    category_name = self.generate_category_name(client, community_texts, community_id)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error generating category name with LLM: {e}\")\n",
    "                    # Fallback to keyword extraction\n",
    "                    #keywords = self.extract_keywords(community_texts)\n",
    "                    category_name = \"not found\"\n",
    "                \n",
    "                categories.append({\n",
    "                    'id': f'category-{community_id}',\n",
    "                    'name': category_name,\n",
    "                    'size': len(group)\n",
    "                })\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error setting up LLM: {e}\")\n",
    "        \n",
    "        # Create category nodes in neo4j\n",
    "        if categories:\n",
    "            params = {\"categories\": categories}\n",
    "            query = \"\"\"\n",
    "            UNWIND $categories AS category\n",
    "            MERGE (t:TaxonomyCategory {id: category.id})\n",
    "            SET t.name = category.name, \n",
    "                t.size = category.size,\n",
    "                t.keywords = split(category.name, ' ')\n",
    "            RETURN count(t) as created\n",
    "            \"\"\"\n",
    "            result = self.run_query(query, params)\n",
    "            logger.info(f\"Created {result[0]['created']} taxonomy category nodes with LLM-generated names\")\n",
    "        \n",
    "        return categories\n",
    "    \n",
    "    def link_nodes_to_categories(self, community_df):\n",
    "        \"\"\"Link nodes to their taxonomy categories.\"\"\"\n",
    "        \n",
    "        # Prepare relationships data\n",
    "        relationships = []\n",
    "        for _, row in community_df.iterrows():\n",
    "            relationships.append({\n",
    "                'nodeId': row['nodeId'],\n",
    "                'categoryId': f\"category-{row['community']}\"\n",
    "            })\n",
    "        \n",
    "        # Create relationships : link nodes to categories\n",
    "        if relationships:\n",
    "            params = {\"relationships\": relationships}\n",
    "            query = \"\"\"\n",
    "            UNWIND $relationships AS rel\n",
    "            MATCH (d {id: rel.nodeId})\n",
    "            MATCH (t:TaxonomyCategory {id: rel.categoryId})\n",
    "            MERGE (d)-[:BELONGS_TO]->(t)\n",
    "            RETURN count(*) as created\n",
    "            \"\"\"\n",
    "            result = self.run_query(query, params)\n",
    "            logger.info(f\"Created {result[0]['created']} BELONGS_TO relationships\")\n",
    "    \n",
    "    def create_constraints_and_indexes(self):\n",
    "        \"\"\"Create constraints and indexes.\"\"\"\n",
    "        # Create unique constraint on TaxonomyCategory.id if it doesn't exist\n",
    "        try:\n",
    "            query = \"\"\"\n",
    "            CREATE CONSTRAINT taxonomy_category_id IF NOT EXISTS\n",
    "            FOR (t:TaxonomyCategory) REQUIRE t.id IS UNIQUE\n",
    "            \"\"\"\n",
    "            self.run_query(query)\n",
    "            logger.info(\"Created constraint on TaxonomyCategory.id\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not create constraint: {e}\")\n",
    "        \n",
    "        # Create index on TaxonomyCategory.name for text search\n",
    "        try:\n",
    "            query = \"\"\"\n",
    "            CREATE INDEX taxonomy_category_name IF NOT EXISTS\n",
    "            FOR (t:TaxonomyCategory) ON (t.name)\n",
    "            \"\"\"\n",
    "            self.run_query(query)\n",
    "            logger.info(\"Created index on TaxonomyCategory.name\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not create index: {e}\")\n",
    "    \n",
    "    def extract_taxonomy(self):\n",
    "        logger.info(\"Starting taxonomy extraction process\")\n",
    "        \n",
    "        self.optimize_similarity_relationships()\n",
    "        \n",
    "        graph_name = self.create_graph_projection()\n",
    "        if not graph_name:\n",
    "            return False\n",
    "        \n",
    "        community_df = self.run_community_detection(graph_name)\n",
    "        \n",
    "        categories = self.create_taxonomy_categories(community_df)\n",
    "        \n",
    "        self.link_nodes_to_categories(community_df)\n",
    "        \n",
    "        self.create_constraints_and_indexes()\n",
    "        \n",
    "        logger.info(\"Taxonomy extraction completed\")\n",
    "        return True\n",
    "            \n",
    "    def print_hierarchical_taxonomy(self):\n",
    "        \"\"\"Print all taxonomy categories in a hierarchical bullet point format,\n",
    "        ensuring all categories are displayed.\"\"\"\n",
    "        \n",
    "        # Get all taxonomy categories\n",
    "        query = \"\"\"\n",
    "            MATCH (t:TaxonomyCategory)\n",
    "            RETURN t.id as id, t.name as name, t.size as size\n",
    "            ORDER BY t.size DESC\n",
    "        \"\"\"\n",
    "        all_categories = self.run_query(query)\n",
    "        \n",
    "        # Track categories already displayed\n",
    "        displayed_categories = set()\n",
    "        \n",
    "        print(\"Generated Hierarchical Taxonomy :\")\n",
    "        \n",
    "        # First, find all root categories (those that have no parents)\n",
    "        query = \"\"\"\n",
    "            MATCH (t:TaxonomyCategory)\n",
    "            WHERE NOT (t)-[:SUBCATEGORY_OF]->()\n",
    "            RETURN t.id as id, t.name as name, t.size as size\n",
    "            ORDER BY t.size DESC\n",
    "        \"\"\"\n",
    "        root_categories = self.run_query(query)\n",
    "        \n",
    "        # Process each root category and its hierarchy\n",
    "        for r in root_categories:\n",
    "            category_id = r['id']\n",
    "            category_name = r['name']\n",
    "            size = r['size']\n",
    "            \n",
    "            # Print the root category\n",
    "            print(f\"* {category_name} ({size} nodes)\")\n",
    "            displayed_categories.add(category_id)\n",
    "            \n",
    "            # Recursively print subcategories\n",
    "            self._print_subcategories(category_id, 1, displayed_categories)\n",
    "\n",
    "    def _print_subcategories(self, category_id, depth, visited_nodes):\n",
    "        \"\"\"Recursively print subcategories of a given category.\"\"\"\n",
    "        # Get all categories related to this one\n",
    "        query = \"\"\"\n",
    "            MATCH (child:TaxonomyCategory)-[:SUBCATEGORY_OF]->(parent:TaxonomyCategory {id: $category_id})\n",
    "            RETURN child.id as id, child.name as name, child.size as size\n",
    "            ORDER BY child.size DESC\n",
    "        \"\"\"\n",
    "        result = self.run_query(query, {\"category_id\": category_id})\n",
    "        \n",
    "        if not result:\n",
    "            # No subcategories found for this category\n",
    "            return\n",
    "        \n",
    "        # Print each related category with proper indentation\n",
    "        for r in result:\n",
    "            sub_id = r['id']\n",
    "            sub_name = r['name']\n",
    "            size = r['size']\n",
    "            \n",
    "            # Create indentation\n",
    "            indent = \"  \" * depth\n",
    "            \n",
    "            # Check for cycles\n",
    "            if sub_id in visited_nodes:\n",
    "                #print(f\"{indent}* {sub_name} ({size} nodes) [CYCLE DETECTED]\")\n",
    "                continue\n",
    "            \n",
    "            # Print subcategory\n",
    "            print(f\"{indent}* {sub_name} ({size} nodes)\")\n",
    "            visited_nodes.add(sub_id)\n",
    "            \n",
    "            # Recursively print its subcategories\n",
    "            self._print_subcategories(sub_id, depth + 1, visited_nodes)\n",
    "        \n",
    "    def generate_hierarchical_taxonomy_with_llm(self, client=None):\n",
    "        \"\"\"Generate a proper hierarchical taxonomy using LLM to organize categories.\"\"\"\n",
    "        \n",
    "        if client is None:\n",
    "            try:\n",
    "                openai_api_key = \"mykey\"\n",
    "                client = openai.OpenAI(api_key=openai_api_key)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error setting up OpenAI client: {e}\")\n",
    "                return False\n",
    "        \n",
    "        # Get all existing categories\n",
    "        query = \"\"\"\n",
    "        MATCH (t:TaxonomyCategory)\n",
    "        RETURN t.id as id, t.name as name, t.size as size\n",
    "        ORDER BY t.size DESC\n",
    "        \"\"\"\n",
    "        result = self.run_query(query)\n",
    "        \n",
    "        if not result:\n",
    "            logger.error(\"No taxonomy categories found\")\n",
    "            return False\n",
    "        \n",
    "        # Prepare category names for LLM\n",
    "        category_names = [r['name'] for r in result]\n",
    "        category_info = {r['name']: {'id': r['id'], 'size': r['size']} for r in result}\n",
    "        \n",
    "        # Generate hierarchy structure using LLM\n",
    "        hierarchy_structure = self._generate_taxonomy_structure(client, category_names)\n",
    "        \n",
    "        if not hierarchy_structure:\n",
    "            logger.error(\"Failed to generate hierarchy structure\")\n",
    "            return False\n",
    "        \n",
    "        # Create the hierarchy in Neo4j\n",
    "        success = self._create_llm_generated_hierarchy(hierarchy_structure, category_info)\n",
    "        \n",
    "        return success\n",
    "    \n",
    "    def _generate_taxonomy_structure(self, client, category_names: List[str]) -> Dict:\n",
    "        \"\"\"Use LLM to generate a hierarchical taxonomy structure.\"\"\"\n",
    "        \n",
    "        categories_text = '\\n'.join([f\"* {name}\" for name in category_names])\n",
    "        \n",
    "        prompt = f\"\"\"Below are legal category names that should be grouped under a tree-like structure (taxonomy):\n",
    "\n",
    "        {categories_text}\n",
    "\n",
    "        Please generate a hierarchical taxonomy with several levels. Create logical parent categories that group related subcategories together. \n",
    "\n",
    "        Requirements:\n",
    "        1. Create a root category that encompasses all legal categories\n",
    "        2. Create 4-6 main parent categories under the root\n",
    "        3. Group the existing categories under appropriate parent categories\n",
    "        4. You can create additional intermediate levels if needed\n",
    "        5. Each existing category should appear exactly once in the hierarchy\n",
    "        6. Use clear, professional legal terminology for parent category names\n",
    "\n",
    "        Return the result as a JSON structure with this format:\n",
    "        {{\n",
    "        \"root\": {{\n",
    "            \"name\": \"Root Category Name\",\n",
    "            \"children\": {{\n",
    "            \"Parent Category 1\": {{\n",
    "                \"name\": \"Parent Category 1 Full Name\",\n",
    "                \"children\": {{\n",
    "                \"Subcategory 1\": {{\"name\": \"Existing Category Name\"}},\n",
    "                \"Subcategory 2\": {{\"name\": \"Another Existing Category Name\"}}\n",
    "                }}\n",
    "            }},\n",
    "            \"Parent Category 2\": {{\n",
    "                \"name\": \"Parent Category 2 Full Name\", \n",
    "                \"children\": {{\n",
    "                \"Subcategory 3\": {{\"name\": \"Yet Another Existing Category Name\"}}\n",
    "                }}\n",
    "            }}\n",
    "            }}\n",
    "        }}\n",
    "        }}\n",
    "\n",
    "        Make sure every existing category from the input list appears exactly once in the JSON structure.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a legal taxonomy expert. Create well-structured hierarchical taxonomies for legal document categorization. Always return valid JSON.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.3,\n",
    "                max_tokens=2000\n",
    "            )\n",
    "            \n",
    "            response_text = response.choices[0].message.content.strip()\n",
    "            \n",
    "            json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_text = json_match.group()\n",
    "                hierarchy_structure = json.loads(json_text)\n",
    "                logger.info(\"Successfully generated hierarchy structure with LLM\")\n",
    "                return hierarchy_structure\n",
    "            else:\n",
    "                logger.error(\"No JSON structure found in LLM response\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating taxonomy structure with LLM: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _create_llm_generated_hierarchy(self, hierarchy_structure: Dict, category_info: Dict) -> bool:\n",
    "        \"\"\"Create the LLM-generated hierarchy in Neo4j.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            new_categories = []\n",
    "            category_mappings = []\n",
    "            \n",
    "            def process_hierarchy_level(node_data, parent_id=None, level=0):\n",
    "                \"\"\"Recursively process hierarchy levels.\"\"\"\n",
    "                \n",
    "                if isinstance(node_data, dict) and 'name' in node_data:\n",
    "                    category_name = node_data['name']\n",
    "                    \n",
    "                    # Generate ID for this category\n",
    "                    if category_name in category_info:\n",
    "                        # Existing leaf category\n",
    "                        category_id = category_info[category_name]['id']\n",
    "                        size = category_info[category_name]['size']\n",
    "                    else:\n",
    "                        # New parent category\n",
    "                        category_id = f\"category-parent-{len(new_categories)}\"\n",
    "                        size = 0 \n",
    "                        new_categories.append({\n",
    "                            'id': category_id,\n",
    "                            'name': category_name,\n",
    "                            'size': size,\n",
    "                            'is_parent': True\n",
    "                        })\n",
    "                    \n",
    "                    # Record parent-child relationship\n",
    "                    if parent_id:\n",
    "                        category_mappings.append({\n",
    "                            'child_id': category_id,\n",
    "                            'parent_id': parent_id\n",
    "                        })\n",
    "                    \n",
    "                    # Process children if they exist\n",
    "                    if 'children' in node_data and node_data['children']:\n",
    "                        for child_key, child_data in node_data['children'].items():\n",
    "                            process_hierarchy_level(child_data, category_id, level + 1)\n",
    "                    \n",
    "                    return category_id\n",
    "                \n",
    "                return None\n",
    "            \n",
    "            # Process the hierarchy starting from root\n",
    "            if 'root' in hierarchy_structure:\n",
    "                root_data = hierarchy_structure['root']\n",
    "                process_hierarchy_level(root_data)\n",
    "            else:\n",
    "                logger.error(\"No root found in hierarchy structure\")\n",
    "                return False\n",
    "            \n",
    "            # Create new parent categories in Neo4j\n",
    "            if new_categories:\n",
    "                params = {\"categories\": new_categories}\n",
    "                query = \"\"\"\n",
    "                UNWIND $categories AS category\n",
    "                MERGE (t:TaxonomyCategory {id: category.id})\n",
    "                SET t.name = category.name, \n",
    "                    t.size = category.size,\n",
    "                    t.keywords = split(category.name, ' '),\n",
    "                    t.is_parent = category.is_parent\n",
    "                RETURN count(t) as created\n",
    "                \"\"\"\n",
    "                result = self.run_query(query, params)\n",
    "                logger.info(f\"Created {result[0]['created']} new parent taxonomy categories\")\n",
    "            \n",
    "            # Create hierarchy relationships in Neo4j\n",
    "            if category_mappings:\n",
    "                params = {\"mappings\": category_mappings}\n",
    "                query = \"\"\"\n",
    "                UNWIND $mappings AS mapping\n",
    "                MATCH (child:TaxonomyCategory {id: mapping.child_id})\n",
    "                MATCH (parent:TaxonomyCategory {id: mapping.parent_id})\n",
    "                MERGE (child)-[:SUBCATEGORY_OF]->(parent)\n",
    "                RETURN count(*) as created\n",
    "                \"\"\"\n",
    "                result = self.run_query(query, params)\n",
    "                logger.info(f\"Created {result[0]['created']} SUBCATEGORY_OF relationships\")\n",
    "            \n",
    "            # Update parent category sizes based on their children\n",
    "            self._update_parent_category_sizes()\n",
    "            \n",
    "            logger.info(\"Successfully created LLM-generated hierarchical taxonomy\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating LLM-generated hierarchy: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _update_parent_category_sizes(self):\n",
    "        \"\"\"Update parent category sizes based on the sum of their children's sizes.\"\"\"\n",
    "        query = \"\"\"\n",
    "        MATCH (parent:TaxonomyCategory)<-[:SUBCATEGORY_OF*]-(leaf:TaxonomyCategory)\n",
    "        WHERE NOT ()-[:SUBCATEGORY_OF]->(leaf)\n",
    "        WITH parent, sum(leaf.size) as total_size\n",
    "        SET parent.size = total_size\n",
    "        RETURN parent.name as name, total_size\n",
    "        \"\"\"\n",
    "        result = self.run_query(query)\n",
    "        logger.info(f\"Updated sizes for {len(result)} parent categories\")\n",
    "    \n",
    "    \n",
    "    def build_hierarchical_taxonomy(self):\n",
    "        \"\"\"Complete process to rebuild taxonomy with LLM-generated hierarchy.\"\"\"\n",
    "        logger.info(\"Starting LLM-based taxonomy hierarchy rebuild\")\n",
    "        \n",
    "        success = self.generate_hierarchical_taxonomy_with_llm()\n",
    "        \n",
    "        if success:\n",
    "            logger.info(\"Successfully rebuilt taxonomy with LLM-generated hierarchy\")\n",
    "            self.print_hierarchical_taxonomy()\n",
    "        else:\n",
    "            logger.error(\"Failed to rebuild taxonomy with LLM hierarchy\")\n",
    "        \n",
    "        return success\n",
    "        \n",
    "\n",
    "def main():\n",
    "    NEO4J_URI = \"neo4j://localhost:7687\"\n",
    "    NEO4J_USER = \"neo4j\"\n",
    "    NEO4J_PASSWORD = \"hide\"\n",
    "    \n",
    "    try:\n",
    "        extractor = LegalTaxonomyExtractor(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)\n",
    "        \n",
    "        extractor.extract_taxonomy()\n",
    "\n",
    "        extractor.print_hierarchical_taxonomy()\n",
    "        \n",
    "        extractor.build_hierarchical_taxonomy()\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {e}\")\n",
    "    finally:\n",
    "        if 'extractor' in locals():\n",
    "            extractor.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e33764",
   "metadata": {},
   "source": [
    "# GraphRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fWWNu8FHgtU",
   "metadata": {
    "id": "4fWWNu8FHgtU"
   },
   "outputs": [],
   "source": [
    "from neo4j_graphrag.indexes import create_vector_index, upsert_vector\n",
    "import neo4j\n",
    "\n",
    "VECTOR_STORE_NAME = \"legal_corpus\"\n",
    "DIMENSION = 1536\n",
    "\n",
    "def drop_vector_index(driver, index_name):\n",
    "    \"\"\"\n",
    "    Drop a vector index from Neo4j database using various methods\n",
    "    depending on Neo4j version.\n",
    "    \n",
    "    Args:\n",
    "        driver: Neo4j driver instance\n",
    "        index_name: Name of the vector index to drop\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            try:\n",
    "                session.run(f\"DROP INDEX {index_name}\")\n",
    "                print(f\"Vector index '{index_name}' dropped successfully using standard DROP INDEX.\")\n",
    "                return True\n",
    "            except Exception as e1:\n",
    "                print(f\"Standard DROP INDEX failed: {e1}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error dropping vector index '{index_name}': {e}\")\n",
    "        return False\n",
    "\n",
    "def get_chunk_embeddings(driver) -> list:\n",
    "    \"\"\"\n",
    "    Retrieve all Chunk nodes along with their embedding properties.\n",
    "    Returns a list of tuples (node_id, embedding).\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\n",
    "            \"\"\"\n",
    "            MATCH (c:Chunk)\n",
    "            WHERE c.embedding IS NOT NULL\n",
    "            RETURN id(c) AS node_id, c.embedding AS embedding\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        embeddings_data = [\n",
    "            (record[\"node_id\"], record[\"embedding\"]) for record in result\n",
    "        ]\n",
    "\n",
    "        return embeddings_data\n",
    "\n",
    "def main():\n",
    "    URI = \"neo4j://localhost:7687\"\n",
    "    USERNAME = \"neo4j\"\n",
    "    PASSWORD = \"hide\"\n",
    "    AUTH = (USERNAME, PASSWORD)\n",
    "\n",
    "    driver = neo4j.GraphDatabase.driver(URI, auth=AUTH)\n",
    "\n",
    "    drop_vector_index(driver, VECTOR_STORE_NAME)\n",
    "\n",
    "    create_vector_index(\n",
    "        driver,\n",
    "        name=VECTOR_STORE_NAME,\n",
    "        label=\"Legal_corpus\",\n",
    "        embedding_property=\"vectorProperty\",\n",
    "        dimensions=DIMENSION,\n",
    "        similarity_fn=\"cosine\",\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Fetch all chunk nodes along with their embeddings from the knowledge graph.\n",
    "        embeddings_data = get_chunk_embeddings(driver)\n",
    "\n",
    "        if not embeddings_data:\n",
    "            print(\"No Chunk nodes with embeddings found.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Adding {len(embeddings_data)} embeddings to vector store '{VECTOR_STORE_NAME}'...\")\n",
    "\n",
    "        successful_ops_counter = 0\n",
    "\n",
    "        # Process and upsert each node's embedding into the vector store.\n",
    "        # This transfers embeddings data from standard node properties into a dedicated Neo4j vector index\n",
    "        for node_id, embedding in embeddings_data:\n",
    "            upsert_vector(\n",
    "                driver,\n",
    "                node_id=node_id,\n",
    "                embedding_property=\"embedding\",\n",
    "                vector=embedding,\n",
    "            )\n",
    "            successful_ops_counter += 1\n",
    "\n",
    "        print(f\"Successfully added {successful_ops_counter} embeddings to the vector store.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        \n",
    "    finally:\n",
    "        driver.close()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QC-tEU51Hsgh",
   "metadata": {
    "id": "QC-tEU51Hsgh"
   },
   "outputs": [],
   "source": [
    "from neo4j_graphrag.embeddings import OpenAIEmbeddings\n",
    "from neo4j_graphrag.generation import GraphRAG\n",
    "from neo4j_graphrag.llm import OpenAILLM\n",
    "from neo4j_graphrag.retrievers import VectorRetriever\n",
    "\n",
    "URI = \"neo4j://localhost:7687\"\n",
    "USERNAME = \"neo4j\"\n",
    "PASSWORD = \"hide\"\n",
    "AUTH = (USERNAME, PASSWORD)\n",
    "INDEX_NAME = \"legal_corpus\"\n",
    "DATABASE = \"neo4j\"\n",
    "\n",
    "driver = neo4j.GraphDatabase.driver(URI, auth=AUTH)\n",
    "\n",
    "# Create an Embedder object to convert the user's question into a vector. The same embedding model used for knowledge graph creation is used here.\n",
    "embedder = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=\"mykey\")\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = VectorRetriever(driver, INDEX_NAME, embedder)\n",
    "\n",
    "# Set up the LLM for generating answers based on the retrieved knowledge graph leal texts.\n",
    "llm = OpenAILLM(\n",
    "    api_key=\"mykey\",\n",
    "    model_name=\"gpt-4\",\n",
    "    model_params={\n",
    "        \"temperature\": 0,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Initialize the RAG pipeline\n",
    "rag = GraphRAG(retriever=retriever, llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5fe92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the graph\n",
    "#query_text = \"Tell me something about risks\"\n",
    "#query_text = \"What is the legal form of FinaStream Fund?\"\n",
    "#query_text = \"What assets can the company invest in?\"\n",
    "#query_text = \"Can you give the different types of shares?\"\n",
    "query_text = \"Give all you know about the Investment Regulations and Guidelines taxonomyCategory?\"\n",
    "#query_text = \"How are the Tolaxis Single Asset Funds legally organised?\"\n",
    "#query_text = \"Provide the paragraph that explains under what conditions an investorâ€™s shares can be forcibly redeemed by the company.\"\n",
    "#query_text = \"Provide and show the paragraph that defines the different share classes and their specific rights. Dont forget to give the provision number.\"\n",
    "#query_text = \"What can you say about the fith article of the Shareholders from the FinaStream Fund in the constitution? Is it related to anything in the offering document?\"\n",
    "#query_text = \"Can you say about the FinaStream Fund? Give many details related in documents as possible and give the answer only from those\"\n",
    "response = rag.search(query_text=query_text, retriever_config={\"top_k\": 5})\n",
    "print(response.answer)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
